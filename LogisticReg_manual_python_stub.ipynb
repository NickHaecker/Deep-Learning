{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TmURzPPRtODn"},"source":["# Logistic Regression Assignment (due 26 November)\n","\n","In this practical you will learn how to apply logistic regression to the task of predicting two digits from the MNIST database: http://yann.lecun.com/exdb/mnist/. The database contains 60000 train images containing digits and 10000 test images. The images are of size 28 × 28. We will use the images in a vectorized form: a vector of size of 784. The code extracting the digits 0 and 1 is provided in the stubs.\n"]},{"cell_type":"code","metadata":{"id":"MQkzOf0dFpxc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b34c078a-9abd-425a-ba4a-96d4301f389d","executionInfo":{"status":"ok","timestamp":1682087851210,"user_tz":-120,"elapsed":541,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["%tensorflow_version 2.x"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}]},{"cell_type":"code","metadata":{"id":"xI0dT0-WEYuh","executionInfo":{"status":"ok","timestamp":1682087852939,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqiqzu3dFXKj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"15a26c85-2430-4566-8118-8057ee191104","executionInfo":{"status":"ok","timestamp":1682087854391,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(tf.__version__)\n"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["2.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"ZSjE8NINEvVO","executionInfo":{"status":"ok","timestamp":1682087855754,"user_tz":-120,"elapsed":1,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["import numpy as np "],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybQxEwHnGsgY","executionInfo":{"status":"ok","timestamp":1682087857470,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["def data_preprocess(images, labels):\n","\n","    # number of examples m  \n","    m = images.shape[0]\n","    \n","    print(m)\n","    # create vector of ones to concatenate to our data matrix (for intercept terms)\n","    ones = np.ones(shape=[m, 1])\n","    images = np.concatenate((ones, images), axis=1)\n","    \n","    # to retrieve the images and corresponding labels where the label is either 0 or 1, \n","    # we define two logical vectors that can be used to subset our data_matrices\n","    logical_mask_0 = labels == 0\n","    logical_mask_1 = labels == 1\n","    \n","    images_zeros = images[logical_mask_0]\n","    labels_zeros = labels[logical_mask_0]\n","    images_ones = images[logical_mask_1]\n","    labels_ones = labels[logical_mask_1]\n","    \n","    X = np.concatenate((images_zeros, images_ones), axis=0)\n","    y = np.concatenate((labels_zeros, labels_ones), axis=0)\n","    \n","    # shuffle the data and corresponding labels in unison\n","    def _shuffle_in_unison(a, b):\n","        assert len(a) == len(b)\n","        p = np.random.permutation(len(a))\n","        print('length ', len(a))\n","        print(a.shape)\n","        print(a[p].shape)\n","        return a[p], b[p]\n","\n","    return _shuffle_in_unison(X,y)   "],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrkpG7BYI8MT","executionInfo":{"status":"ok","timestamp":1682087865267,"user_tz":-120,"elapsed":836,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHEiLlEnNhIm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"32c378b3-cc75-4375-e627-198b40f32219","executionInfo":{"status":"ok","timestamp":1682087867034,"user_tz":-120,"elapsed":415,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print (x_train.shape)"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n"]}]},{"cell_type":"code","metadata":{"id":"Cb5Hr9aENMTy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fd54621-ac10-4ffb-e7f3-fc1a551ac1a9","executionInfo":{"status":"ok","timestamp":1682087868420,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["x_train = x_train.reshape([60000,784])\n","x_test = x_test.reshape([10000,784])\n","print(x_train.shape)\n"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n"]}]},{"cell_type":"code","metadata":{"id":"cMi47AaKcHzQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"149494aa-990d-4c29-8f58-932c4bf2b84b","executionInfo":{"status":"ok","timestamp":1682087870038,"user_tz":-120,"elapsed":349,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["X,y = data_preprocess(x_train, y_train)\n","print('shape: ', X.shape)\n","print('shape: ', y.shape)"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","length  12665\n","(12665, 785)\n","(12665, 785)\n","shape:  (12665, 785)\n","shape:  (12665,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"UTx9l6kePGbf"},"source":["Define hyperparams: learning rate and gradient descent steps\n"]},{"cell_type":"code","metadata":{"id":"NzE8JqGXdBy2","executionInfo":{"status":"ok","timestamp":1682087872663,"user_tz":-120,"elapsed":346,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["learning_rate = 0.01\n","gdc_steps = 1000\n","\n","    "],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5ujHEHrQDy_"},"source":["Initialize your parameters W\n"]},{"cell_type":"code","metadata":{"id":"RnXlpZqMR4jP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682087875427,"user_tz":-120,"elapsed":423,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"503f0e56-5822-4eae-d4e5-ce7e74fc2ac2"},"source":["# number of features n\n","n = X.shape[1]\n","print(n)\n","# we need to define our model parameters to be learned. we use W (weights) instead of theta this time.\n","mu, sigma = 0, 0.01 # mean and standard deviation\n","w = np.random.normal(mu, sigma, n)\n"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["785\n"]}]},{"cell_type":"code","metadata":{"id":"1b-fVYzzR6_r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd64c93c-0001-4722-bdcd-a30e15ccb638","executionInfo":{"status":"ok","timestamp":1682087877290,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(X.shape, w.shape)"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["(12665, 785) (785,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Yp3Uc6qUSIi0"},"source":["Define the sigmoid function, your code here:\n"]},{"cell_type":"code","metadata":{"id":"O_gdiFglSMYN","executionInfo":{"status":"ok","timestamp":1682087896276,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m14UBJJ6SRGS"},"source":["Define the loss function as provided in equation 12 (Logistic regression slides)\n"]},{"cell_type":"code","metadata":{"id":"NyCliB0USThU","executionInfo":{"status":"ok","timestamp":1682090250999,"user_tz":-120,"elapsed":659,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["def compute_cross_entropy_loss(y, y_hat):\n","    loss = 0\n","    for i in range(len(y)):\n","        loss += -y[i] * np.log(y_hat[i]) - (1 - y[i]) * np.log(1 - y_hat[i])\n","    return loss / len(y)\n","\n"],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vusNro8SX5-"},"source":["Start optimization. During training you minimize the loss function. In every iteration your loss should decrease. You also want to look how many correct predictions you have at every iteration. Reminder: the belonging to class digit 1 is when your prediction, $\\hat y$ is greater or equal to 0.5. "]},{"cell_type":"markdown","metadata":{"id":"fKXM0YdDcajm"},"source":["When you test your prediction vector (containing zero and ones) with the labels (also zero and ones) you can use the equal function. \n","\n","Example:\n","prediction = (1, 0, 1, 1) and the true labels are y = (0, 0, 1, 0).\n","\n","When you test on equality you get following result: correct = (0, 1, 1, 0). Your accuracy is: 0+1+1+0\n","4 = 0.5.\n","You compute the accuracy for the training and test."]},{"cell_type":"code","metadata":{"id":"5ToLLOp2Scv0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682089883639,"user_tz":-120,"elapsed":286781,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"fb754980-7747-4e5b-88bf-f6e0bfe55ccd"},"source":["for step in range(0, gdc_steps):\n","    print(\"Performing step \" + str(step) + \" of gradient descent.\")\n","    # perform the dot product between the weights and the examples\n","    z = np.dot(X,w)\n","    print('z', z, z.shape)\n","    # apply the nonlinearity\n","    y_hat = sigmoid(z)\n","    print(\"y hat: \" + str(y_hat))\n","    # normally normalized with -1/m \n","    loss = compute_cross_entropy_loss(y, y_hat)\n","    print(\"Loss at step \" + str(step) + \": \" + str(loss))\n","    \n","    # compute the error term, i.e. the difference between labels and estimated labels y_hat, see equation 24 in the slides\n","    error_term = y_hat - y\n","    \n","    # compute the gradient. as our data matrix X is currently layed out as X_j_i, we got to transpose it \n","    # see derived formula of the gradient calculation\n","    gradients =  (1/len(y)) * np.dot(np.transpose(X),error_term)\n","    print(X.shape)\n","    print(error_term.shape)\n","    print(gradients.shape)\n","    \n","    # update w using the gdc update rule\n","    w = w-learning_rate*gradients\n","    \n","    # compute the predictions and cast them to int values\n","    predictions = np.int64(y_hat >= 0.5)\n","    print(predictions)\n","    print(predictions.shape)\n","    # compute mean accuracy\n","    accuracy =  np.mean(predictions == y)\n","    print(\"Accuracy at step \" + str(step) + \": \" + str(accuracy))\n","    \n"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing step 0 of gradient descent.\n","z [ 393.03083444 -443.7641193   377.47323784 ...  517.64468397 -827.98938379\n"," -693.18550942] (12665,)\n","y hat: [1.00000000e+000 1.88665165e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 8.98169543e-302]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-57-dc64dee5606b>:2: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-z))\n","<ipython-input-58-cc79fee54ce2>:5: RuntimeWarning: divide by zero encountered in log\n","  loss += -y[i] * np.log(y_hat[i]) - (1 - y[i]) * np.log(1 - y_hat[i])\n","<ipython-input-58-cc79fee54ce2>:5: RuntimeWarning: invalid value encountered in multiply\n","  loss += -y[i] * np.log(y_hat[i]) - (1 - y[i]) * np.log(1 - y_hat[i])\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mDie letzten 5000 Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n"," 0.00000000e+000 9.26501915e-302]\n","Loss at step 583: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 583: 0.9989735491512041\n","Performing step 584 of gradient descent.\n","z [ 393.01676428 -443.74189957  377.45867998 ...  517.61285436 -827.96280497\n"," -693.15439888] (12665,)\n","y hat: [1.00000000e+000 1.92904174e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26551274e-302]\n","Loss at step 584: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 584: 0.9989735491512041\n","Performing step 585 of gradient descent.\n","z [ 393.01674018 -443.74186153  377.45865505 ...  517.61279986 -827.96275946\n"," -693.15434561] (12665,)\n","y hat: [1.00000000e+000 1.92911514e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26600636e-302]\n","Loss at step 585: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 585: 0.9989735491512041\n","Performing step 586 of gradient descent.\n","z [ 393.01671609 -443.74182348  377.45863013 ...  517.61274536 -827.96271395\n"," -693.15429234] (12665,)\n","y hat: [1.00000000e+000 1.92918854e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26650001e-302]\n","Loss at step 586: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 586: 0.9989735491512041\n","Performing step 587 of gradient descent.\n","z [ 393.016692   -443.74178543  377.4586052  ...  517.61269086 -827.96266844\n"," -693.15423906] (12665,)\n","y hat: [1.00000000e+000 1.92926194e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26699368e-302]\n","Loss at step 587: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 587: 0.9989735491512041\n","Performing step 588 of gradient descent.\n","z [ 393.01666791 -443.74174739  377.45858027 ...  517.61263636 -827.96262293\n"," -693.15418579] (12665,)\n","y hat: [1.00000000e+000 1.92933534e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26748737e-302]\n","Loss at step 588: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 588: 0.9989735491512041\n","Performing step 589 of gradient descent.\n","z [ 393.01664381 -443.74170934  377.45855534 ...  517.61258186 -827.96257742\n"," -693.15413252] (12665,)\n","y hat: [1.00000000e+000 1.92940875e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26798110e-302]\n","Loss at step 589: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 589: 0.9989735491512041\n","Performing step 590 of gradient descent.\n","z [ 393.01661972 -443.74167129  377.45853041 ...  517.61252736 -827.9625319\n"," -693.15407924] (12665,)\n","y hat: [1.00000000e+000 1.92948215e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26847484e-302]\n","Loss at step 590: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 590: 0.9989735491512041\n","Performing step 591 of gradient descent.\n","z [ 393.01659563 -443.74163325  377.45850548 ...  517.61247286 -827.96248639\n"," -693.15402597] (12665,)\n","y hat: [1.00000000e+000 1.92955557e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26896862e-302]\n","Loss at step 591: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 591: 0.9989735491512041\n","Performing step 592 of gradient descent.\n","z [ 393.01657154 -443.7415952   377.45848056 ...  517.61241835 -827.96244088\n"," -693.1539727 ] (12665,)\n","y hat: [1.00000000e+000 1.92962898e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26946242e-302]\n","Loss at step 592: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 592: 0.9989735491512041\n","Performing step 593 of gradient descent.\n","z [ 393.01654745 -443.74155716  377.45845563 ...  517.61236385 -827.96239537\n"," -693.15391942] (12665,)\n","y hat: [1.00000000e+000 1.92970240e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.26995625e-302]\n","Loss at step 593: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 593: 0.9989735491512041\n","Performing step 594 of gradient descent.\n","z [ 393.01652335 -443.74151911  377.4584307  ...  517.61230935 -827.96234986\n"," -693.15386615] (12665,)\n","y hat: [1.00000000e+000 1.92977582e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27045011e-302]\n","Loss at step 594: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 594: 0.9989735491512041\n","Performing step 595 of gradient descent.\n","z [ 393.01649926 -443.74148106  377.45840577 ...  517.61225485 -827.96230435\n"," -693.15381288] (12665,)\n","y hat: [1.00000000e+000 1.92984924e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27094399e-302]\n","Loss at step 595: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 595: 0.9989735491512041\n","Performing step 596 of gradient descent.\n","z [ 393.01647517 -443.74144302  377.45838084 ...  517.61220035 -827.96225884\n"," -693.1537596 ] (12665,)\n","y hat: [1.00000000e+000 1.92992267e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27143789e-302]\n","Loss at step 596: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 596: 0.9989735491512041\n","Performing step 597 of gradient descent.\n","z [ 393.01645108 -443.74140497  377.45835592 ...  517.61214585 -827.96221332\n"," -693.15370633] (12665,)\n","y hat: [1.00000000e+000 1.92999609e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27193183e-302]\n","Loss at step 597: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 597: 0.9989735491512041\n","Performing step 598 of gradient descent.\n","z [ 393.01642698 -443.74136692  377.45833099 ...  517.61209135 -827.96216781\n"," -693.15365306] (12665,)\n","y hat: [1.00000000e+000 1.93006952e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27242579e-302]\n","Loss at step 598: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 598: 0.9989735491512041\n","Performing step 599 of gradient descent.\n","z [ 393.01640289 -443.74132888  377.45830606 ...  517.61203685 -827.9621223\n"," -693.15359978] (12665,)\n","y hat: [1.00000000e+000 1.93014296e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27291978e-302]\n","Loss at step 599: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 599: 0.9989735491512041\n","Performing step 600 of gradient descent.\n","z [ 393.0163788  -443.74129083  377.45828113 ...  517.61198235 -827.96207679\n"," -693.15354651] (12665,)\n","y hat: [1.00000000e+000 1.93021639e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27341379e-302]\n","Loss at step 600: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 600: 0.9989735491512041\n","Performing step 601 of gradient descent.\n","z [ 393.01635471 -443.74125278  377.4582562  ...  517.61192785 -827.96203128\n"," -693.15349324] (12665,)\n","y hat: [1.00000000e+000 1.93028983e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27390783e-302]\n","Loss at step 601: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 601: 0.9989735491512041\n","Performing step 602 of gradient descent.\n","z [ 393.01633062 -443.74121474  377.45823128 ...  517.61187334 -827.96198577\n"," -693.15343996] (12665,)\n","y hat: [1.00000000e+000 1.93036328e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27440189e-302]\n","Loss at step 602: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 602: 0.9989735491512041\n","Performing step 603 of gradient descent.\n","z [ 393.01630652 -443.74117669  377.45820635 ...  517.61181884 -827.96194025\n"," -693.15338669] (12665,)\n","y hat: [1.00000000e+000 1.93043672e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27489599e-302]\n","Loss at step 603: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 603: 0.9989735491512041\n","Performing step 604 of gradient descent.\n","z [ 393.01628243 -443.74113864  377.45818142 ...  517.61176434 -827.96189474\n"," -693.15333342] (12665,)\n","y hat: [1.00000000e+000 1.93051017e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27539010e-302]\n","Loss at step 604: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 604: 0.9989735491512041\n","Performing step 605 of gradient descent.\n","z [ 393.01625834 -443.7411006   377.45815649 ...  517.61170984 -827.96184923\n"," -693.15328014] (12665,)\n","y hat: [1.00000000e+000 1.93058362e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27588425e-302]\n","Loss at step 605: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 605: 0.9989735491512041\n","Performing step 606 of gradient descent.\n","z [ 393.01623425 -443.74106255  377.45813156 ...  517.61165534 -827.96180372\n"," -693.15322687] (12665,)\n","y hat: [1.00000000e+000 1.93065707e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27637842e-302]\n","Loss at step 606: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 606: 0.9989735491512041\n","Performing step 607 of gradient descent.\n","z [ 393.01621016 -443.7410245   377.45810663 ...  517.61160084 -827.96175821\n"," -693.1531736 ] (12665,)\n","y hat: [1.00000000e+000 1.93073053e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27687262e-302]\n","Loss at step 607: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 607: 0.9989735491512041\n","Performing step 608 of gradient descent.\n","z [ 393.01618606 -443.74098646  377.45808171 ...  517.61154634 -827.9617127\n"," -693.15312032] (12665,)\n","y hat: [1.00000000e+000 1.93080399e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27736684e-302]\n","Loss at step 608: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 608: 0.9989735491512041\n","Performing step 609 of gradient descent.\n","z [ 393.01616197 -443.74094841  377.45805678 ...  517.61149184 -827.96166718\n"," -693.15306705] (12665,)\n","y hat: [1.00000000e+000 1.93087745e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27786109e-302]\n","Loss at step 609: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 609: 0.9989735491512041\n","Performing step 610 of gradient descent.\n","z [ 393.01613788 -443.74091036  377.45803185 ...  517.61143734 -827.96162167\n"," -693.15301378] (12665,)\n","y hat: [1.00000000e+000 1.93095091e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27835537e-302]\n","Loss at step 610: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 610: 0.9989735491512041\n","Performing step 611 of gradient descent.\n","z [ 393.01611379 -443.74087232  377.45800692 ...  517.61138284 -827.96157616\n"," -693.1529605 ] (12665,)\n","y hat: [1.00000000e+000 1.93102438e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27884967e-302]\n","Loss at step 611: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 611: 0.9989735491512041\n","Performing step 612 of gradient descent.\n","z [ 393.0160897  -443.74083427  377.45798199 ...  517.61132834 -827.96153065\n"," -693.15290723] (12665,)\n","y hat: [1.00000000e+000 1.93109785e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27934400e-302]\n","Loss at step 612: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 612: 0.9989735491512041\n","Performing step 613 of gradient descent.\n","z [ 393.0160656  -443.74079623  377.45795707 ...  517.61127383 -827.96148514\n"," -693.15285396] (12665,)\n","y hat: [1.00000000e+000 1.93117132e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.27983836e-302]\n","Loss at step 613: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 613: 0.9989735491512041\n","Performing step 614 of gradient descent.\n","z [ 393.01604151 -443.74075818  377.45793214 ...  517.61121933 -827.96143963\n"," -693.15280068] (12665,)\n","y hat: [1.00000000e+000 1.93124480e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28033274e-302]\n","Loss at step 614: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 614: 0.9989735491512041\n","Performing step 615 of gradient descent.\n","z [ 393.01601742 -443.74072013  377.45790721 ...  517.61116483 -827.96139412\n"," -693.15274741] (12665,)\n","y hat: [1.00000000e+000 1.93131828e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28082715e-302]\n","Loss at step 615: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 615: 0.9989735491512041\n","Performing step 616 of gradient descent.\n","z [ 393.01599333 -443.74068209  377.45788228 ...  517.61111033 -827.9613486\n"," -693.15269414] (12665,)\n","y hat: [1.00000000e+000 1.93139176e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28132159e-302]\n","Loss at step 616: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 616: 0.9989735491512041\n","Performing step 617 of gradient descent.\n","z [ 393.01596923 -443.74064404  377.45785735 ...  517.61105583 -827.96130309\n"," -693.15264086] (12665,)\n","y hat: [1.00000000e+000 1.93146524e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28181605e-302]\n","Loss at step 617: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 617: 0.9989735491512041\n","Performing step 618 of gradient descent.\n","z [ 393.01594514 -443.74060599  377.45783243 ...  517.61100133 -827.96125758\n"," -693.15258759] (12665,)\n","y hat: [1.00000000e+000 1.93153873e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28231054e-302]\n","Loss at step 618: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 618: 0.9989735491512041\n","Performing step 619 of gradient descent.\n","z [ 393.01592105 -443.74056795  377.4578075  ...  517.61094683 -827.96121207\n"," -693.15253431] (12665,)\n","y hat: [1.00000000e+000 1.93161222e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28280505e-302]\n","Loss at step 619: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 619: 0.9989735491512041\n","Performing step 620 of gradient descent.\n","z [ 393.01589696 -443.7405299   377.45778257 ...  517.61089233 -827.96116656\n"," -693.15248104] (12665,)\n","y hat: [1.00000000e+000 1.93168571e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28329959e-302]\n","Loss at step 620: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 620: 0.9989735491512041\n","Performing step 621 of gradient descent.\n","z [ 393.01587287 -443.74049185  377.45775764 ...  517.61083783 -827.96112105\n"," -693.15242777] (12665,)\n","y hat: [1.00000000e+000 1.93175921e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28379416e-302]\n","Loss at step 621: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 621: 0.9989735491512041\n","Performing step 622 of gradient descent.\n","z [ 393.01584877 -443.74045381  377.45773271 ...  517.61078333 -827.96107553\n"," -693.15237449] (12665,)\n","y hat: [1.00000000e+000 1.93183271e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28428875e-302]\n","Loss at step 622: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 622: 0.9989735491512041\n","Performing step 623 of gradient descent.\n","z [ 393.01582468 -443.74041576  377.45770778 ...  517.61072883 -827.96103002\n"," -693.15232122] (12665,)\n","y hat: [1.00000000e+000 1.93190621e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28478337e-302]\n","Loss at step 623: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 623: 0.9989735491512041\n","Performing step 624 of gradient descent.\n","z [ 393.01580059 -443.74037771  377.45768286 ...  517.61067432 -827.96098451\n"," -693.15226795] (12665,)\n","y hat: [1.00000000e+000 1.93197971e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28527802e-302]\n","Loss at step 624: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 624: 0.9989735491512041\n","Performing step 625 of gradient descent.\n","z [ 393.0157765  -443.74033967  377.45765793 ...  517.61061982 -827.960939\n"," -693.15221467] (12665,)\n","y hat: [1.00000000e+000 1.93205322e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28577269e-302]\n","Loss at step 625: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 625: 0.9989735491512041\n","Performing step 626 of gradient descent.\n","z [ 393.01575241 -443.74030162  377.457633   ...  517.61056532 -827.96089349\n"," -693.1521614 ] (12665,)\n","y hat: [1.00000000e+000 1.93212672e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28626739e-302]\n","Loss at step 626: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 626: 0.9989735491512041\n","Performing step 627 of gradient descent.\n","z [ 393.01572831 -443.74026358  377.45760807 ...  517.61051082 -827.96084798\n"," -693.15210813] (12665,)\n","y hat: [1.00000000e+000 1.93220024e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28676212e-302]\n","Loss at step 627: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 627: 0.9989735491512041\n","Performing step 628 of gradient descent.\n","z [ 393.01570422 -443.74022553  377.45758314 ...  517.61045632 -827.96080247\n"," -693.15205485] (12665,)\n","y hat: [1.00000000e+000 1.93227375e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28725687e-302]\n","Loss at step 628: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 628: 0.9989735491512041\n","Performing step 629 of gradient descent.\n","z [ 393.01568013 -443.74018748  377.45755822 ...  517.61040182 -827.96075695\n"," -693.15200158] (12665,)\n","y hat: [1.00000000e+000 1.93234727e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28775165e-302]\n","Loss at step 629: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 629: 0.9989735491512041\n","Performing step 630 of gradient descent.\n","z [ 393.01565604 -443.74014944  377.45753329 ...  517.61034732 -827.96071144\n"," -693.15194831] (12665,)\n","y hat: [1.00000000e+000 1.93242079e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28824645e-302]\n","Loss at step 630: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 630: 0.9989735491512041\n","Performing step 631 of gradient descent.\n","z [ 393.01563195 -443.74011139  377.45750836 ...  517.61029282 -827.96066593\n"," -693.15189503] (12665,)\n","y hat: [1.00000000e+000 1.93249431e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28874128e-302]\n","Loss at step 631: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 631: 0.9989735491512041\n","Performing step 632 of gradient descent.\n","z [ 393.01560785 -443.74007334  377.45748343 ...  517.61023832 -827.96062042\n"," -693.15184176] (12665,)\n","y hat: [1.00000000e+000 1.93256784e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28923614e-302]\n","Loss at step 632: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 632: 0.9989735491512041\n","Performing step 633 of gradient descent.\n","z [ 393.01558376 -443.7400353   377.4574585  ...  517.61018382 -827.96057491\n"," -693.15178848] (12665,)\n","y hat: [1.00000000e+000 1.93264137e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.28973103e-302]\n","Loss at step 633: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 633: 0.9989735491512041\n","Performing step 634 of gradient descent.\n","z [ 393.01555967 -443.73999725  377.45743358 ...  517.61012932 -827.9605294\n"," -693.15173521] (12665,)\n","y hat: [1.00000000e+000 1.93271490e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29022594e-302]\n","Loss at step 634: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 634: 0.9989735491512041\n","Performing step 635 of gradient descent.\n","z [ 393.01553558 -443.7399592   377.45740865 ...  517.61007482 -827.96048388\n"," -693.15168194] (12665,)\n","y hat: [1.00000000e+000 1.93278843e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29072087e-302]\n","Loss at step 635: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 635: 0.9989735491512041\n","Performing step 636 of gradient descent.\n","z [ 393.01551148 -443.73992116  377.45738372 ...  517.61002032 -827.96043837\n"," -693.15162866] (12665,)\n","y hat: [1.00000000e+000 1.93286197e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29121584e-302]\n","Loss at step 636: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 636: 0.9989735491512041\n","Performing step 637 of gradient descent.\n","z [ 393.01548739 -443.73988311  377.45735879 ...  517.60996581 -827.96039286\n"," -693.15157539] (12665,)\n","y hat: [1.00000000e+000 1.93293551e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29171083e-302]\n","Loss at step 637: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 637: 0.9989735491512041\n","Performing step 638 of gradient descent.\n","z [ 393.0154633  -443.73984506  377.45733386 ...  517.60991131 -827.96034735\n"," -693.15152212] (12665,)\n","y hat: [1.00000000e+000 1.93300905e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29220584e-302]\n","Loss at step 638: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 638: 0.9989735491512041\n","Performing step 639 of gradient descent.\n","z [ 393.01543921 -443.73980702  377.45730893 ...  517.60985681 -827.96030184\n"," -693.15146884] (12665,)\n","y hat: [1.00000000e+000 1.93308260e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29270089e-302]\n","Loss at step 639: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 639: 0.9989735491512041\n","Performing step 640 of gradient descent.\n","z [ 393.01541512 -443.73976897  377.45728401 ...  517.60980231 -827.96025633\n"," -693.15141557] (12665,)\n","y hat: [1.00000000e+000 1.93315615e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29319596e-302]\n","Loss at step 640: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 640: 0.9989735491512041\n","Performing step 641 of gradient descent.\n","z [ 393.01539102 -443.73973093  377.45725908 ...  517.60974781 -827.96021082\n"," -693.1513623 ] (12665,)\n","y hat: [1.00000000e+000 1.93322970e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29369105e-302]\n","Loss at step 641: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 641: 0.9989735491512041\n","Performing step 642 of gradient descent.\n","z [ 393.01536693 -443.73969288  377.45723415 ...  517.60969331 -827.9601653\n"," -693.15130902] (12665,)\n","y hat: [1.00000000e+000 1.93330325e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29418617e-302]\n","Loss at step 642: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 642: 0.9989735491512041\n","Performing step 643 of gradient descent.\n","z [ 393.01534284 -443.73965483  377.45720922 ...  517.60963881 -827.96011979\n"," -693.15125575] (12665,)\n","y hat: [1.00000000e+000 1.93337681e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29468132e-302]\n","Loss at step 643: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 643: 0.9989735491512041\n","Performing step 644 of gradient descent.\n","z [ 393.01531875 -443.73961679  377.45718429 ...  517.60958431 -827.96007428\n"," -693.15120247] (12665,)\n","y hat: [1.00000000e+000 1.93345037e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29517650e-302]\n","Loss at step 644: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 644: 0.9989735491512041\n","Performing step 645 of gradient descent.\n","z [ 393.01529466 -443.73957874  377.45715937 ...  517.60952981 -827.96002877\n"," -693.1511492 ] (12665,)\n","y hat: [1.00000000e+000 1.93352393e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29567170e-302]\n","Loss at step 645: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 645: 0.9989735491512041\n","Performing step 646 of gradient descent.\n","z [ 393.01527056 -443.73954069  377.45713444 ...  517.60947531 -827.95998326\n"," -693.15109593] (12665,)\n","y hat: [1.00000000e+000 1.93359749e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29616693e-302]\n","Loss at step 646: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 646: 0.9989735491512041\n","Performing step 647 of gradient descent.\n","z [ 393.01524647 -443.73950265  377.45710951 ...  517.60942081 -827.95993775\n"," -693.15104265] (12665,)\n","y hat: [1.00000000e+000 1.93367106e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29666218e-302]\n","Loss at step 647: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 647: 0.9989735491512041\n","Performing step 648 of gradient descent.\n","z [ 393.01522238 -443.7394646   377.45708458 ...  517.60936631 -827.95989223\n"," -693.15098938] (12665,)\n","y hat: [1.00000000e+000 1.93374463e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29715746e-302]\n","Loss at step 648: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 648: 0.9989735491512041\n","Performing step 649 of gradient descent.\n","z [ 393.01519829 -443.73942656  377.45705965 ...  517.60931181 -827.95984672\n"," -693.15093611] (12665,)\n","y hat: [1.00000000e+000 1.93381820e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29765277e-302]\n","Loss at step 649: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 649: 0.9989735491512041\n","Performing step 650 of gradient descent.\n","z [ 393.0151742  -443.73938851  377.45703472 ...  517.6092573  -827.95980121\n"," -693.15088283] (12665,)\n","y hat: [1.00000000e+000 1.93389178e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29814810e-302]\n","Loss at step 650: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 650: 0.9989735491512041\n","Performing step 651 of gradient descent.\n","z [ 393.0151501  -443.73935046  377.4570098  ...  517.6092028  -827.9597557\n"," -693.15082956] (12665,)\n","y hat: [1.00000000e+000 1.93396536e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29864346e-302]\n","Loss at step 651: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 651: 0.9989735491512041\n","Performing step 652 of gradient descent.\n","z [ 393.01512601 -443.73931242  377.45698487 ...  517.6091483  -827.95971019\n"," -693.15077629] (12665,)\n","y hat: [1.00000000e+000 1.93403894e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29913885e-302]\n","Loss at step 652: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 652: 0.9989735491512041\n","Performing step 653 of gradient descent.\n","z [ 393.01510192 -443.73927437  377.45695994 ...  517.6090938  -827.95966468\n"," -693.15072301] (12665,)\n","y hat: [1.00000000e+000 1.93411253e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.29963426e-302]\n","Loss at step 653: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 653: 0.9989735491512041\n","Performing step 654 of gradient descent.\n","z [ 393.01507783 -443.73923632  377.45693501 ...  517.6090393  -827.95961916\n"," -693.15066974] (12665,)\n","y hat: [1.00000000e+000 1.93418611e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30012970e-302]\n","Loss at step 654: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 654: 0.9989735491512041\n","Performing step 655 of gradient descent.\n","z [ 393.01505374 -443.73919828  377.45691008 ...  517.6089848  -827.95957365\n"," -693.15061646] (12665,)\n","y hat: [1.00000000e+000 1.93425970e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30062517e-302]\n","Loss at step 655: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 655: 0.9989735491512041\n","Performing step 656 of gradient descent.\n","z [ 393.01502964 -443.73916023  377.45688516 ...  517.6089303  -827.95952814\n"," -693.15056319] (12665,)\n","y hat: [1.00000000e+000 1.93433330e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30112066e-302]\n","Loss at step 656: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 656: 0.9989735491512041\n","Performing step 657 of gradient descent.\n","z [ 393.01500555 -443.73912218  377.45686023 ...  517.6088758  -827.95948263\n"," -693.15050992] (12665,)\n","y hat: [1.00000000e+000 1.93440689e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30161618e-302]\n","Loss at step 657: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 657: 0.9989735491512041\n","Performing step 658 of gradient descent.\n","z [ 393.01498146 -443.73908414  377.4568353  ...  517.6088213  -827.95943712\n"," -693.15045664] (12665,)\n","y hat: [1.00000000e+000 1.93448049e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30211172e-302]\n","Loss at step 658: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 658: 0.9989735491512041\n","Performing step 659 of gradient descent.\n","z [ 393.01495737 -443.73904609  377.45681037 ...  517.6087668  -827.95939161\n"," -693.15040337] (12665,)\n","y hat: [1.00000000e+000 1.93455409e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30260729e-302]\n","Loss at step 659: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 659: 0.9989735491512041\n","Performing step 660 of gradient descent.\n","z [ 393.01493328 -443.73900805  377.45678544 ...  517.6087123  -827.9593461\n"," -693.1503501 ] (12665,)\n","y hat: [1.00000000e+000 1.93462770e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30310289e-302]\n","Loss at step 660: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 660: 0.9989735491512041\n","Performing step 661 of gradient descent.\n","z [ 393.01490918 -443.73897     377.45676051 ...  517.6086578  -827.95930058\n"," -693.15029682] (12665,)\n","y hat: [1.00000000e+000 1.93470130e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30359852e-302]\n","Loss at step 661: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 661: 0.9989735491512041\n","Performing step 662 of gradient descent.\n","z [ 393.01488509 -443.73893195  377.45673559 ...  517.6086033  -827.95925507\n"," -693.15024355] (12665,)\n","y hat: [1.00000000e+000 1.93477491e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30409417e-302]\n","Loss at step 662: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 662: 0.9989735491512041\n","Performing step 663 of gradient descent.\n","z [ 393.014861   -443.73889391  377.45671066 ...  517.6085488  -827.95920956\n"," -693.15019027] (12665,)\n","y hat: [1.00000000e+000 1.93484852e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30458985e-302]\n","Loss at step 663: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 663: 0.9989735491512041\n","Performing step 664 of gradient descent.\n","z [ 393.01483691 -443.73885586  377.45668573 ...  517.6084943  -827.95916405\n"," -693.150137  ] (12665,)\n","y hat: [1.00000000e+000 1.93492214e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30508555e-302]\n","Loss at step 664: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 664: 0.9989735491512041\n","Performing step 665 of gradient descent.\n","z [ 393.01481282 -443.73881781  377.4566608  ...  517.60843979 -827.95911854\n"," -693.15008373] (12665,)\n","y hat: [1.00000000e+000 1.93499576e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30558128e-302]\n","Loss at step 665: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 665: 0.9989735491512041\n","Performing step 666 of gradient descent.\n","z [ 393.01478872 -443.73877977  377.45663587 ...  517.60838529 -827.95907303\n"," -693.15003045] (12665,)\n","y hat: [1.00000000e+000 1.93506938e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30607704e-302]\n","Loss at step 666: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 666: 0.9989735491512041\n","Performing step 667 of gradient descent.\n","z [ 393.01476463 -443.73874172  377.45661095 ...  517.60833079 -827.95902751\n"," -693.14997718] (12665,)\n","y hat: [1.00000000e+000 1.93514300e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30657282e-302]\n","Loss at step 667: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 667: 0.9989735491512041\n","Performing step 668 of gradient descent.\n","z [ 393.01474054 -443.73870368  377.45658602 ...  517.60827629 -827.958982\n"," -693.1499239 ] (12665,)\n","y hat: [1.00000000e+000 1.93521663e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30706863e-302]\n","Loss at step 668: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 668: 0.9989735491512041\n","Performing step 669 of gradient descent.\n","z [ 393.01471645 -443.73866563  377.45656109 ...  517.60822179 -827.95893649\n"," -693.14987063] (12665,)\n","y hat: [1.00000000e+000 1.93529026e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30756447e-302]\n","Loss at step 669: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 669: 0.9989735491512041\n","Performing step 670 of gradient descent.\n","z [ 393.01469236 -443.73862758  377.45653616 ...  517.60816729 -827.95889098\n"," -693.14981736] (12665,)\n","y hat: [1.00000000e+000 1.93536389e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30806033e-302]\n","Loss at step 670: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 670: 0.9989735491512041\n","Performing step 671 of gradient descent.\n","z [ 393.01466826 -443.73858954  377.45651123 ...  517.60811279 -827.95884547\n"," -693.14976408] (12665,)\n","y hat: [1.00000000e+000 1.93543752e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30855622e-302]\n","Loss at step 671: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 671: 0.9989735491512041\n","Performing step 672 of gradient descent.\n","z [ 393.01464417 -443.73855149  377.4564863  ...  517.60805829 -827.95879996\n"," -693.14971081] (12665,)\n","y hat: [1.00000000e+000 1.93551116e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30905213e-302]\n","Loss at step 672: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 672: 0.9989735491512041\n","Performing step 673 of gradient descent.\n","z [ 393.01462008 -443.73851344  377.45646138 ...  517.60800379 -827.95875445\n"," -693.14965754] (12665,)\n","y hat: [1.00000000e+000 1.93558480e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.30954808e-302]\n","Loss at step 673: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 673: 0.9989735491512041\n","Performing step 674 of gradient descent.\n","z [ 393.01459599 -443.7384754   377.45643645 ...  517.60794929 -827.95870893\n"," -693.14960426] (12665,)\n","y hat: [1.00000000e+000 1.93565845e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31004405e-302]\n","Loss at step 674: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 674: 0.9989735491512041\n","Performing step 675 of gradient descent.\n","z [ 393.0145719  -443.73843735  377.45641152 ...  517.60789479 -827.95866342\n"," -693.14955099] (12665,)\n","y hat: [1.00000000e+000 1.93573209e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31054004e-302]\n","Loss at step 675: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 675: 0.9989735491512041\n","Performing step 676 of gradient descent.\n","z [ 393.0145478  -443.73839931  377.45638659 ...  517.60784029 -827.95861791\n"," -693.14949771] (12665,)\n","y hat: [1.00000000e+000 1.93580574e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31103606e-302]\n","Loss at step 676: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 676: 0.9989735491512041\n","Performing step 677 of gradient descent.\n","z [ 393.01452371 -443.73836126  377.45636166 ...  517.60778579 -827.9585724\n"," -693.14944444] (12665,)\n","y hat: [1.00000000e+000 1.93587939e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31153211e-302]\n","Loss at step 677: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 677: 0.9989735491512041\n","Performing step 678 of gradient descent.\n","z [ 393.01449962 -443.73832321  377.45633674 ...  517.60773129 -827.95852689\n"," -693.14939117] (12665,)\n","y hat: [1.00000000e+000 1.93595305e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31202819e-302]\n","Loss at step 678: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 678: 0.9989735491512041\n","Performing step 679 of gradient descent.\n","z [ 393.01447553 -443.73828517  377.45631181 ...  517.60767679 -827.95848138\n"," -693.14933789] (12665,)\n","y hat: [1.00000000e+000 1.93602670e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31252429e-302]\n","Loss at step 679: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 679: 0.9989735491512041\n","Performing step 680 of gradient descent.\n","z [ 393.01445144 -443.73824712  377.45628688 ...  517.60762229 -827.95843586\n"," -693.14928462] (12665,)\n","y hat: [1.00000000e+000 1.93610036e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31302041e-302]\n","Loss at step 680: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 680: 0.9989735491512041\n","Performing step 681 of gradient descent.\n","z [ 393.01442734 -443.73820907  377.45626195 ...  517.60756779 -827.95839035\n"," -693.14923134] (12665,)\n","y hat: [1.00000000e+000 1.93617403e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31351657e-302]\n","Loss at step 681: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 681: 0.9989735491512041\n","Performing step 682 of gradient descent.\n","z [ 393.01440325 -443.73817103  377.45623702 ...  517.60751329 -827.95834484\n"," -693.14917807] (12665,)\n","y hat: [1.00000000e+000 1.93624769e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31401275e-302]\n","Loss at step 682: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 682: 0.9989735491512041\n","Performing step 683 of gradient descent.\n","z [ 393.01437916 -443.73813298  377.45621209 ...  517.60745878 -827.95829933\n"," -693.1491248 ] (12665,)\n","y hat: [1.00000000e+000 1.93632136e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31450896e-302]\n","Loss at step 683: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 683: 0.9989735491512041\n","Performing step 684 of gradient descent.\n","z [ 393.01435507 -443.73809494  377.45618717 ...  517.60740428 -827.95825382\n"," -693.14907152] (12665,)\n","y hat: [1.00000000e+000 1.93639503e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31500519e-302]\n","Loss at step 684: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 684: 0.9989735491512041\n","Performing step 685 of gradient descent.\n","z [ 393.01433098 -443.73805689  377.45616224 ...  517.60734978 -827.95820831\n"," -693.14901825] (12665,)\n","y hat: [1.00000000e+000 1.93646870e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31550145e-302]\n","Loss at step 685: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 685: 0.9989735491512041\n","Performing step 686 of gradient descent.\n","z [ 393.01430688 -443.73801884  377.45613731 ...  517.60729528 -827.9581628\n"," -693.14896498] (12665,)\n","y hat: [1.00000000e+000 1.93654238e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31599774e-302]\n","Loss at step 686: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 686: 0.9989735491512041\n","Performing step 687 of gradient descent.\n","z [ 393.01428279 -443.7379808   377.45611238 ...  517.60724078 -827.95811728\n"," -693.1489117 ] (12665,)\n","y hat: [1.00000000e+000 1.93661606e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31649405e-302]\n","Loss at step 687: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 687: 0.9989735491512041\n","Performing step 688 of gradient descent.\n","z [ 393.0142587  -443.73794275  377.45608745 ...  517.60718628 -827.95807177\n"," -693.14885843] (12665,)\n","y hat: [1.00000000e+000 1.93668974e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31699039e-302]\n","Loss at step 688: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 688: 0.9989735491512041\n","Performing step 689 of gradient descent.\n","z [ 393.01423461 -443.7379047   377.45606253 ...  517.60713178 -827.95802626\n"," -693.14880515] (12665,)\n","y hat: [1.00000000e+000 1.93676343e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31748676e-302]\n","Loss at step 689: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 689: 0.9989735491512041\n","Performing step 690 of gradient descent.\n","z [ 393.01421052 -443.73786666  377.4560376  ...  517.60707728 -827.95798075\n"," -693.14875188] (12665,)\n","y hat: [1.00000000e+000 1.93683712e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31798315e-302]\n","Loss at step 690: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 690: 0.9989735491512041\n","Performing step 691 of gradient descent.\n","z [ 393.01418642 -443.73782861  377.45601267 ...  517.60702278 -827.95793524\n"," -693.14869861] (12665,)\n","y hat: [1.00000000e+000 1.93691081e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31847957e-302]\n","Loss at step 691: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 691: 0.9989735491512041\n","Performing step 692 of gradient descent.\n","z [ 393.01416233 -443.73779057  377.45598774 ...  517.60696828 -827.95788973\n"," -693.14864533] (12665,)\n","y hat: [1.00000000e+000 1.93698450e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31897601e-302]\n","Loss at step 692: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 692: 0.9989735491512041\n","Performing step 693 of gradient descent.\n","z [ 393.01413824 -443.73775252  377.45596281 ...  517.60691378 -827.95784421\n"," -693.14859206] (12665,)\n","y hat: [1.00000000e+000 1.93705820e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31947249e-302]\n","Loss at step 693: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 693: 0.9989735491512041\n","Performing step 694 of gradient descent.\n","z [ 393.01411415 -443.73771447  377.45593788 ...  517.60685928 -827.9577987\n"," -693.14853878] (12665,)\n","y hat: [1.00000000e+000 1.93713190e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.31996898e-302]\n","Loss at step 694: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 694: 0.9989735491512041\n","Performing step 695 of gradient descent.\n","z [ 393.01409006 -443.73767643  377.45591296 ...  517.60680478 -827.95775319\n"," -693.14848551] (12665,)\n","y hat: [1.00000000e+000 1.93720560e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32046551e-302]\n","Loss at step 695: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 695: 0.9989735491512041\n","Performing step 696 of gradient descent.\n","z [ 393.01406596 -443.73763838  377.45588803 ...  517.60675028 -827.95770768\n"," -693.14843224] (12665,)\n","y hat: [1.00000000e+000 1.93727930e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32096206e-302]\n","Loss at step 696: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 696: 0.9989735491512041\n","Performing step 697 of gradient descent.\n","z [ 393.01404187 -443.73760033  377.4558631  ...  517.60669578 -827.95766217\n"," -693.14837896] (12665,)\n","y hat: [1.00000000e+000 1.93735301e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32145864e-302]\n","Loss at step 697: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 697: 0.9989735491512041\n","Performing step 698 of gradient descent.\n","z [ 393.01401778 -443.73756229  377.45583817 ...  517.60664128 -827.95761666\n"," -693.14832569] (12665,)\n","y hat: [1.00000000e+000 1.93742672e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32195524e-302]\n","Loss at step 698: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 698: 0.9989735491512041\n","Performing step 699 of gradient descent.\n","z [ 393.01399369 -443.73752424  377.45581324 ...  517.60658678 -827.95757115\n"," -693.14827241] (12665,)\n","y hat: [1.00000000e+000 1.93750043e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32245187e-302]\n","Loss at step 699: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 699: 0.9989735491512041\n","Performing step 700 of gradient descent.\n","z [ 393.0139696  -443.7374862   377.45578831 ...  517.60653228 -827.95752563\n"," -693.14821914] (12665,)\n","y hat: [1.00000000e+000 1.93757415e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32294853e-302]\n","Loss at step 700: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 700: 0.9989735491512041\n","Performing step 701 of gradient descent.\n","z [ 393.0139455  -443.73744815  377.45576339 ...  517.60647778 -827.95748012\n"," -693.14816587] (12665,)\n","y hat: [1.00000000e+000 1.93764787e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32344522e-302]\n","Loss at step 701: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 701: 0.9989735491512041\n","Performing step 702 of gradient descent.\n","z [ 393.01392141 -443.7374101   377.45573846 ...  517.60642328 -827.95743461\n"," -693.14811259] (12665,)\n","y hat: [1.00000000e+000 1.93772159e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32394193e-302]\n","Loss at step 702: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 702: 0.9989735491512041\n","Performing step 703 of gradient descent.\n","z [ 393.01389732 -443.73737206  377.45571353 ...  517.60636878 -827.9573891\n"," -693.14805932] (12665,)\n","y hat: [1.00000000e+000 1.93779531e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32443866e-302]\n","Loss at step 703: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 703: 0.9989735491512041\n","Performing step 704 of gradient descent.\n","z [ 393.01387323 -443.73733401  377.4556886  ...  517.60631428 -827.95734359\n"," -693.14800604] (12665,)\n","y hat: [1.00000000e+000 1.93786904e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32493543e-302]\n","Loss at step 704: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 704: 0.9989735491512041\n","Performing step 705 of gradient descent.\n","z [ 393.01384914 -443.73729597  377.45566367 ...  517.60625977 -827.95729808\n"," -693.14795277] (12665,)\n","y hat: [1.00000000e+000 1.93794277e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32543222e-302]\n","Loss at step 705: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 705: 0.9989735491512041\n","Performing step 706 of gradient descent.\n","z [ 393.01382505 -443.73725792  377.45563875 ...  517.60620527 -827.95725256\n"," -693.1478995 ] (12665,)\n","y hat: [1.00000000e+000 1.93801650e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32592904e-302]\n","Loss at step 706: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 706: 0.9989735491512041\n","Performing step 707 of gradient descent.\n","z [ 393.01380095 -443.73721987  377.45561382 ...  517.60615077 -827.95720705\n"," -693.14784622] (12665,)\n","y hat: [1.00000000e+000 1.93809024e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32642588e-302]\n","Loss at step 707: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 707: 0.9989735491512041\n","Performing step 708 of gradient descent.\n","z [ 393.01377686 -443.73718183  377.45558889 ...  517.60609627 -827.95716154\n"," -693.14779295] (12665,)\n","y hat: [1.00000000e+000 1.93816398e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32692275e-302]\n","Loss at step 708: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 708: 0.9989735491512041\n","Performing step 709 of gradient descent.\n","z [ 393.01375277 -443.73714378  377.45556396 ...  517.60604177 -827.95711603\n"," -693.14773967] (12665,)\n","y hat: [1.00000000e+000 1.93823772e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32741965e-302]\n","Loss at step 709: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 709: 0.9989735491512041\n","Performing step 710 of gradient descent.\n","z [ 393.01372868 -443.73710573  377.45553903 ...  517.60598727 -827.95707052\n"," -693.1476864 ] (12665,)\n","y hat: [1.00000000e+000 1.93831146e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32791657e-302]\n","Loss at step 710: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 710: 0.9989735491512041\n","Performing step 711 of gradient descent.\n","z [ 393.01370459 -443.73706769  377.4555141  ...  517.60593277 -827.95702501\n"," -693.14763312] (12665,)\n","y hat: [1.00000000e+000 1.93838521e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32841352e-302]\n","Loss at step 711: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 711: 0.9989735491512041\n","Performing step 712 of gradient descent.\n","z [ 393.01368049 -443.73702964  377.45548918 ...  517.60587827 -827.9569795\n"," -693.14757985] (12665,)\n","y hat: [1.00000000e+000 1.93845896e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32891049e-302]\n","Loss at step 712: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 712: 0.9989735491512041\n","Performing step 713 of gradient descent.\n","z [ 393.0136564  -443.7369916   377.45546425 ...  517.60582377 -827.95693398\n"," -693.14752658] (12665,)\n","y hat: [1.00000000e+000 1.93853271e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32940750e-302]\n","Loss at step 713: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 713: 0.9989735491512041\n","Performing step 714 of gradient descent.\n","z [ 393.01363231 -443.73695355  377.45543932 ...  517.60576927 -827.95688847\n"," -693.1474733 ] (12665,)\n","y hat: [1.00000000e+000 1.93860647e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.32990453e-302]\n","Loss at step 714: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 714: 0.9989735491512041\n","Performing step 715 of gradient descent.\n","z [ 393.01360822 -443.7369155   377.45541439 ...  517.60571477 -827.95684296\n"," -693.14742003] (12665,)\n","y hat: [1.00000000e+000 1.93868022e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33040158e-302]\n","Loss at step 715: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 715: 0.9989735491512041\n","Performing step 716 of gradient descent.\n","z [ 393.01358413 -443.73687746  377.45538946 ...  517.60566027 -827.95679745\n"," -693.14736675] (12665,)\n","y hat: [1.00000000e+000 1.93875398e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33089866e-302]\n","Loss at step 716: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 716: 0.9989735491512041\n","Performing step 717 of gradient descent.\n","z [ 393.01356003 -443.73683941  377.45536454 ...  517.60560577 -827.95675194\n"," -693.14731348] (12665,)\n","y hat: [1.00000000e+000 1.93882775e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33139577e-302]\n","Loss at step 717: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 717: 0.9989735491512041\n","Performing step 718 of gradient descent.\n","z [ 393.01353594 -443.73680137  377.45533961 ...  517.60555127 -827.95670643\n"," -693.14726021] (12665,)\n","y hat: [1.00000000e+000 1.93890151e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33189291e-302]\n","Loss at step 718: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 718: 0.9989735491512041\n","Performing step 719 of gradient descent.\n","z [ 393.01351185 -443.73676332  377.45531468 ...  517.60549677 -827.95666091\n"," -693.14720693] (12665,)\n","y hat: [1.00000000e+000 1.93897528e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33239007e-302]\n","Loss at step 719: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 719: 0.9989735491512041\n","Performing step 720 of gradient descent.\n","z [ 393.01348776 -443.73672527  377.45528975 ...  517.60544227 -827.9566154\n"," -693.14715366] (12665,)\n","y hat: [1.00000000e+000 1.93904905e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33288726e-302]\n","Loss at step 720: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 720: 0.9989735491512041\n","Performing step 721 of gradient descent.\n","z [ 393.01346367 -443.73668723  377.45526482 ...  517.60538777 -827.95656989\n"," -693.14710038] (12665,)\n","y hat: [1.00000000e+000 1.93912283e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33338447e-302]\n","Loss at step 721: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 721: 0.9989735491512041\n","Performing step 722 of gradient descent.\n","z [ 393.01343957 -443.73664918  377.45523989 ...  517.60533327 -827.95652438\n"," -693.14704711] (12665,)\n","y hat: [1.00000000e+000 1.93919661e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33388171e-302]\n","Loss at step 722: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 722: 0.9989735491512041\n","Performing step 723 of gradient descent.\n","z [ 393.01341548 -443.73661113  377.45521497 ...  517.60527877 -827.95647887\n"," -693.14699384] (12665,)\n","y hat: [1.00000000e+000 1.93927039e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33437898e-302]\n","Loss at step 723: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 723: 0.9989735491512041\n","Performing step 724 of gradient descent.\n","z [ 393.01339139 -443.73657309  377.45519004 ...  517.60522427 -827.95643336\n"," -693.14694056] (12665,)\n","y hat: [1.00000000e+000 1.93934417e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33487628e-302]\n","Loss at step 724: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 724: 0.9989735491512041\n","Performing step 725 of gradient descent.\n","z [ 393.0133673  -443.73653504  377.45516511 ...  517.60516977 -827.95638785\n"," -693.14688729] (12665,)\n","y hat: [1.00000000e+000 1.93941796e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33537360e-302]\n","Loss at step 725: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 725: 0.9989735491512041\n","Performing step 726 of gradient descent.\n","z [ 393.01334321 -443.736497    377.45514018 ...  517.60511527 -827.95634233\n"," -693.14683401] (12665,)\n","y hat: [1.00000000e+000 1.93949174e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33587095e-302]\n","Loss at step 726: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 726: 0.9989735491512041\n","Performing step 727 of gradient descent.\n","z [ 393.01331912 -443.73645895  377.45511525 ...  517.60506077 -827.95629682\n"," -693.14678074] (12665,)\n","y hat: [1.00000000e+000 1.93956554e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33636832e-302]\n","Loss at step 727: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 727: 0.9989735491512041\n","Performing step 728 of gradient descent.\n","z [ 393.01329502 -443.7364209   377.45509032 ...  517.60500627 -827.95625131\n"," -693.14672746] (12665,)\n","y hat: [1.00000000e+000 1.93963933e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33686572e-302]\n","Loss at step 728: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 728: 0.9989735491512041\n","Performing step 729 of gradient descent.\n","z [ 393.01327093 -443.73638286  377.4550654  ...  517.60495177 -827.9562058\n"," -693.14667419] (12665,)\n","y hat: [1.00000000e+000 1.93971313e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33736315e-302]\n","Loss at step 729: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 729: 0.9989735491512041\n","Performing step 730 of gradient descent.\n","z [ 393.01324684 -443.73634481  377.45504047 ...  517.60489727 -827.95616029\n"," -693.14662092] (12665,)\n","y hat: [1.00000000e+000 1.93978693e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33786060e-302]\n","Loss at step 730: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 730: 0.9989735491512041\n","Performing step 731 of gradient descent.\n","z [ 393.01322275 -443.73630677  377.45501554 ...  517.60484277 -827.95611478\n"," -693.14656764] (12665,)\n","y hat: [1.00000000e+000 1.93986073e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33835808e-302]\n","Loss at step 731: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 731: 0.9989735491512041\n","Performing step 732 of gradient descent.\n","z [ 393.01319866 -443.73626872  377.45499061 ...  517.60478827 -827.95606926\n"," -693.14651437] (12665,)\n","y hat: [1.00000000e+000 1.93993454e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33885559e-302]\n","Loss at step 732: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 732: 0.9989735491512041\n","Performing step 733 of gradient descent.\n","z [ 393.01317456 -443.73623067  377.45496568 ...  517.60473377 -827.95602375\n"," -693.14646109] (12665,)\n","y hat: [1.00000000e+000 1.94000834e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33935312e-302]\n","Loss at step 733: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 733: 0.9989735491512041\n","Performing step 734 of gradient descent.\n","z [ 393.01315047 -443.73619263  377.45494075 ...  517.60467927 -827.95597824\n"," -693.14640782] (12665,)\n","y hat: [1.00000000e+000 1.94008216e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.33985068e-302]\n","Loss at step 734: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 734: 0.9989735491512041\n","Performing step 735 of gradient descent.\n","z [ 393.01312638 -443.73615458  377.45491583 ...  517.60462477 -827.95593273\n"," -693.14635454] (12665,)\n","y hat: [1.00000000e+000 1.94015597e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34034827e-302]\n","Loss at step 735: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 735: 0.9989735491512041\n","Performing step 736 of gradient descent.\n","z [ 393.01310229 -443.73611654  377.4548909  ...  517.60457027 -827.95588722\n"," -693.14630127] (12665,)\n","y hat: [1.00000000e+000 1.94022979e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34084588e-302]\n","Loss at step 736: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 736: 0.9989735491512041\n","Performing step 737 of gradient descent.\n","z [ 393.0130782  -443.73607849  377.45486597 ...  517.60451577 -827.95584171\n"," -693.146248  ] (12665,)\n","y hat: [1.00000000e+000 1.94030361e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34134352e-302]\n","Loss at step 737: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 737: 0.9989735491512041\n","Performing step 738 of gradient descent.\n","z [ 393.0130541  -443.73604044  377.45484104 ...  517.60446127 -827.9557962\n"," -693.14619472] (12665,)\n","y hat: [1.00000000e+000 1.94037743e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34184119e-302]\n","Loss at step 738: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 738: 0.9989735491512041\n","Performing step 739 of gradient descent.\n","z [ 393.01303001 -443.7360024   377.45481611 ...  517.60440676 -827.95575068\n"," -693.14614145] (12665,)\n","y hat: [1.00000000e+000 1.94045125e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34233888e-302]\n","Loss at step 739: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 739: 0.9989735491512041\n","Performing step 740 of gradient descent.\n","z [ 393.01300592 -443.73596435  377.45479119 ...  517.60435226 -827.95570517\n"," -693.14608817] (12665,)\n","y hat: [1.00000000e+000 1.94052508e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34283660e-302]\n","Loss at step 740: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 740: 0.9989735491512041\n","Performing step 741 of gradient descent.\n","z [ 393.01298183 -443.73592631  377.45476626 ...  517.60429776 -827.95565966\n"," -693.1460349 ] (12665,)\n","y hat: [1.00000000e+000 1.94059891e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34333435e-302]\n","Loss at step 741: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 741: 0.9989735491512041\n","Performing step 742 of gradient descent.\n","z [ 393.01295774 -443.73588826  377.45474133 ...  517.60424326 -827.95561415\n"," -693.14598162] (12665,)\n","y hat: [1.00000000e+000 1.94067274e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34383212e-302]\n","Loss at step 742: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 742: 0.9989735491512041\n","Performing step 743 of gradient descent.\n","z [ 393.01293365 -443.73585021  377.4547164  ...  517.60418876 -827.95556864\n"," -693.14592835] (12665,)\n","y hat: [1.00000000e+000 1.94074658e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34432992e-302]\n","Loss at step 743: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 743: 0.9989735491512041\n","Performing step 744 of gradient descent.\n","z [ 393.01290955 -443.73581217  377.45469147 ...  517.60413426 -827.95552313\n"," -693.14587508] (12665,)\n","y hat: [1.00000000e+000 1.94082042e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34482775e-302]\n","Loss at step 744: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 744: 0.9989735491512041\n","Performing step 745 of gradient descent.\n","z [ 393.01288546 -443.73577412  377.45466654 ...  517.60407976 -827.95547762\n"," -693.1458218 ] (12665,)\n","y hat: [1.00000000e+000 1.94089426e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34532560e-302]\n","Loss at step 745: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 745: 0.9989735491512041\n","Performing step 746 of gradient descent.\n","z [ 393.01286137 -443.73573608  377.45464162 ...  517.60402526 -827.9554321\n"," -693.14576853] (12665,)\n","y hat: [1.00000000e+000 1.94096811e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34582348e-302]\n","Loss at step 746: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 746: 0.9989735491512041\n","Performing step 747 of gradient descent.\n","z [ 393.01283728 -443.73569803  377.45461669 ...  517.60397076 -827.95538659\n"," -693.14571525] (12665,)\n","y hat: [1.00000000e+000 1.94104195e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34632138e-302]\n","Loss at step 747: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 747: 0.9989735491512041\n","Performing step 748 of gradient descent.\n","z [ 393.01281319 -443.73565998  377.45459176 ...  517.60391626 -827.95534108\n"," -693.14566198] (12665,)\n","y hat: [1.00000000e+000 1.94111580e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34681931e-302]\n","Loss at step 748: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 748: 0.9989735491512041\n","Performing step 749 of gradient descent.\n","z [ 393.01278909 -443.73562194  377.45456683 ...  517.60386176 -827.95529557\n"," -693.1456087 ] (12665,)\n","y hat: [1.00000000e+000 1.94118966e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34731727e-302]\n","Loss at step 749: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 749: 0.9989735491512041\n","Performing step 750 of gradient descent.\n","z [ 393.012765   -443.73558389  377.4545419  ...  517.60380726 -827.95525006\n"," -693.14555543] (12665,)\n","y hat: [1.00000000e+000 1.94126351e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34781526e-302]\n","Loss at step 750: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 750: 0.9989735491512041\n","Performing step 751 of gradient descent.\n","z [ 393.01274091 -443.73554584  377.45451697 ...  517.60375276 -827.95520455\n"," -693.14550216] (12665,)\n","y hat: [1.00000000e+000 1.94133737e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34831327e-302]\n","Loss at step 751: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 751: 0.9989735491512041\n","Performing step 752 of gradient descent.\n","z [ 393.01271682 -443.7355078   377.45449205 ...  517.60369826 -827.95515903\n"," -693.14544888] (12665,)\n","y hat: [1.00000000e+000 1.94141123e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34881131e-302]\n","Loss at step 752: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 752: 0.9989735491512041\n","Performing step 753 of gradient descent.\n","z [ 393.01269273 -443.73546975  377.45446712 ...  517.60364376 -827.95511352\n"," -693.14539561] (12665,)\n","y hat: [1.00000000e+000 1.94148510e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34930937e-302]\n","Loss at step 753: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 753: 0.9989735491512041\n","Performing step 754 of gradient descent.\n","z [ 393.01266863 -443.73543171  377.45444219 ...  517.60358926 -827.95506801\n"," -693.14534233] (12665,)\n","y hat: [1.00000000e+000 1.94155897e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.34980747e-302]\n","Loss at step 754: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 754: 0.9989735491512041\n","Performing step 755 of gradient descent.\n","z [ 393.01264454 -443.73539366  377.45441726 ...  517.60353476 -827.9550225\n"," -693.14528906] (12665,)\n","y hat: [1.00000000e+000 1.94163284e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35030558e-302]\n","Loss at step 755: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 755: 0.9989735491512041\n","Performing step 756 of gradient descent.\n","z [ 393.01262045 -443.73535561  377.45439233 ...  517.60348026 -827.95497699\n"," -693.14523578] (12665,)\n","y hat: [1.00000000e+000 1.94170671e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35080373e-302]\n","Loss at step 756: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 756: 0.9989735491512041\n","Performing step 757 of gradient descent.\n","z [ 393.01259636 -443.73531757  377.45436741 ...  517.60342576 -827.95493148\n"," -693.14518251] (12665,)\n","y hat: [1.00000000e+000 1.94178058e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35130190e-302]\n","Loss at step 757: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 757: 0.9989735491512041\n","Performing step 758 of gradient descent.\n","z [ 393.01257227 -443.73527952  377.45434248 ...  517.60337126 -827.95488597\n"," -693.14512924] (12665,)\n","y hat: [1.00000000e+000 1.94185446e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35180010e-302]\n","Loss at step 758: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 758: 0.9989735491512041\n","Performing step 759 of gradient descent.\n","z [ 393.01254818 -443.73524148  377.45431755 ...  517.60331676 -827.95484045\n"," -693.14507596] (12665,)\n","y hat: [1.00000000e+000 1.94192834e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35229832e-302]\n","Loss at step 759: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 759: 0.9989735491512041\n","Performing step 760 of gradient descent.\n","z [ 393.01252408 -443.73520343  377.45429262 ...  517.60326226 -827.95479494\n"," -693.14502269] (12665,)\n","y hat: [1.00000000e+000 1.94200223e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35279658e-302]\n","Loss at step 760: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 760: 0.9989735491512041\n","Performing step 761 of gradient descent.\n","z [ 393.01249999 -443.73516538  377.45426769 ...  517.60320776 -827.95474943\n"," -693.14496941] (12665,)\n","y hat: [1.00000000e+000 1.94207611e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35329485e-302]\n","Loss at step 761: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 761: 0.9989735491512041\n","Performing step 762 of gradient descent.\n","z [ 393.0124759  -443.73512734  377.45424276 ...  517.60315326 -827.95470392\n"," -693.14491614] (12665,)\n","y hat: [1.00000000e+000 1.94215000e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35379316e-302]\n","Loss at step 762: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 762: 0.9989735491512041\n","Performing step 763 of gradient descent.\n","z [ 393.01245181 -443.73508929  377.45421784 ...  517.60309876 -827.95465841\n"," -693.14486286] (12665,)\n","y hat: [1.00000000e+000 1.94222390e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35429149e-302]\n","Loss at step 763: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 763: 0.9989735491512041\n","Performing step 764 of gradient descent.\n","z [ 393.01242772 -443.73505125  377.45419291 ...  517.60304426 -827.9546129\n"," -693.14480959] (12665,)\n","y hat: [1.00000000e+000 1.94229779e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35478985e-302]\n","Loss at step 764: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 764: 0.9989735491512041\n","Performing step 765 of gradient descent.\n","z [ 393.01240362 -443.7350132   377.45416798 ...  517.60298976 -827.95456738\n"," -693.14475631] (12665,)\n","y hat: [1.00000000e+000 1.94237169e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35528823e-302]\n","Loss at step 765: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 765: 0.9989735491512041\n","Performing step 766 of gradient descent.\n","z [ 393.01237953 -443.73497516  377.45414305 ...  517.60293526 -827.95452187\n"," -693.14470304] (12665,)\n","y hat: [1.00000000e+000 1.94244559e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35578664e-302]\n","Loss at step 766: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 766: 0.9989735491512041\n","Performing step 767 of gradient descent.\n","z [ 393.01235544 -443.73493711  377.45411812 ...  517.60288076 -827.95447636\n"," -693.14464977] (12665,)\n","y hat: [1.00000000e+000 1.94251949e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35628508e-302]\n","Loss at step 767: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 767: 0.9989735491512041\n","Performing step 768 of gradient descent.\n","z [ 393.01233135 -443.73489906  377.45409319 ...  517.60282626 -827.95443085\n"," -693.14459649] (12665,)\n","y hat: [1.00000000e+000 1.94259340e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35678354e-302]\n","Loss at step 768: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 768: 0.9989735491512041\n","Performing step 769 of gradient descent.\n","z [ 393.01230726 -443.73486102  377.45406827 ...  517.60277176 -827.95438534\n"," -693.14454322] (12665,)\n","y hat: [1.00000000e+000 1.94266731e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35728203e-302]\n","Loss at step 769: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 769: 0.9989735491512041\n","Performing step 770 of gradient descent.\n","z [ 393.01228317 -443.73482297  377.45404334 ...  517.60271726 -827.95433983\n"," -693.14448994] (12665,)\n","y hat: [1.00000000e+000 1.94274122e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35778055e-302]\n","Loss at step 770: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 770: 0.9989735491512041\n","Performing step 771 of gradient descent.\n","z [ 393.01225907 -443.73478493  377.45401841 ...  517.60266276 -827.95429432\n"," -693.14443667] (12665,)\n","y hat: [1.00000000e+000 1.94281514e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35827910e-302]\n","Loss at step 771: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 771: 0.9989735491512041\n","Performing step 772 of gradient descent.\n","z [ 393.01223498 -443.73474688  377.45399348 ...  517.60260826 -827.9542488\n"," -693.14438339] (12665,)\n","y hat: [1.00000000e+000 1.94288905e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35877767e-302]\n","Loss at step 772: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 772: 0.9989735491512041\n","Performing step 773 of gradient descent.\n","z [ 393.01221089 -443.73470883  377.45396855 ...  517.60255376 -827.95420329\n"," -693.14433012] (12665,)\n","y hat: [1.00000000e+000 1.94296297e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35927626e-302]\n","Loss at step 773: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 773: 0.9989735491512041\n","Performing step 774 of gradient descent.\n","z [ 393.0121868  -443.73467079  377.45394362 ...  517.60249926 -827.95415778\n"," -693.14427684] (12665,)\n","y hat: [1.00000000e+000 1.94303690e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.35977489e-302]\n","Loss at step 774: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 774: 0.9989735491512041\n","Performing step 775 of gradient descent.\n","z [ 393.01216271 -443.73463274  377.4539187  ...  517.60244476 -827.95411227\n"," -693.14422357] (12665,)\n","y hat: [1.00000000e+000 1.94311082e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36027354e-302]\n","Loss at step 775: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 775: 0.9989735491512041\n","Performing step 776 of gradient descent.\n","z [ 393.01213861 -443.7345947   377.45389377 ...  517.60239026 -827.95406676\n"," -693.1441703 ] (12665,)\n","y hat: [1.00000000e+000 1.94318475e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36077222e-302]\n","Loss at step 776: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 776: 0.9989735491512041\n","Performing step 777 of gradient descent.\n","z [ 393.01211452 -443.73455665  377.45386884 ...  517.60233576 -827.95402125\n"," -693.14411702] (12665,)\n","y hat: [1.00000000e+000 1.94325868e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36127092e-302]\n","Loss at step 777: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 777: 0.9989735491512041\n","Performing step 778 of gradient descent.\n","z [ 393.01209043 -443.7345186   377.45384391 ...  517.60228126 -827.95397574\n"," -693.14406375] (12665,)\n","y hat: [1.00000000e+000 1.94333262e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36176965e-302]\n","Loss at step 778: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 778: 0.9989735491512041\n","Performing step 779 of gradient descent.\n","z [ 393.01206634 -443.73448056  377.45381898 ...  517.60222676 -827.95393022\n"," -693.14401047] (12665,)\n","y hat: [1.00000000e+000 1.94340656e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36226841e-302]\n","Loss at step 779: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 779: 0.9989735491512041\n","Performing step 780 of gradient descent.\n","z [ 393.01204225 -443.73444251  377.45379405 ...  517.60217226 -827.95388471\n"," -693.1439572 ] (12665,)\n","y hat: [1.00000000e+000 1.94348050e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36276719e-302]\n","Loss at step 780: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 780: 0.9989735491512041\n","Performing step 781 of gradient descent.\n","z [ 393.01201816 -443.73440447  377.45376913 ...  517.60211776 -827.9538392\n"," -693.14390392] (12665,)\n","y hat: [1.00000000e+000 1.94355444e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36326600e-302]\n","Loss at step 781: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 781: 0.9989735491512041\n","Performing step 782 of gradient descent.\n","z [ 393.01199406 -443.73436642  377.4537442  ...  517.60206326 -827.95379369\n"," -693.14385065] (12665,)\n","y hat: [1.00000000e+000 1.94362839e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36376484e-302]\n","Loss at step 782: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 782: 0.9989735491512041\n","Performing step 783 of gradient descent.\n","z [ 393.01196997 -443.73432837  377.45371927 ...  517.60200876 -827.95374818\n"," -693.14379737] (12665,)\n","y hat: [1.00000000e+000 1.94370233e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36426370e-302]\n","Loss at step 783: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 783: 0.9989735491512041\n","Performing step 784 of gradient descent.\n","z [ 393.01194588 -443.73429033  377.45369434 ...  517.60195426 -827.95370267\n"," -693.1437441 ] (12665,)\n","y hat: [1.00000000e+000 1.94377628e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36476259e-302]\n","Loss at step 784: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 784: 0.9989735491512041\n","Performing step 785 of gradient descent.\n","z [ 393.01192179 -443.73425228  377.45366941 ...  517.60189976 -827.95365715\n"," -693.14369082] (12665,)\n","y hat: [1.00000000e+000 1.94385024e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36526151e-302]\n","Loss at step 785: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 785: 0.9989735491512041\n","Performing step 786 of gradient descent.\n","z [ 393.0118977  -443.73421424  377.45364448 ...  517.60184526 -827.95361164\n"," -693.14363755] (12665,)\n","y hat: [1.00000000e+000 1.94392420e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36576045e-302]\n","Loss at step 786: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 786: 0.9989735491512041\n","Performing step 787 of gradient descent.\n","z [ 393.01187361 -443.73417619  377.45361956 ...  517.60179076 -827.95356613\n"," -693.14358428] (12665,)\n","y hat: [1.00000000e+000 1.94399816e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36625942e-302]\n","Loss at step 787: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 787: 0.9989735491512041\n","Performing step 788 of gradient descent.\n","z [ 393.01184951 -443.73413814  377.45359463 ...  517.60173626 -827.95352062\n"," -693.143531  ] (12665,)\n","y hat: [1.00000000e+000 1.94407212e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36675842e-302]\n","Loss at step 788: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 788: 0.9989735491512041\n","Performing step 789 of gradient descent.\n","z [ 393.01182542 -443.7341001   377.4535697  ...  517.60168176 -827.95347511\n"," -693.14347773] (12665,)\n","y hat: [1.00000000e+000 1.94414608e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36725744e-302]\n","Loss at step 789: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 789: 0.9989735491512041\n","Performing step 790 of gradient descent.\n","z [ 393.01180133 -443.73406205  377.45354477 ...  517.60162726 -827.9534296\n"," -693.14342445] (12665,)\n","y hat: [1.00000000e+000 1.94422005e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36775649e-302]\n","Loss at step 790: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 790: 0.9989735491512041\n","Performing step 791 of gradient descent.\n","z [ 393.01177724 -443.73402401  377.45351984 ...  517.60157276 -827.95338409\n"," -693.14337118] (12665,)\n","y hat: [1.00000000e+000 1.94429402e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36825557e-302]\n","Loss at step 791: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 791: 0.9989735491512041\n","Performing step 792 of gradient descent.\n","z [ 393.01175315 -443.73398596  377.45349492 ...  517.60151826 -827.95333857\n"," -693.1433179 ] (12665,)\n","y hat: [1.00000000e+000 1.94436800e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36875467e-302]\n","Loss at step 792: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 792: 0.9989735491512041\n","Performing step 793 of gradient descent.\n","z [ 393.01172905 -443.73394792  377.45346999 ...  517.60146376 -827.95329306\n"," -693.14326463] (12665,)\n","y hat: [1.00000000e+000 1.94444197e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36925380e-302]\n","Loss at step 793: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 793: 0.9989735491512041\n","Performing step 794 of gradient descent.\n","z [ 393.01170496 -443.73390987  377.45344506 ...  517.60140926 -827.95324755\n"," -693.14321135] (12665,)\n","y hat: [1.00000000e+000 1.94451595e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.36975296e-302]\n","Loss at step 794: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 794: 0.9989735491512041\n","Performing step 795 of gradient descent.\n","z [ 393.01168087 -443.73387182  377.45342013 ...  517.60135476 -827.95320204\n"," -693.14315808] (12665,)\n","y hat: [1.00000000e+000 1.94458993e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37025214e-302]\n","Loss at step 795: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 795: 0.9989735491512041\n","Performing step 796 of gradient descent.\n","z [ 393.01165678 -443.73383378  377.4533952  ...  517.60130026 -827.95315653\n"," -693.1431048 ] (12665,)\n","y hat: [1.00000000e+000 1.94466392e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37075135e-302]\n","Loss at step 796: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 796: 0.9989735491512041\n","Performing step 797 of gradient descent.\n","z [ 393.01163269 -443.73379573  377.45337027 ...  517.60124576 -827.95311102\n"," -693.14305153] (12665,)\n","y hat: [1.00000000e+000 1.94473791e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37125059e-302]\n","Loss at step 797: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 797: 0.9989735491512041\n","Performing step 798 of gradient descent.\n","z [ 393.0116086  -443.73375769  377.45334535 ...  517.60119126 -827.9530655\n"," -693.14299825] (12665,)\n","y hat: [1.00000000e+000 1.94481190e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37174985e-302]\n","Loss at step 798: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 798: 0.9989735491512041\n","Performing step 799 of gradient descent.\n","z [ 393.0115845  -443.73371964  377.45332042 ...  517.60113676 -827.95301999\n"," -693.14294498] (12665,)\n","y hat: [1.00000000e+000 1.94488589e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37224914e-302]\n","Loss at step 799: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 799: 0.9989735491512041\n","Performing step 800 of gradient descent.\n","z [ 393.01156041 -443.73368159  377.45329549 ...  517.60108226 -827.95297448\n"," -693.14289171] (12665,)\n","y hat: [1.00000000e+000 1.94495989e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37274846e-302]\n","Loss at step 800: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 800: 0.9989735491512041\n","Performing step 801 of gradient descent.\n","z [ 393.01153632 -443.73364355  377.45327056 ...  517.60102776 -827.95292897\n"," -693.14283843] (12665,)\n","y hat: [1.00000000e+000 1.94503389e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37324780e-302]\n","Loss at step 801: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 801: 0.9989735491512041\n","Performing step 802 of gradient descent.\n","z [ 393.01151223 -443.7336055   377.45324563 ...  517.60097326 -827.95288346\n"," -693.14278516] (12665,)\n","y hat: [1.00000000e+000 1.94510789e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37374717e-302]\n","Loss at step 802: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 802: 0.9989735491512041\n","Performing step 803 of gradient descent.\n","z [ 393.01148814 -443.73356746  377.4532207  ...  517.60091876 -827.95283795\n"," -693.14273188] (12665,)\n","y hat: [1.00000000e+000 1.94518189e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37424657e-302]\n","Loss at step 803: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 803: 0.9989735491512041\n","Performing step 804 of gradient descent.\n","z [ 393.01146405 -443.73352941  377.45319578 ...  517.60086426 -827.95279244\n"," -693.14267861] (12665,)\n","y hat: [1.00000000e+000 1.94525590e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37474599e-302]\n","Loss at step 804: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 804: 0.9989735491512041\n","Performing step 805 of gradient descent.\n","z [ 393.01143995 -443.73349136  377.45317085 ...  517.60080976 -827.95274692\n"," -693.14262533] (12665,)\n","y hat: [1.00000000e+000 1.94532991e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37524544e-302]\n","Loss at step 805: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 805: 0.9989735491512041\n","Performing step 806 of gradient descent.\n","z [ 393.01141586 -443.73345332  377.45314592 ...  517.60075526 -827.95270141\n"," -693.14257206] (12665,)\n","y hat: [1.00000000e+000 1.94540392e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37574492e-302]\n","Loss at step 806: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 806: 0.9989735491512041\n","Performing step 807 of gradient descent.\n","z [ 393.01139177 -443.73341527  377.45312099 ...  517.60070077 -827.9526559\n"," -693.14251878] (12665,)\n","y hat: [1.00000000e+000 1.94547794e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37624442e-302]\n","Loss at step 807: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 807: 0.9989735491512041\n","Performing step 808 of gradient descent.\n","z [ 393.01136768 -443.73337723  377.45309606 ...  517.60064627 -827.95261039\n"," -693.14246551] (12665,)\n","y hat: [1.00000000e+000 1.94555196e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37674395e-302]\n","Loss at step 808: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 808: 0.9989735491512041\n","Performing step 809 of gradient descent.\n","z [ 393.01134359 -443.73333918  377.45307113 ...  517.60059177 -827.95256488\n"," -693.14241223] (12665,)\n","y hat: [1.00000000e+000 1.94562598e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37724351e-302]\n","Loss at step 809: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 809: 0.9989735491512041\n","Performing step 810 of gradient descent.\n","z [ 393.0113195  -443.73330114  377.45304621 ...  517.60053727 -827.95251937\n"," -693.14235896] (12665,)\n","y hat: [1.00000000e+000 1.94570001e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37774309e-302]\n","Loss at step 810: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 810: 0.9989735491512041\n","Performing step 811 of gradient descent.\n","z [ 393.0112954  -443.73326309  377.45302128 ...  517.60048277 -827.95247386\n"," -693.14230568] (12665,)\n","y hat: [1.00000000e+000 1.94577403e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37824270e-302]\n","Loss at step 811: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 811: 0.9989735491512041\n","Performing step 812 of gradient descent.\n","z [ 393.01127131 -443.73322504  377.45299635 ...  517.60042827 -827.95242834\n"," -693.14225241] (12665,)\n","y hat: [1.00000000e+000 1.94584806e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37874234e-302]\n","Loss at step 812: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 812: 0.9989735491512041\n","Performing step 813 of gradient descent.\n","z [ 393.01124722 -443.733187    377.45297142 ...  517.60037377 -827.95238283\n"," -693.14219913] (12665,)\n","y hat: [1.0000000e+000 1.9459221e-193 1.0000000e+000 ... 1.0000000e+000\n"," 0.0000000e+000 9.3792420e-302]\n","Loss at step 813: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 813: 0.9989735491512041\n","Performing step 814 of gradient descent.\n","z [ 393.01122313 -443.73314895  377.45294649 ...  517.60031927 -827.95233732\n"," -693.14214586] (12665,)\n","y hat: [1.00000000e+000 1.94599613e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.37974169e-302]\n","Loss at step 814: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 814: 0.9989735491512041\n","Performing step 815 of gradient descent.\n","z [ 393.01119904 -443.73311091  377.45292156 ...  517.60026477 -827.95229181\n"," -693.14209259] (12665,)\n","y hat: [1.00000000e+000 1.94607017e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38024141e-302]\n","Loss at step 815: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 815: 0.9989735491512041\n","Performing step 816 of gradient descent.\n","z [ 393.01117494 -443.73307286  377.45289664 ...  517.60021027 -827.9522463\n"," -693.14203931] (12665,)\n","y hat: [1.00000000e+000 1.94614421e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38074115e-302]\n","Loss at step 816: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 816: 0.9989735491512041\n","Performing step 817 of gradient descent.\n","z [ 393.01115085 -443.73303481  377.45287171 ...  517.60015577 -827.95220079\n"," -693.14198604] (12665,)\n","y hat: [1.00000000e+000 1.94621825e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38124092e-302]\n","Loss at step 817: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 817: 0.9989735491512041\n","Performing step 818 of gradient descent.\n","z [ 393.01112676 -443.73299677  377.45284678 ...  517.60010127 -827.95215527\n"," -693.14193276] (12665,)\n","y hat: [1.00000000e+000 1.94629230e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38174072e-302]\n","Loss at step 818: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 818: 0.9989735491512041\n","Performing step 819 of gradient descent.\n","z [ 393.01110267 -443.73295872  377.45282185 ...  517.60004677 -827.95210976\n"," -693.14187949] (12665,)\n","y hat: [1.00000000e+000 1.94636635e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38224054e-302]\n","Loss at step 819: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 819: 0.9989735491512041\n","Performing step 820 of gradient descent.\n","z [ 393.01107858 -443.73292068  377.45279692 ...  517.59999227 -827.95206425\n"," -693.14182621] (12665,)\n","y hat: [1.00000000e+000 1.94644040e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38274039e-302]\n","Loss at step 820: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 820: 0.9989735491512041\n","Performing step 821 of gradient descent.\n","z [ 393.01105449 -443.73288263  377.45277199 ...  517.59993777 -827.95201874\n"," -693.14177294] (12665,)\n","y hat: [1.00000000e+000 1.94651446e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38324027e-302]\n","Loss at step 821: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 821: 0.9989735491512041\n","Performing step 822 of gradient descent.\n","z [ 393.01103039 -443.73284459  377.45274707 ...  517.59988327 -827.95197323\n"," -693.14171966] (12665,)\n","y hat: [1.00000000e+000 1.94658852e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38374017e-302]\n","Loss at step 822: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 822: 0.9989735491512041\n","Performing step 823 of gradient descent.\n","z [ 393.0110063  -443.73280654  377.45272214 ...  517.59982877 -827.95192772\n"," -693.14166639] (12665,)\n","y hat: [1.00000000e+000 1.94666258e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38424010e-302]\n","Loss at step 823: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 823: 0.9989735491512041\n","Performing step 824 of gradient descent.\n","z [ 393.01098221 -443.73276849  377.45269721 ...  517.59977427 -827.95188221\n"," -693.14161311] (12665,)\n","y hat: [1.00000000e+000 1.94673664e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38474006e-302]\n","Loss at step 824: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 824: 0.9989735491512041\n","Performing step 825 of gradient descent.\n","z [ 393.01095812 -443.73273045  377.45267228 ...  517.59971977 -827.95183669\n"," -693.14155984] (12665,)\n","y hat: [1.00000000e+000 1.94681071e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38524004e-302]\n","Loss at step 825: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 825: 0.9989735491512041\n","Performing step 826 of gradient descent.\n","z [ 393.01093403 -443.7326924   377.45264735 ...  517.59966527 -827.95179118\n"," -693.14150656] (12665,)\n","y hat: [1.00000000e+000 1.94688478e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38574005e-302]\n","Loss at step 826: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 826: 0.9989735491512041\n","Performing step 827 of gradient descent.\n","z [ 393.01090994 -443.73265436  377.45262242 ...  517.59961077 -827.95174567\n"," -693.14145329] (12665,)\n","y hat: [1.00000000e+000 1.94695885e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38624009e-302]\n","Loss at step 827: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 827: 0.9989735491512041\n","Performing step 828 of gradient descent.\n","z [ 393.01088584 -443.73261631  377.4525975  ...  517.59955627 -827.95170016\n"," -693.14140001] (12665,)\n","y hat: [1.00000000e+000 1.94703292e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38674015e-302]\n","Loss at step 828: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 828: 0.9989735491512041\n","Performing step 829 of gradient descent.\n","z [ 393.01086175 -443.73257827  377.45257257 ...  517.59950177 -827.95165465\n"," -693.14134674] (12665,)\n","y hat: [1.00000000e+000 1.94710700e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38724024e-302]\n","Loss at step 829: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 829: 0.9989735491512041\n","Performing step 830 of gradient descent.\n","z [ 393.01083766 -443.73254022  377.45254764 ...  517.59944727 -827.95160914\n"," -693.14129346] (12665,)\n","y hat: [1.00000000e+000 1.94718108e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38774036e-302]\n","Loss at step 830: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 830: 0.9989735491512041\n","Performing step 831 of gradient descent.\n","z [ 393.01081357 -443.73250217  377.45252271 ...  517.59939277 -827.95156363\n"," -693.14124019] (12665,)\n","y hat: [1.00000000e+000 1.94725517e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38824050e-302]\n","Loss at step 831: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 831: 0.9989735491512041\n","Performing step 832 of gradient descent.\n","z [ 393.01078948 -443.73246413  377.45249778 ...  517.59933827 -827.95151811\n"," -693.14118691] (12665,)\n","y hat: [1.00000000e+000 1.94732925e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38874067e-302]\n","Loss at step 832: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 832: 0.9989735491512041\n","Performing step 833 of gradient descent.\n","z [ 393.01076539 -443.73242608  377.45247285 ...  517.59928377 -827.9514726\n"," -693.14113364] (12665,)\n","y hat: [1.00000000e+000 1.94740334e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38924087e-302]\n","Loss at step 833: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 833: 0.9989735491512041\n","Performing step 834 of gradient descent.\n","z [ 393.01074129 -443.73238804  377.45244793 ...  517.59922927 -827.95142709\n"," -693.14108036] (12665,)\n","y hat: [1.00000000e+000 1.94747743e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.38974109e-302]\n","Loss at step 834: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 834: 0.9989735491512041\n","Performing step 835 of gradient descent.\n","z [ 393.0107172  -443.73234999  377.452423   ...  517.59917477 -827.95138158\n"," -693.14102709] (12665,)\n","y hat: [1.00000000e+000 1.94755153e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39024134e-302]\n","Loss at step 835: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 835: 0.9989735491512041\n","Performing step 836 of gradient descent.\n","z [ 393.01069311 -443.73231195  377.45239807 ...  517.59912027 -827.95133607\n"," -693.14097381] (12665,)\n","y hat: [1.00000000e+000 1.94762563e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39074162e-302]\n","Loss at step 836: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 836: 0.9989735491512041\n","Performing step 837 of gradient descent.\n","z [ 393.01066902 -443.7322739   377.45237314 ...  517.59906577 -827.95129056\n"," -693.14092054] (12665,)\n","y hat: [1.00000000e+000 1.94769973e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39124193e-302]\n","Loss at step 837: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 837: 0.9989735491512041\n","Performing step 838 of gradient descent.\n","z [ 393.01064493 -443.73223585  377.45234821 ...  517.59901127 -827.95124504\n"," -693.14086726] (12665,)\n","y hat: [1.00000000e+000 1.94777383e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39174226e-302]\n","Loss at step 838: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 838: 0.9989735491512041\n","Performing step 839 of gradient descent.\n","z [ 393.01062084 -443.73219781  377.45232328 ...  517.59895677 -827.95119953\n"," -693.14081399] (12665,)\n","y hat: [1.00000000e+000 1.94784793e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39224261e-302]\n","Loss at step 839: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 839: 0.9989735491512041\n","Performing step 840 of gradient descent.\n","z [ 393.01059674 -443.73215976  377.45229836 ...  517.59890227 -827.95115402\n"," -693.14076071] (12665,)\n","y hat: [1.00000000e+000 1.94792204e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39274300e-302]\n","Loss at step 840: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 840: 0.9989735491512041\n","Performing step 841 of gradient descent.\n","z [ 393.01057265 -443.73212172  377.45227343 ...  517.59884778 -827.95110851\n"," -693.14070744] (12665,)\n","y hat: [1.00000000e+000 1.94799615e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39324341e-302]\n","Loss at step 841: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 841: 0.9989735491512041\n","Performing step 842 of gradient descent.\n","z [ 393.01054856 -443.73208367  377.4522485  ...  517.59879328 -827.951063\n"," -693.14065416] (12665,)\n","y hat: [1.00000000e+000 1.94807027e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39374385e-302]\n","Loss at step 842: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 842: 0.9989735491512041\n","Performing step 843 of gradient descent.\n","z [ 393.01052447 -443.73204563  377.45222357 ...  517.59873878 -827.95101749\n"," -693.14060089] (12665,)\n","y hat: [1.00000000e+000 1.94814439e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39424431e-302]\n","Loss at step 843: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 843: 0.9989735491512041\n","Performing step 844 of gradient descent.\n","z [ 393.01050038 -443.73200758  377.45219864 ...  517.59868428 -827.95097198\n"," -693.14054761] (12665,)\n","y hat: [1.00000000e+000 1.94821851e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39474480e-302]\n","Loss at step 844: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 844: 0.9989735491512041\n","Performing step 845 of gradient descent.\n","z [ 393.01047629 -443.73196953  377.45217371 ...  517.59862978 -827.95092646\n"," -693.14049434] (12665,)\n","y hat: [1.00000000e+000 1.94829263e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39524532e-302]\n","Loss at step 845: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 845: 0.9989735491512041\n","Performing step 846 of gradient descent.\n","z [ 393.01045219 -443.73193149  377.45214879 ...  517.59857528 -827.95088095\n"," -693.14044107] (12665,)\n","y hat: [1.00000000e+000 1.94836675e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39574586e-302]\n","Loss at step 846: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 846: 0.9989735491512041\n","Performing step 847 of gradient descent.\n","z [ 393.0104281  -443.73189344  377.45212386 ...  517.59852078 -827.95083544\n"," -693.14038779] (12665,)\n","y hat: [1.00000000e+000 1.94844088e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39624644e-302]\n","Loss at step 847: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 847: 0.9989735491512041\n","Performing step 848 of gradient descent.\n","z [ 393.01040401 -443.7318554   377.45209893 ...  517.59846628 -827.95078993\n"," -693.14033452] (12665,)\n","y hat: [1.00000000e+000 1.94851501e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39674703e-302]\n","Loss at step 848: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 848: 0.9989735491512041\n","Performing step 849 of gradient descent.\n","z [ 393.01037992 -443.73181735  377.452074   ...  517.59841178 -827.95074442\n"," -693.14028124] (12665,)\n","y hat: [1.00000000e+000 1.94858915e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39724766e-302]\n","Loss at step 849: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 849: 0.9989735491512041\n","Performing step 850 of gradient descent.\n","z [ 393.01035583 -443.73177931  377.45204907 ...  517.59835728 -827.95069891\n"," -693.14022797] (12665,)\n","y hat: [1.00000000e+000 1.94866329e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39774831e-302]\n","Loss at step 850: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 850: 0.9989735491512041\n","Performing step 851 of gradient descent.\n","z [ 393.01033174 -443.73174126  377.45202414 ...  517.59830278 -827.9506534\n"," -693.14017469] (12665,)\n","y hat: [1.00000000e+000 1.94873742e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39824899e-302]\n","Loss at step 851: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 851: 0.9989735491512041\n","Performing step 852 of gradient descent.\n","z [ 393.01030764 -443.73170321  377.45199922 ...  517.59824828 -827.95060788\n"," -693.14012142] (12665,)\n","y hat: [1.00000000e+000 1.94881157e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39874969e-302]\n","Loss at step 852: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 852: 0.9989735491512041\n","Performing step 853 of gradient descent.\n","z [ 393.01028355 -443.73166517  377.45197429 ...  517.59819378 -827.95056237\n"," -693.14006814] (12665,)\n","y hat: [1.00000000e+000 1.94888571e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39925042e-302]\n","Loss at step 853: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 853: 0.9989735491512041\n","Performing step 854 of gradient descent.\n","z [ 393.01025946 -443.73162712  377.45194936 ...  517.59813928 -827.95051686\n"," -693.14001487] (12665,)\n","y hat: [1.00000000e+000 1.94895986e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.39975118e-302]\n","Loss at step 854: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 854: 0.9989735491512041\n","Performing step 855 of gradient descent.\n","z [ 393.01023537 -443.73158908  377.45192443 ...  517.59808478 -827.95047135\n"," -693.13996159] (12665,)\n","y hat: [1.00000000e+000 1.94903401e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40025197e-302]\n","Loss at step 855: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 855: 0.9989735491512041\n","Performing step 856 of gradient descent.\n","z [ 393.01021128 -443.73155103  377.4518995  ...  517.59803028 -827.95042584\n"," -693.13990832] (12665,)\n","y hat: [1.00000000e+000 1.94910817e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40075278e-302]\n","Loss at step 856: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 856: 0.9989735491512041\n","Performing step 857 of gradient descent.\n","z [ 393.01018719 -443.73151299  377.45187457 ...  517.59797578 -827.95038033\n"," -693.13985504] (12665,)\n","y hat: [1.00000000e+000 1.94918232e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40125362e-302]\n","Loss at step 857: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 857: 0.9989735491512041\n","Performing step 858 of gradient descent.\n","z [ 393.01016309 -443.73147494  377.45184965 ...  517.59792128 -827.95033482\n"," -693.13980177] (12665,)\n","y hat: [1.00000000e+000 1.94925648e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40175448e-302]\n","Loss at step 858: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 858: 0.9989735491512041\n","Performing step 859 of gradient descent.\n","z [ 393.010139   -443.73143689  377.45182472 ...  517.59786678 -827.9502893\n"," -693.13974849] (12665,)\n","y hat: [1.00000000e+000 1.94933064e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40225537e-302]\n","Loss at step 859: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 859: 0.9989735491512041\n","Performing step 860 of gradient descent.\n","z [ 393.01011491 -443.73139885  377.45179979 ...  517.59781228 -827.95024379\n"," -693.13969522] (12665,)\n","y hat: [1.00000000e+000 1.94940481e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40275629e-302]\n","Loss at step 860: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 860: 0.9989735491512041\n","Performing step 861 of gradient descent.\n","z [ 393.01009082 -443.7313608   377.45177486 ...  517.59775778 -827.95019828\n"," -693.13964194] (12665,)\n","y hat: [1.00000000e+000 1.94947898e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40325724e-302]\n","Loss at step 861: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 861: 0.9989735491512041\n","Performing step 862 of gradient descent.\n","z [ 393.01006673 -443.73132276  377.45174993 ...  517.59770328 -827.95015277\n"," -693.13958867] (12665,)\n","y hat: [1.00000000e+000 1.94955315e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40375821e-302]\n","Loss at step 862: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 862: 0.9989735491512041\n","Performing step 863 of gradient descent.\n","z [ 393.01004264 -443.73128471  377.451725   ...  517.59764879 -827.95010726\n"," -693.13953539] (12665,)\n","y hat: [1.00000000e+000 1.94962732e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40425921e-302]\n","Loss at step 863: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 863: 0.9989735491512041\n","Performing step 864 of gradient descent.\n","z [ 393.01001855 -443.73124667  377.45170008 ...  517.59759429 -827.95006175\n"," -693.13948212] (12665,)\n","y hat: [1.00000000e+000 1.94970150e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40476024e-302]\n","Loss at step 864: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 864: 0.9989735491512041\n","Performing step 865 of gradient descent.\n","z [ 393.00999445 -443.73120862  377.45167515 ...  517.59753979 -827.95001623\n"," -693.13942884] (12665,)\n","y hat: [1.00000000e+000 1.94977568e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40526129e-302]\n","Loss at step 865: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 865: 0.9989735491512041\n","Performing step 866 of gradient descent.\n","z [ 393.00997036 -443.73117057  377.45165022 ...  517.59748529 -827.94997072\n"," -693.13937556] (12665,)\n","y hat: [1.00000000e+000 1.94984986e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40576237e-302]\n","Loss at step 866: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 866: 0.9989735491512041\n","Performing step 867 of gradient descent.\n","z [ 393.00994627 -443.73113253  377.45162529 ...  517.59743079 -827.94992521\n"," -693.13932229] (12665,)\n","y hat: [1.00000000e+000 1.94992404e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40626347e-302]\n","Loss at step 867: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 867: 0.9989735491512041\n","Performing step 868 of gradient descent.\n","z [ 393.00992218 -443.73109448  377.45160036 ...  517.59737629 -827.9498797\n"," -693.13926901] (12665,)\n","y hat: [1.00000000e+000 1.94999823e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40676461e-302]\n","Loss at step 868: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 868: 0.9989735491512041\n","Performing step 869 of gradient descent.\n","z [ 393.00989809 -443.73105644  377.45157543 ...  517.59732179 -827.94983419\n"," -693.13921574] (12665,)\n","y hat: [1.00000000e+000 1.95007242e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40726577e-302]\n","Loss at step 869: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 869: 0.9989735491512041\n","Performing step 870 of gradient descent.\n","z [ 393.009874   -443.73101839  377.45155051 ...  517.59726729 -827.94978868\n"," -693.13916246] (12665,)\n","y hat: [1.00000000e+000 1.95014661e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40776695e-302]\n","Loss at step 870: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 870: 0.9989735491512041\n","Performing step 871 of gradient descent.\n","z [ 393.0098499  -443.73098035  377.45152558 ...  517.59721279 -827.94974317\n"," -693.13910919] (12665,)\n","y hat: [1.00000000e+000 1.95022081e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40826816e-302]\n","Loss at step 871: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 871: 0.9989735491512041\n","Performing step 872 of gradient descent.\n","z [ 393.00982581 -443.7309423   377.45150065 ...  517.59715829 -827.94969765\n"," -693.13905591] (12665,)\n","y hat: [1.00000000e+000 1.95029501e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40876940e-302]\n","Loss at step 872: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 872: 0.9989735491512041\n","Performing step 873 of gradient descent.\n","z [ 393.00980172 -443.73090425  377.45147572 ...  517.59710379 -827.94965214\n"," -693.13900264] (12665,)\n","y hat: [1.00000000e+000 1.95036921e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40927067e-302]\n","Loss at step 873: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 873: 0.9989735491512041\n","Performing step 874 of gradient descent.\n","z [ 393.00977763 -443.73086621  377.45145079 ...  517.59704929 -827.94960663\n"," -693.13894936] (12665,)\n","y hat: [1.00000000e+000 1.95044341e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.40977196e-302]\n","Loss at step 874: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 874: 0.9989735491512041\n","Performing step 875 of gradient descent.\n","z [ 393.00975354 -443.73082816  377.45142586 ...  517.59699479 -827.94956112\n"," -693.13889609] (12665,)\n","y hat: [1.00000000e+000 1.95051762e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41027328e-302]\n","Loss at step 875: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 875: 0.9989735491512041\n","Performing step 876 of gradient descent.\n","z [ 393.00972945 -443.73079012  377.45140093 ...  517.59694029 -827.94951561\n"," -693.13884281] (12665,)\n","y hat: [1.00000000e+000 1.95059183e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41077463e-302]\n","Loss at step 876: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 876: 0.9989735491512041\n","Performing step 877 of gradient descent.\n","z [ 393.00970535 -443.73075207  377.45137601 ...  517.59688579 -827.9494701\n"," -693.13878954] (12665,)\n","y hat: [1.00000000e+000 1.95066604e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41127600e-302]\n","Loss at step 877: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 877: 0.9989735491512041\n","Performing step 878 of gradient descent.\n","z [ 393.00968126 -443.73071403  377.45135108 ...  517.59683129 -827.94942459\n"," -693.13873626] (12665,)\n","y hat: [1.00000000e+000 1.95074026e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41177740e-302]\n","Loss at step 878: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 878: 0.9989735491512041\n","Performing step 879 of gradient descent.\n","z [ 393.00965717 -443.73067598  377.45132615 ...  517.59677679 -827.94937907\n"," -693.13868299] (12665,)\n","y hat: [1.00000000e+000 1.95081448e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41227883e-302]\n","Loss at step 879: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 879: 0.9989735491512041\n","Performing step 880 of gradient descent.\n","z [ 393.00963308 -443.73063794  377.45130122 ...  517.59672229 -827.94933356\n"," -693.13862971] (12665,)\n","y hat: [1.00000000e+000 1.95088870e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41278029e-302]\n","Loss at step 880: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 880: 0.9989735491512041\n","Performing step 881 of gradient descent.\n","z [ 393.00960899 -443.73059989  377.45127629 ...  517.5966678  -827.94928805\n"," -693.13857644] (12665,)\n","y hat: [1.00000000e+000 1.95096292e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41328177e-302]\n","Loss at step 881: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 881: 0.9989735491512041\n","Performing step 882 of gradient descent.\n","z [ 393.0095849  -443.73056184  377.45125136 ...  517.5966133  -827.94924254\n"," -693.13852316] (12665,)\n","y hat: [1.00000000e+000 1.95103715e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41378327e-302]\n","Loss at step 882: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 882: 0.9989735491512041\n","Performing step 883 of gradient descent.\n","z [ 393.0095608  -443.7305238   377.45122644 ...  517.5965588  -827.94919703\n"," -693.13846989] (12665,)\n","y hat: [1.00000000e+000 1.95111138e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41428481e-302]\n","Loss at step 883: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 883: 0.9989735491512041\n","Performing step 884 of gradient descent.\n","z [ 393.00953671 -443.73048575  377.45120151 ...  517.5965043  -827.94915152\n"," -693.13841661] (12665,)\n","y hat: [1.00000000e+000 1.95118561e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41478637e-302]\n","Loss at step 884: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 884: 0.9989735491512041\n","Performing step 885 of gradient descent.\n","z [ 393.00951262 -443.73044771  377.45117658 ...  517.5964498  -827.949106\n"," -693.13836334] (12665,)\n","y hat: [1.00000000e+000 1.95125985e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41528796e-302]\n","Loss at step 885: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 885: 0.9989735491512041\n","Performing step 886 of gradient descent.\n","z [ 393.00948853 -443.73040966  377.45115165 ...  517.5963953  -827.94906049\n"," -693.13831006] (12665,)\n","y hat: [1.00000000e+000 1.95133409e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41578957e-302]\n","Loss at step 886: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 886: 0.9989735491512041\n","Performing step 887 of gradient descent.\n","z [ 393.00946444 -443.73037162  377.45112672 ...  517.5963408  -827.94901498\n"," -693.13825679] (12665,)\n","y hat: [1.00000000e+000 1.95140833e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41629121e-302]\n","Loss at step 887: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 887: 0.9989735491512041\n","Performing step 888 of gradient descent.\n","z [ 393.00944035 -443.73033357  377.45110179 ...  517.5962863  -827.94896947\n"," -693.13820351] (12665,)\n","y hat: [1.00000000e+000 1.95148257e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41679288e-302]\n","Loss at step 888: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 888: 0.9989735491512041\n","Performing step 889 of gradient descent.\n","z [ 393.00941626 -443.73029553  377.45107687 ...  517.5962318  -827.94892396\n"," -693.13815024] (12665,)\n","y hat: [1.00000000e+000 1.95155682e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41729458e-302]\n","Loss at step 889: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 889: 0.9989735491512041\n","Performing step 890 of gradient descent.\n","z [ 393.00939216 -443.73025748  377.45105194 ...  517.5961773  -827.94887845\n"," -693.13809696] (12665,)\n","y hat: [1.00000000e+000 1.95163107e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41779630e-302]\n","Loss at step 890: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 890: 0.9989735491512041\n","Performing step 891 of gradient descent.\n","z [ 393.00936807 -443.73021943  377.45102701 ...  517.5961228  -827.94883294\n"," -693.13804369] (12665,)\n","y hat: [1.00000000e+000 1.95170532e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41829805e-302]\n","Loss at step 891: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 891: 0.9989735491512041\n","Performing step 892 of gradient descent.\n","z [ 393.00934398 -443.73018139  377.45100208 ...  517.5960683  -827.94878742\n"," -693.13799041] (12665,)\n","y hat: [1.00000000e+000 1.95177958e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41879982e-302]\n","Loss at step 892: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 892: 0.9989735491512041\n","Performing step 893 of gradient descent.\n","z [ 393.00931989 -443.73014334  377.45097715 ...  517.5960138  -827.94874191\n"," -693.13793714] (12665,)\n","y hat: [1.00000000e+000 1.95185383e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41930162e-302]\n","Loss at step 893: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 893: 0.9989735491512041\n","Performing step 894 of gradient descent.\n","z [ 393.0092958  -443.7301053   377.45095222 ...  517.5959593  -827.9486964\n"," -693.13788386] (12665,)\n","y hat: [1.00000000e+000 1.95192809e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.41980345e-302]\n","Loss at step 894: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 894: 0.9989735491512041\n","Performing step 895 of gradient descent.\n","z [ 393.00927171 -443.73006725  377.4509273  ...  517.5959048  -827.94865089\n"," -693.13783059] (12665,)\n","y hat: [1.00000000e+000 1.95200236e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42030531e-302]\n","Loss at step 895: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 895: 0.9989735491512041\n","Performing step 896 of gradient descent.\n","z [ 393.00924761 -443.73002921  377.45090237 ...  517.59585031 -827.94860538\n"," -693.13777731] (12665,)\n","y hat: [1.00000000e+000 1.95207662e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42080719e-302]\n","Loss at step 896: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 896: 0.9989735491512041\n","Performing step 897 of gradient descent.\n","z [ 393.00922352 -443.72999116  377.45087744 ...  517.59579581 -827.94855987\n"," -693.13772403] (12665,)\n","y hat: [1.00000000e+000 1.95215089e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42130910e-302]\n","Loss at step 897: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 897: 0.9989735491512041\n","Performing step 898 of gradient descent.\n","z [ 393.00919943 -443.72995312  377.45085251 ...  517.59574131 -827.94851436\n"," -693.13767076] (12665,)\n","y hat: [1.00000000e+000 1.95222517e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42181104e-302]\n","Loss at step 898: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 898: 0.9989735491512041\n","Performing step 899 of gradient descent.\n","z [ 393.00917534 -443.72991507  377.45082758 ...  517.59568681 -827.94846884\n"," -693.13761748] (12665,)\n","y hat: [1.00000000e+000 1.95229944e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42231300e-302]\n","Loss at step 899: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 899: 0.9989735491512041\n","Performing step 900 of gradient descent.\n","z [ 393.00915125 -443.72987702  377.45080265 ...  517.59563231 -827.94842333\n"," -693.13756421] (12665,)\n","y hat: [1.00000000e+000 1.95237372e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42281499e-302]\n","Loss at step 900: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 900: 0.9989735491512041\n","Performing step 901 of gradient descent.\n","z [ 393.00912716 -443.72983898  377.45077773 ...  517.59557781 -827.94837782\n"," -693.13751093] (12665,)\n","y hat: [1.00000000e+000 1.95244800e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42331701e-302]\n","Loss at step 901: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 901: 0.9989735491512041\n","Performing step 902 of gradient descent.\n","z [ 393.00910307 -443.72980093  377.4507528  ...  517.59552331 -827.94833231\n"," -693.13745766] (12665,)\n","y hat: [1.00000000e+000 1.95252228e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42381905e-302]\n","Loss at step 902: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 902: 0.9989735491512041\n","Performing step 903 of gradient descent.\n","z [ 393.00907897 -443.72976289  377.45072787 ...  517.59546881 -827.9482868\n"," -693.13740438] (12665,)\n","y hat: [1.00000000e+000 1.95259657e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42432112e-302]\n","Loss at step 903: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 903: 0.9989735491512041\n","Performing step 904 of gradient descent.\n","z [ 393.00905488 -443.72972484  377.45070294 ...  517.59541431 -827.94824129\n"," -693.13735111] (12665,)\n","y hat: [1.00000000e+000 1.95267086e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42482322e-302]\n","Loss at step 904: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 904: 0.9989735491512041\n","Performing step 905 of gradient descent.\n","z [ 393.00903079 -443.7296868   377.45067801 ...  517.59535981 -827.94819578\n"," -693.13729783] (12665,)\n","y hat: [1.00000000e+000 1.95274515e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42532534e-302]\n","Loss at step 905: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 905: 0.9989735491512041\n","Performing step 906 of gradient descent.\n","z [ 393.0090067  -443.72964875  377.45065308 ...  517.59530531 -827.94815026\n"," -693.13724456] (12665,)\n","y hat: [1.00000000e+000 1.95281944e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42582749e-302]\n","Loss at step 906: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 906: 0.9989735491512041\n","Performing step 907 of gradient descent.\n","z [ 393.00898261 -443.72961071  377.45062815 ...  517.59525081 -827.94810475\n"," -693.13719128] (12665,)\n","y hat: [1.00000000e+000 1.95289374e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42632967e-302]\n","Loss at step 907: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 907: 0.9989735491512041\n","Performing step 908 of gradient descent.\n","z [ 393.00895852 -443.72957266  377.45060323 ...  517.59519631 -827.94805924\n"," -693.13713801] (12665,)\n","y hat: [1.00000000e+000 1.95296804e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42683187e-302]\n","Loss at step 908: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 908: 0.9989735491512041\n","Performing step 909 of gradient descent.\n","z [ 393.00893442 -443.72953461  377.4505783  ...  517.59514181 -827.94801373\n"," -693.13708473] (12665,)\n","y hat: [1.00000000e+000 1.95304234e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42733410e-302]\n","Loss at step 909: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 909: 0.9989735491512041\n","Performing step 910 of gradient descent.\n","z [ 393.00891033 -443.72949657  377.45055337 ...  517.59508732 -827.94796822\n"," -693.13703146] (12665,)\n","y hat: [1.00000000e+000 1.95311665e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42783636e-302]\n","Loss at step 910: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 910: 0.9989735491512041\n","Performing step 911 of gradient descent.\n","z [ 393.00888624 -443.72945852  377.45052844 ...  517.59503282 -827.94792271\n"," -693.13697818] (12665,)\n","y hat: [1.00000000e+000 1.95319096e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42833864e-302]\n","Loss at step 911: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 911: 0.9989735491512041\n","Performing step 912 of gradient descent.\n","z [ 393.00886215 -443.72942048  377.45050351 ...  517.59497832 -827.9478772\n"," -693.13692491] (12665,)\n","y hat: [1.00000000e+000 1.95326527e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42884096e-302]\n","Loss at step 912: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 912: 0.9989735491512041\n","Performing step 913 of gradient descent.\n","z [ 393.00883806 -443.72938243  377.45047858 ...  517.59492382 -827.94783168\n"," -693.13687163] (12665,)\n","y hat: [1.00000000e+000 1.95333959e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42934329e-302]\n","Loss at step 913: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 913: 0.9989735491512041\n","Performing step 914 of gradient descent.\n","z [ 393.00881397 -443.72934439  377.45045366 ...  517.59486932 -827.94778617\n"," -693.13681835] (12665,)\n","y hat: [1.00000000e+000 1.95341390e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.42984566e-302]\n","Loss at step 914: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 914: 0.9989735491512041\n","Performing step 915 of gradient descent.\n","z [ 393.00878988 -443.72930634  377.45042873 ...  517.59481482 -827.94774066\n"," -693.13676508] (12665,)\n","y hat: [1.00000000e+000 1.95348822e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43034805e-302]\n","Loss at step 915: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 915: 0.9989735491512041\n","Performing step 916 of gradient descent.\n","z [ 393.00876578 -443.7292683   377.4504038  ...  517.59476032 -827.94769515\n"," -693.1367118 ] (12665,)\n","y hat: [1.00000000e+000 1.95356255e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43085047e-302]\n","Loss at step 916: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 916: 0.9989735491512041\n","Performing step 917 of gradient descent.\n","z [ 393.00874169 -443.72923025  377.45037887 ...  517.59470582 -827.94764964\n"," -693.13665853] (12665,)\n","y hat: [1.00000000e+000 1.95363687e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43135292e-302]\n","Loss at step 917: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 917: 0.9989735491512041\n","Performing step 918 of gradient descent.\n","z [ 393.0087176  -443.7291922   377.45035394 ...  517.59465132 -827.94760413\n"," -693.13660525] (12665,)\n","y hat: [1.00000000e+000 1.95371120e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43185539e-302]\n","Loss at step 918: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 918: 0.9989735491512041\n","Performing step 919 of gradient descent.\n","z [ 393.00869351 -443.72915416  377.45032901 ...  517.59459682 -827.94755861\n"," -693.13655198] (12665,)\n","y hat: [1.00000000e+000 1.95378553e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43235789e-302]\n","Loss at step 919: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 919: 0.9989735491512041\n","Performing step 920 of gradient descent.\n","z [ 393.00866942 -443.72911611  377.45030409 ...  517.59454232 -827.9475131\n"," -693.1364987 ] (12665,)\n","y hat: [1.00000000e+000 1.95385986e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43286041e-302]\n","Loss at step 920: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 920: 0.9989735491512041\n","Performing step 921 of gradient descent.\n","z [ 393.00864533 -443.72907807  377.45027916 ...  517.59448782 -827.94746759\n"," -693.13644543] (12665,)\n","y hat: [1.00000000e+000 1.95393420e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43336297e-302]\n","Loss at step 921: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 921: 0.9989735491512041\n","Performing step 922 of gradient descent.\n","z [ 393.00862123 -443.72904002  377.45025423 ...  517.59443333 -827.94742208\n"," -693.13639215] (12665,)\n","y hat: [1.00000000e+000 1.95400854e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43386555e-302]\n","Loss at step 922: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 922: 0.9989735491512041\n","Performing step 923 of gradient descent.\n","z [ 393.00859714 -443.72900198  377.4502293  ...  517.59437883 -827.94737657\n"," -693.13633888] (12665,)\n","y hat: [1.00000000e+000 1.95408288e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43436815e-302]\n","Loss at step 923: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 923: 0.9989735491512041\n","Performing step 924 of gradient descent.\n","z [ 393.00857305 -443.72896393  377.45020437 ...  517.59432433 -827.94733106\n"," -693.1362856 ] (12665,)\n","y hat: [1.00000000e+000 1.95415723e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43487079e-302]\n","Loss at step 924: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 924: 0.9989735491512041\n","Performing step 925 of gradient descent.\n","z [ 393.00854896 -443.72892589  377.45017944 ...  517.59426983 -827.94728555\n"," -693.13623233] (12665,)\n","y hat: [1.00000000e+000 1.95423158e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43537345e-302]\n","Loss at step 925: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 925: 0.9989735491512041\n","Performing step 926 of gradient descent.\n","z [ 393.00852487 -443.72888784  377.45015451 ...  517.59421533 -827.94724003\n"," -693.13617905] (12665,)\n","y hat: [1.00000000e+000 1.95430593e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43587613e-302]\n","Loss at step 926: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 926: 0.9989735491512041\n","Performing step 927 of gradient descent.\n","z [ 393.00850078 -443.7288498   377.45012959 ...  517.59416083 -827.94719452\n"," -693.13612577] (12665,)\n","y hat: [1.00000000e+000 1.95438028e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43637885e-302]\n","Loss at step 927: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 927: 0.9989735491512041\n","Performing step 928 of gradient descent.\n","z [ 393.00847669 -443.72881175  377.45010466 ...  517.59410633 -827.94714901\n"," -693.1360725 ] (12665,)\n","y hat: [1.00000000e+000 1.95445464e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43688159e-302]\n","Loss at step 928: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 928: 0.9989735491512041\n","Performing step 929 of gradient descent.\n","z [ 393.00845259 -443.7287737   377.45007973 ...  517.59405183 -827.9471035\n"," -693.13601922] (12665,)\n","y hat: [1.00000000e+000 1.95452900e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43738436e-302]\n","Loss at step 929: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 929: 0.9989735491512041\n","Performing step 930 of gradient descent.\n","z [ 393.0084285  -443.72873566  377.4500548  ...  517.59399733 -827.94705799\n"," -693.13596595] (12665,)\n","y hat: [1.00000000e+000 1.95460336e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43788715e-302]\n","Loss at step 930: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 930: 0.9989735491512041\n","Performing step 931 of gradient descent.\n","z [ 393.00840441 -443.72869761  377.45002987 ...  517.59394283 -827.94701248\n"," -693.13591267] (12665,)\n","y hat: [1.00000000e+000 1.95467773e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43838997e-302]\n","Loss at step 931: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 931: 0.9989735491512041\n","Performing step 932 of gradient descent.\n","z [ 393.00838032 -443.72865957  377.45000494 ...  517.59388833 -827.94696697\n"," -693.1358594 ] (12665,)\n","y hat: [1.00000000e+000 1.95475209e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43889282e-302]\n","Loss at step 932: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 932: 0.9989735491512041\n","Performing step 933 of gradient descent.\n","z [ 393.00835623 -443.72862152  377.44998002 ...  517.59383384 -827.94692145\n"," -693.13580612] (12665,)\n","y hat: [1.00000000e+000 1.95482647e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43939569e-302]\n","Loss at step 933: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 933: 0.9989735491512041\n","Performing step 934 of gradient descent.\n","z [ 393.00833214 -443.72858348  377.44995509 ...  517.59377934 -827.94687594\n"," -693.13575285] (12665,)\n","y hat: [1.00000000e+000 1.95490084e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.43989860e-302]\n","Loss at step 934: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 934: 0.9989735491512041\n","Performing step 935 of gradient descent.\n","z [ 393.00830805 -443.72854543  377.44993016 ...  517.59372484 -827.94683043\n"," -693.13569957] (12665,)\n","y hat: [1.00000000e+000 1.95497522e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44040152e-302]\n","Loss at step 935: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 935: 0.9989735491512041\n","Performing step 936 of gradient descent.\n","z [ 393.00828395 -443.72850739  377.44990523 ...  517.59367034 -827.94678492\n"," -693.1356463 ] (12665,)\n","y hat: [1.00000000e+000 1.95504959e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44090448e-302]\n","Loss at step 936: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 936: 0.9989735491512041\n","Performing step 937 of gradient descent.\n","z [ 393.00825986 -443.72846934  377.4498803  ...  517.59361584 -827.94673941\n"," -693.13559302] (12665,)\n","y hat: [1.00000000e+000 1.95512398e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44140746e-302]\n","Loss at step 937: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 937: 0.9989735491512041\n","Performing step 938 of gradient descent.\n","z [ 393.00823577 -443.7284313   377.44985537 ...  517.59356134 -827.9466939\n"," -693.13553974] (12665,)\n","y hat: [1.00000000e+000 1.95519836e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44191047e-302]\n","Loss at step 938: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 938: 0.9989735491512041\n","Performing step 939 of gradient descent.\n","z [ 393.00821168 -443.72839325  377.44983045 ...  517.59350684 -827.94664839\n"," -693.13548647] (12665,)\n","y hat: [1.00000000e+000 1.95527275e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44241351e-302]\n","Loss at step 939: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 939: 0.9989735491512041\n","Performing step 940 of gradient descent.\n","z [ 393.00818759 -443.7283552   377.44980552 ...  517.59345234 -827.94660287\n"," -693.13543319] (12665,)\n","y hat: [1.00000000e+000 1.95534714e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44291657e-302]\n","Loss at step 940: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 940: 0.9989735491512041\n","Performing step 941 of gradient descent.\n","z [ 393.0081635  -443.72831716  377.44978059 ...  517.59339784 -827.94655736\n"," -693.13537992] (12665,)\n","y hat: [1.00000000e+000 1.95542153e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44341966e-302]\n","Loss at step 941: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 941: 0.9989735491512041\n","Performing step 942 of gradient descent.\n","z [ 393.0081394  -443.72827911  377.44975566 ...  517.59334334 -827.94651185\n"," -693.13532664] (12665,)\n","y hat: [1.00000000e+000 1.95549593e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44392278e-302]\n","Loss at step 942: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 942: 0.9989735491512041\n","Performing step 943 of gradient descent.\n","z [ 393.00811531 -443.72824107  377.44973073 ...  517.59328884 -827.94646634\n"," -693.13527337] (12665,)\n","y hat: [1.00000000e+000 1.95557033e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44442592e-302]\n","Loss at step 943: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 943: 0.9989735491512041\n","Performing step 944 of gradient descent.\n","z [ 393.00809122 -443.72820302  377.4497058  ...  517.59323435 -827.94642083\n"," -693.13522009] (12665,)\n","y hat: [1.00000000e+000 1.95564473e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44492909e-302]\n","Loss at step 944: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 944: 0.9989735491512041\n","Performing step 945 of gradient descent.\n","z [ 393.00806713 -443.72816498  377.44968087 ...  517.59317985 -827.94637532\n"," -693.13516682] (12665,)\n","y hat: [1.00000000e+000 1.95571914e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44543229e-302]\n","Loss at step 945: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 945: 0.9989735491512041\n","Performing step 946 of gradient descent.\n","z [ 393.00804304 -443.72812693  377.44965595 ...  517.59312535 -827.94632981\n"," -693.13511354] (12665,)\n","y hat: [1.00000000e+000 1.95579354e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44593551e-302]\n","Loss at step 946: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 946: 0.9989735491512041\n","Performing step 947 of gradient descent.\n","z [ 393.00801895 -443.72808889  377.44963102 ...  517.59307085 -827.94628429\n"," -693.13506026] (12665,)\n","y hat: [1.00000000e+000 1.95586795e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44643876e-302]\n","Loss at step 947: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 947: 0.9989735491512041\n","Performing step 948 of gradient descent.\n","z [ 393.00799486 -443.72805084  377.44960609 ...  517.59301635 -827.94623878\n"," -693.13500699] (12665,)\n","y hat: [1.00000000e+000 1.95594237e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44694204e-302]\n","Loss at step 948: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 948: 0.9989735491512041\n","Performing step 949 of gradient descent.\n","z [ 393.00797076 -443.7280128   377.44958116 ...  517.59296185 -827.94619327\n"," -693.13495371] (12665,)\n","y hat: [1.00000000e+000 1.95601678e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44744535e-302]\n","Loss at step 949: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 949: 0.9989735491512041\n","Performing step 950 of gradient descent.\n","z [ 393.00794667 -443.72797475  377.44955623 ...  517.59290735 -827.94614776\n"," -693.13490044] (12665,)\n","y hat: [1.00000000e+000 1.95609120e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44794868e-302]\n","Loss at step 950: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 950: 0.9989735491512041\n","Performing step 951 of gradient descent.\n","z [ 393.00792258 -443.72793671  377.4495313  ...  517.59285285 -827.94610225\n"," -693.13484716] (12665,)\n","y hat: [1.00000000e+000 1.95616562e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44845204e-302]\n","Loss at step 951: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 951: 0.9989735491512041\n","Performing step 952 of gradient descent.\n","z [ 393.00789849 -443.72789866  377.44950638 ...  517.59279835 -827.94605674\n"," -693.13479389] (12665,)\n","y hat: [1.00000000e+000 1.95624005e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44895542e-302]\n","Loss at step 952: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 952: 0.9989735491512041\n","Performing step 953 of gradient descent.\n","z [ 393.0078744  -443.72786061  377.44948145 ...  517.59274385 -827.94601123\n"," -693.13474061] (12665,)\n","y hat: [1.00000000e+000 1.95631447e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44945883e-302]\n","Loss at step 953: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 953: 0.9989735491512041\n","Performing step 954 of gradient descent.\n","z [ 393.00785031 -443.72782257  377.44945652 ...  517.59268936 -827.94596571\n"," -693.13468733] (12665,)\n","y hat: [1.00000000e+000 1.95638890e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.44996227e-302]\n","Loss at step 954: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 954: 0.9989735491512041\n","Performing step 955 of gradient descent.\n","z [ 393.00782622 -443.72778452  377.44943159 ...  517.59263486 -827.9459202\n"," -693.13463406] (12665,)\n","y hat: [1.00000000e+000 1.95646334e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45046574e-302]\n","Loss at step 955: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 955: 0.9989735491512041\n","Performing step 956 of gradient descent.\n","z [ 393.00780212 -443.72774648  377.44940666 ...  517.59258036 -827.94587469\n"," -693.13458078] (12665,)\n","y hat: [1.00000000e+000 1.95653777e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45096923e-302]\n","Loss at step 956: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 956: 0.9989735491512041\n","Performing step 957 of gradient descent.\n","z [ 393.00777803 -443.72770843  377.44938173 ...  517.59252586 -827.94582918\n"," -693.13452751] (12665,)\n","y hat: [1.00000000e+000 1.95661221e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45147275e-302]\n","Loss at step 957: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 957: 0.9989735491512041\n","Performing step 958 of gradient descent.\n","z [ 393.00775394 -443.72767039  377.4493568  ...  517.59247136 -827.94578367\n"," -693.13447423] (12665,)\n","y hat: [1.00000000e+000 1.95668665e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45197630e-302]\n","Loss at step 958: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 958: 0.9989735491512041\n","Performing step 959 of gradient descent.\n","z [ 393.00772985 -443.72763234  377.44933188 ...  517.59241686 -827.94573816\n"," -693.13442096] (12665,)\n","y hat: [1.00000000e+000 1.95676110e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45247987e-302]\n","Loss at step 959: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 959: 0.9989735491512041\n","Performing step 960 of gradient descent.\n","z [ 393.00770576 -443.7275943   377.44930695 ...  517.59236236 -827.94569264\n"," -693.13436768] (12665,)\n","y hat: [1.00000000e+000 1.95683554e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45298347e-302]\n","Loss at step 960: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 960: 0.9989735491512041\n","Performing step 961 of gradient descent.\n","z [ 393.00768167 -443.72755625  377.44928202 ...  517.59230786 -827.94564713\n"," -693.13431441] (12665,)\n","y hat: [1.00000000e+000 1.95690999e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45348710e-302]\n","Loss at step 961: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 961: 0.9989735491512041\n","Performing step 962 of gradient descent.\n","z [ 393.00765758 -443.72751821  377.44925709 ...  517.59225336 -827.94560162\n"," -693.13426113] (12665,)\n","y hat: [1.00000000e+000 1.95698445e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45399075e-302]\n","Loss at step 962: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 962: 0.9989735491512041\n","Performing step 963 of gradient descent.\n","z [ 393.00763348 -443.72748016  377.44923216 ...  517.59219887 -827.94555611\n"," -693.13420785] (12665,)\n","y hat: [1.00000000e+000 1.95705890e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45449444e-302]\n","Loss at step 963: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 963: 0.9989735491512041\n","Performing step 964 of gradient descent.\n","z [ 393.00760939 -443.72744212  377.44920723 ...  517.59214437 -827.9455106\n"," -693.13415458] (12665,)\n","y hat: [1.00000000e+000 1.95713336e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45499814e-302]\n","Loss at step 964: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 964: 0.9989735491512041\n","Performing step 965 of gradient descent.\n","z [ 393.0075853  -443.72740407  377.44918231 ...  517.59208987 -827.94546509\n"," -693.1341013 ] (12665,)\n","y hat: [1.00000000e+000 1.95720782e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45550188e-302]\n","Loss at step 965: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 965: 0.9989735491512041\n","Performing step 966 of gradient descent.\n","z [ 393.00756121 -443.72736603  377.44915738 ...  517.59203537 -827.94541958\n"," -693.13404803] (12665,)\n","y hat: [1.00000000e+000 1.95728229e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45600564e-302]\n","Loss at step 966: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 966: 0.9989735491512041\n","Performing step 967 of gradient descent.\n","z [ 393.00753712 -443.72732798  377.44913245 ...  517.59198087 -827.94537406\n"," -693.13399475] (12665,)\n","y hat: [1.00000000e+000 1.95735675e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45650943e-302]\n","Loss at step 967: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 967: 0.9989735491512041\n","Performing step 968 of gradient descent.\n","z [ 393.00751303 -443.72728993  377.44910752 ...  517.59192637 -827.94532855\n"," -693.13394148] (12665,)\n","y hat: [1.00000000e+000 1.95743122e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45701324e-302]\n","Loss at step 968: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 968: 0.9989735491512041\n","Performing step 969 of gradient descent.\n","z [ 393.00748894 -443.72725189  377.44908259 ...  517.59187187 -827.94528304\n"," -693.1338882 ] (12665,)\n","y hat: [1.00000000e+000 1.95750570e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45751709e-302]\n","Loss at step 969: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 969: 0.9989735491512041\n","Performing step 970 of gradient descent.\n","z [ 393.00746484 -443.72721384  377.44905766 ...  517.59181737 -827.94523753\n"," -693.13383492] (12665,)\n","y hat: [1.00000000e+000 1.95758017e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45802096e-302]\n","Loss at step 970: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 970: 0.9989735491512041\n","Performing step 971 of gradient descent.\n","z [ 393.00744075 -443.7271758   377.44903273 ...  517.59176287 -827.94519202\n"," -693.13378165] (12665,)\n","y hat: [1.00000000e+000 1.95765465e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45852485e-302]\n","Loss at step 971: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 971: 0.9989735491512041\n","Performing step 972 of gradient descent.\n","z [ 393.00741666 -443.72713775  377.44900781 ...  517.59170838 -827.94514651\n"," -693.13372837] (12665,)\n","y hat: [1.00000000e+000 1.95772913e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45902878e-302]\n","Loss at step 972: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 972: 0.9989735491512041\n","Performing step 973 of gradient descent.\n","z [ 393.00739257 -443.72709971  377.44898288 ...  517.59165388 -827.945101\n"," -693.1336751 ] (12665,)\n","y hat: [1.00000000e+000 1.95780361e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.45953273e-302]\n","Loss at step 973: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 973: 0.9989735491512041\n","Performing step 974 of gradient descent.\n","z [ 393.00736848 -443.72706166  377.44895795 ...  517.59159938 -827.94505548\n"," -693.13362182] (12665,)\n","y hat: [1.0000000e+000 1.9578781e-193 1.0000000e+000 ... 1.0000000e+000\n"," 0.0000000e+000 9.4600367e-302]\n","Loss at step 974: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 974: 0.9989735491512041\n","Performing step 975 of gradient descent.\n","z [ 393.00734439 -443.72702362  377.44893302 ...  517.59154488 -827.94500997\n"," -693.13356855] (12665,)\n","y hat: [1.00000000e+000 1.95795259e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46054071e-302]\n","Loss at step 975: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 975: 0.9989735491512041\n","Performing step 976 of gradient descent.\n","z [ 393.0073203  -443.72698557  377.44890809 ...  517.59149038 -827.94496446\n"," -693.13351527] (12665,)\n","y hat: [1.00000000e+000 1.95802708e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46104474e-302]\n","Loss at step 976: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 976: 0.9989735491512041\n","Performing step 977 of gradient descent.\n","z [ 393.0072962  -443.72694753  377.44888316 ...  517.59143588 -827.94491895\n"," -693.13346199] (12665,)\n","y hat: [1.00000000e+000 1.95810158e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46154880e-302]\n","Loss at step 977: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 977: 0.9989735491512041\n","Performing step 978 of gradient descent.\n","z [ 393.00727211 -443.72690948  377.44885824 ...  517.59138138 -827.94487344\n"," -693.13340872] (12665,)\n","y hat: [1.00000000e+000 1.95817608e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46205288e-302]\n","Loss at step 978: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 978: 0.9989735491512041\n","Performing step 979 of gradient descent.\n","z [ 393.00724802 -443.72687144  377.44883331 ...  517.59132688 -827.94482793\n"," -693.13335544] (12665,)\n","y hat: [1.00000000e+000 1.95825058e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46255699e-302]\n","Loss at step 979: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 979: 0.9989735491512041\n","Performing step 980 of gradient descent.\n","z [ 393.00722393 -443.72683339  377.44880838 ...  517.59127238 -827.94478242\n"," -693.13330217] (12665,)\n","y hat: [1.00000000e+000 1.95832508e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46306113e-302]\n","Loss at step 980: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 980: 0.9989735491512041\n","Performing step 981 of gradient descent.\n","z [ 393.00719984 -443.72679535  377.44878345 ...  517.59121789 -827.9447369\n"," -693.13324889] (12665,)\n","y hat: [1.00000000e+000 1.95839959e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46356530e-302]\n","Loss at step 981: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 981: 0.9989735491512041\n","Performing step 982 of gradient descent.\n","z [ 393.00717575 -443.7267573   377.44875852 ...  517.59116339 -827.94469139\n"," -693.13319562] (12665,)\n","y hat: [1.00000000e+000 1.95847410e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46406949e-302]\n","Loss at step 982: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 982: 0.9989735491512041\n","Performing step 983 of gradient descent.\n","z [ 393.00715166 -443.72671926  377.44873359 ...  517.59110889 -827.94464588\n"," -693.13314234] (12665,)\n","y hat: [1.00000000e+000 1.95854861e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46457371e-302]\n","Loss at step 983: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 983: 0.9989735491512041\n","Performing step 984 of gradient descent.\n","z [ 393.00712757 -443.72668121  377.44870866 ...  517.59105439 -827.94460037\n"," -693.13308906] (12665,)\n","y hat: [1.00000000e+000 1.95862312e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46507795e-302]\n","Loss at step 984: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 984: 0.9989735491512041\n","Performing step 985 of gradient descent.\n","z [ 393.00710347 -443.72664316  377.44868374 ...  517.59099989 -827.94455486\n"," -693.13303579] (12665,)\n","y hat: [1.00000000e+000 1.95869764e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46558223e-302]\n","Loss at step 985: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 985: 0.9989735491512041\n","Performing step 986 of gradient descent.\n","z [ 393.00707938 -443.72660512  377.44865881 ...  517.59094539 -827.94450935\n"," -693.13298251] (12665,)\n","y hat: [1.00000000e+000 1.95877216e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46608653e-302]\n","Loss at step 986: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 986: 0.9989735491512041\n","Performing step 987 of gradient descent.\n","z [ 393.00705529 -443.72656707  377.44863388 ...  517.59089089 -827.94446384\n"," -693.13292924] (12665,)\n","y hat: [1.00000000e+000 1.95884668e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46659085e-302]\n","Loss at step 987: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 987: 0.9989735491512041\n","Performing step 988 of gradient descent.\n","z [ 393.0070312  -443.72652903  377.44860895 ...  517.59083639 -827.94441832\n"," -693.13287596] (12665,)\n","y hat: [1.00000000e+000 1.95892121e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46709521e-302]\n","Loss at step 988: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 988: 0.9989735491512041\n","Performing step 989 of gradient descent.\n","z [ 393.00700711 -443.72649098  377.44858402 ...  517.5907819  -827.94437281\n"," -693.13282268] (12665,)\n","y hat: [1.00000000e+000 1.95899574e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46759959e-302]\n","Loss at step 989: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 989: 0.9989735491512041\n","Performing step 990 of gradient descent.\n","z [ 393.00698302 -443.72645294  377.44855909 ...  517.5907274  -827.9443273\n"," -693.13276941] (12665,)\n","y hat: [1.00000000e+000 1.95907027e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46810400e-302]\n","Loss at step 990: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 990: 0.9989735491512041\n","Performing step 991 of gradient descent.\n","z [ 393.00695893 -443.72641489  377.44853417 ...  517.5906729  -827.94428179\n"," -693.13271613] (12665,)\n","y hat: [1.00000000e+000 1.95914481e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46860843e-302]\n","Loss at step 991: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 991: 0.9989735491512041\n","Performing step 992 of gradient descent.\n","z [ 393.00693483 -443.72637685  377.44850924 ...  517.5906184  -827.94423628\n"," -693.13266286] (12665,)\n","y hat: [1.00000000e+000 1.95921934e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46911289e-302]\n","Loss at step 992: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 992: 0.9989735491512041\n","Performing step 993 of gradient descent.\n","z [ 393.00691074 -443.7263388   377.44848431 ...  517.5905639  -827.94419077\n"," -693.13260958] (12665,)\n","y hat: [1.00000000e+000 1.95929388e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.46961738e-302]\n","Loss at step 993: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 993: 0.9989735491512041\n","Performing step 994 of gradient descent.\n","z [ 393.00688665 -443.72630076  377.44845938 ...  517.5905094  -827.94414526\n"," -693.13255631] (12665,)\n","y hat: [1.00000000e+000 1.95936843e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.47012190e-302]\n","Loss at step 994: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 994: 0.9989735491512041\n","Performing step 995 of gradient descent.\n","z [ 393.00686256 -443.72626271  377.44843445 ...  517.5904549  -827.94409974\n"," -693.13250303] (12665,)\n","y hat: [1.00000000e+000 1.95944297e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.47062644e-302]\n","Loss at step 995: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 995: 0.9989735491512041\n","Performing step 996 of gradient descent.\n","z [ 393.00683847 -443.72622467  377.44840952 ...  517.5904004  -827.94405423\n"," -693.13244975] (12665,)\n","y hat: [1.00000000e+000 1.95951752e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.47113101e-302]\n","Loss at step 996: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 996: 0.9989735491512041\n","Performing step 997 of gradient descent.\n","z [ 393.00681438 -443.72618662  377.44838459 ...  517.59034591 -827.94400872\n"," -693.13239648] (12665,)\n","y hat: [1.00000000e+000 1.95959207e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.47163560e-302]\n","Loss at step 997: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 997: 0.9989735491512041\n","Performing step 998 of gradient descent.\n","z [ 393.00679029 -443.72614858  377.44835967 ...  517.59029141 -827.94396321\n"," -693.1323432 ] (12665,)\n","y hat: [1.00000000e+000 1.95966663e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.47214023e-302]\n","Loss at step 998: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 998: 0.9989735491512041\n","Performing step 999 of gradient descent.\n","z [ 393.00676619 -443.72611053  377.44833474 ...  517.59023691 -827.9439177\n"," -693.13228993] (12665,)\n","y hat: [1.00000000e+000 1.95974119e-193 1.00000000e+000 ... 1.00000000e+000\n"," 0.00000000e+000 9.47264488e-302]\n","Loss at step 999: nan\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 0 1 ... 1 0 0]\n","(12665,)\n","Accuracy at step 999: 0.9989735491512041\n"]}]},{"cell_type":"markdown","metadata":{"id":"Dt_XyizgcD7H"},"source":["Evaluate model on test set"]},{"cell_type":"code","metadata":{"id":"RIysfpyCcIN2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08aec661-5484-4816-a78f-577e9f5c0596","executionInfo":{"status":"ok","timestamp":1682090255131,"user_tz":-120,"elapsed":342,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(\"_______________________________\")\n","print(\"Starting evaluation of test set\")\n","\n","X,y = data_preprocess(x_test, y_test)\n","z = np.dot(X,w) \n","y_hat = sigmoid(z)\n","predictions = (y_hat>0.5).astype(np.int32)\n","accuracy = np.mean(predictions==y)\n","print(\"Accuracy of test set: \" + str(accuracy))"],"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["_______________________________\n","Starting evaluation of test set\n","10000\n","length  2115\n","(2115, 785)\n","(2115, 785)\n","Accuracy of test set: 0.9995271867612293\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-57-dc64dee5606b>:2: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-z))\n"]}]}]}