{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TmURzPPRtODn"},"source":["# Logistic Regression Assignment (due 26 November)\n","\n","In this practical you will learn how to apply logistic regression to the task of predicting two digits from the MNIST database: http://yann.lecun.com/exdb/mnist/. The database contains 60000 train images containing digits and 10000 test images. The images are of size 28 × 28. We will use the images in a vectorized form: a vector of size of 784. The code extracting the digits 0 and 1 is provided in the stubs.\n"]},{"cell_type":"code","metadata":{"id":"MQkzOf0dFpxc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea25777e-afca-45b9-e157-6e718475dab0","executionInfo":{"status":"ok","timestamp":1682323748147,"user_tz":-120,"elapsed":7,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["%tensorflow_version 2.x"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}]},{"cell_type":"code","metadata":{"id":"xI0dT0-WEYuh","executionInfo":{"status":"ok","timestamp":1682323758443,"user_tz":-120,"elapsed":7369,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqiqzu3dFXKj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ff63c7d-dcb1-471b-b380-daaac2649a1b","executionInfo":{"status":"ok","timestamp":1682323760892,"user_tz":-120,"elapsed":387,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(tf.__version__)\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"ZSjE8NINEvVO","executionInfo":{"status":"ok","timestamp":1682323764196,"user_tz":-120,"elapsed":793,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["import numpy as np "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybQxEwHnGsgY","executionInfo":{"status":"ok","timestamp":1682323766316,"user_tz":-120,"elapsed":5,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["def data_preprocess(images, labels):\n","\n","    # number of examples m  \n","    m = images.shape[0]\n","    \n","    print(m)\n","    # create vector of ones to concatenate to our data matrix (for intercept terms)\n","    ones = np.ones(shape=[m, 1])\n","    images = np.concatenate((ones, images), axis=1)\n","    \n","    # to retrieve the images and corresponding labels where the label is either 0 or 1, \n","    # we define two logical vectors that can be used to subset our data_matrices\n","    logical_mask_0 = labels == 0\n","    logical_mask_1 = labels == 1\n","    \n","    images_zeros = images[logical_mask_0]\n","    labels_zeros = labels[logical_mask_0]\n","    images_ones = images[logical_mask_1]\n","    labels_ones = labels[logical_mask_1]\n","    \n","    X = np.concatenate((images_zeros, images_ones), axis=0)\n","    y = np.concatenate((labels_zeros, labels_ones), axis=0)\n","    \n","    # shuffle the data and corresponding labels in unison\n","    def _shuffle_in_unison(a, b):\n","        assert len(a) == len(b)\n","        p = np.random.permutation(len(a))\n","        print('length ', len(a))\n","        print(a.shape)\n","        print(a[p].shape)\n","        return a[p], b[p]\n","\n","    return _shuffle_in_unison(X,y)   "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrkpG7BYI8MT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682323932947,"user_tz":-120,"elapsed":1734,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"820d3018-bd9c-47b6-b1cf-5a2844ac36e8"},"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"CHEiLlEnNhIm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"36d8df19-588b-40c0-87e8-449a19a18d81","executionInfo":{"status":"ok","timestamp":1682323935145,"user_tz":-120,"elapsed":373,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print (x_train.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n"]}]},{"cell_type":"code","metadata":{"id":"Cb5Hr9aENMTy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8e6cca8-3a22-4a0d-9b4f-b8cb74103679","executionInfo":{"status":"ok","timestamp":1682323936747,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["x_train = x_train.reshape([60000,784])\n","x_test = x_test.reshape([10000,784])\n","print(x_train.shape)\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n"]}]},{"cell_type":"code","metadata":{"id":"cMi47AaKcHzQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa164a78-cc1a-431d-9ffe-72363c0f7913","executionInfo":{"status":"ok","timestamp":1682323939841,"user_tz":-120,"elapsed":689,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["X,y = data_preprocess(x_train, y_train)\n","print('shape: ', X.shape)\n","print('shape: ', y.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","length  12665\n","(12665, 785)\n","(12665, 785)\n","shape:  (12665, 785)\n","shape:  (12665,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"UTx9l6kePGbf"},"source":["Define hyperparams: learning rate and gradient descent steps\n"]},{"cell_type":"code","metadata":{"id":"NzE8JqGXdBy2","executionInfo":{"status":"ok","timestamp":1682323941876,"user_tz":-120,"elapsed":348,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["learning_rate = 0.01\n","gdc_steps = 1000\n","\n","    "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5ujHEHrQDy_"},"source":["Initialize your parameters W\n"]},{"cell_type":"code","metadata":{"id":"RnXlpZqMR4jP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682323943722,"user_tz":-120,"elapsed":301,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"75a31fa2-c498-468d-949d-d05277e6a01c"},"source":["# number of features n\n","n = X.shape[1]\n","print(n)\n","# we need to define our model parameters to be learned. we use W (weights) instead of theta this time.\n","mu, sigma = 0, 0.01 # mean and standard deviation\n","w = np.random.normal(mu, sigma, n)\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["785\n"]}]},{"cell_type":"code","metadata":{"id":"1b-fVYzzR6_r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94bc83f6-5367-43bd-c700-e1280c1ccc79","executionInfo":{"status":"ok","timestamp":1682323945570,"user_tz":-120,"elapsed":10,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(X.shape, w.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(12665, 785) (785,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Yp3Uc6qUSIi0"},"source":["Define the sigmoid function, your code here:\n"]},{"cell_type":"code","metadata":{"id":"O_gdiFglSMYN","executionInfo":{"status":"ok","timestamp":1682323947184,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m14UBJJ6SRGS"},"source":["Define the loss function as provided in equation 12 (Logistic regression slides)\n"]},{"cell_type":"code","metadata":{"id":"NyCliB0USThU","executionInfo":{"status":"ok","timestamp":1682323948995,"user_tz":-120,"elapsed":6,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["def compute_cross_entropy_loss(y, y_hat):\n","      return   -1 * y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vusNro8SX5-"},"source":["Start optimization. During training you minimize the loss function. In every iteration your loss should decrease. You also want to look how many correct predictions you have at every iteration. Reminder: the belonging to class digit 1 is when your prediction, $\\hat y$ is greater or equal to 0.5. "]},{"cell_type":"markdown","metadata":{"id":"fKXM0YdDcajm"},"source":["When you test your prediction vector (containing zero and ones) with the labels (also zero and ones) you can use the equal function. \n","\n","Example:\n","prediction = (1, 0, 1, 1) and the true labels are y = (0, 0, 1, 0).\n","\n","When you test on equality you get following result: correct = (0, 1, 1, 0). Your accuracy is: 0+1+1+0\n","4 = 0.5.\n","You compute the accuracy for the training and test."]},{"cell_type":"code","metadata":{"id":"5ToLLOp2Scv0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682323978602,"user_tz":-120,"elapsed":22226,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"1436fde5-0811-4923-c522-96b553cc61cc"},"source":["for step in range(0, gdc_steps):\n","    print(\"Performing step \" + str(step) + \" of gradient descent.\")\n","    # perform the dot product between the weights and the examples\n","    z = np.dot(X,w)\n","    print('z', z, z.shape)\n","    # apply the nonlinearity\n","    y_hat = sigmoid(z)\n","    print(\"y hat: \" + str(y_hat))\n","    # normally normalized with -1/m \n","    loss = compute_cross_entropy_loss(y, y_hat)\n","    print(\"Loss at step \" + str(step) + \": \" + str(loss))\n","    \n","    # compute the error term, i.e. the difference between labels and estimated labels y_hat, see equation 24 in the slides\n","    error_term = y_hat - y\n","    \n","    # compute the gradient. as our data matrix X is currently layed out as X_j_i, we got to transpose it \n","    # see derived formula of the gradient calculation\n","    gradients =  np.dot(np.transpose(X),error_term)\n","    print(X.shape)\n","    print(error_term.shape)\n","    print(gradients.shape)\n","    \n","    # update w using the gdc update rule\n","    w = w-learning_rate*gradients\n","    \n","    # compute the predictions and cast them to int values\n","    predictions = (y_hat>0.5).astype(np.int32)\n","    print(predictions)\n","    print(predictions.shape)\n","    # compute mean accuracy\n","    accuracy =  np.mean(predictions == y)\n","    print(\"Accuracy at step \" + str(step) + \": \" + str(accuracy))\n","    \n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing step 0 of gradient descent.\n","z [-0.05775666 -0.03851479 -0.02713634 ...  0.01609644  0.21846231\n","  0.14053149] (12665,)\n","y hat: [0.48556485 0.49037249 0.49321633 ... 0.50402402 0.5543994  0.53507517]\n","Loss at step 0: [ 0.72244243  0.71258999 -0.67967106 ...  0.68513135 -0.80833223\n"," -0.76587953]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[0 0 0 ... 1 1 1]\n","(12665,)\n","Accuracy at step 0: 0.21208053691275167\n","Performing step 1 of gradient descent.\n","z [  395.25003174   359.19102531 -1206.32002915 ...   471.91805344\n"," -1838.09761072 -1895.41270448] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 1: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 1: 0.9883932096328464\n","Performing step 2 of gradient descent.\n","z [  420.63273601   378.62171906 -1171.67890631 ...   505.3763352\n"," -1791.48413514 -1835.25044447] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 2: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 2: 0.9908409001184366\n","Performing step 3 of gradient descent.\n","z [  439.48448329   393.08110074 -1144.0060526  ...   529.8846146\n"," -1755.18255506 -1788.73928055] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 3: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 3: 0.99249901302803\n","Performing step 4 of gradient descent.\n","z [  453.21921696   403.72285264 -1122.06209727 ...   547.36793463\n"," -1726.97074426 -1752.97435676] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 4: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 4: 0.9933675483616266\n","Performing step 5 of gradient descent.\n","z [  463.46082355   411.66252001 -1104.32891702 ...   560.14039896\n"," -1704.3967164  -1725.49982114] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 5: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 5: 0.9936044216344255\n","Performing step 6 of gradient descent.\n","z [  472.85287927   418.94880262 -1087.69606469 ...   571.86027549\n"," -1683.13345777 -1699.70598803] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 6: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 6: 0.9941571259376234\n","Performing step 7 of gradient descent.\n","z [  481.27999175   425.36276864 -1072.41377916 ...   582.40769395\n"," -1663.70381544 -1676.21674781] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 7: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 7: 0.9943150414528228\n","Performing step 8 of gradient descent.\n","z [  488.9170542    431.19192306 -1058.20752984 ...   591.87692007\n"," -1645.50754182 -1654.53742698] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 8: [nan nan nan ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 8: 0.9948677457560205\n","Performing step 9 of gradient descent.\n","z [  494.71731593   435.71529607 -1046.3307462  ...   599.08351769\n"," -1630.14345896 -1636.62444949] (12665,)\n","y hat: [1. 1. 0. ... 1. 0. 0.]\n","Loss at step 9: [nan nan nan ... nan nan nan]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-dc64dee5606b>:2: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-z))\n","<ipython-input-14-c6e70183a7ea>:2: RuntimeWarning: divide by zero encountered in log\n","  return   -1 * y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n","<ipython-input-14-c6e70183a7ea>:2: RuntimeWarning: invalid value encountered in multiply\n","  return   -1 * y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mDie letzten 5000 Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 583: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 583: 0.9993683379392025\n","Performing step 584 of gradient descent.\n","z [  266.71472147   291.79273024  -545.64768886 ...   317.7012797\n","  -931.44102496 -1135.72364307] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.06713574e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 584: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 584: 0.9993683379392025\n","Performing step 585 of gradient descent.\n","z [  266.42087833   291.64432533  -545.373727   ...   317.37957587\n","  -930.91455803 -1135.2626685 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.40345919e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 585: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 585: 0.9993683379392025\n","Performing step 586 of gradient descent.\n","z [  266.13277862   291.49932928  -545.0980974  ...   317.06548347\n","  -930.38892132 -1134.79792869] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.84886075e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 586: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 586: 0.9993683379392025\n","Performing step 587 of gradient descent.\n","z [  265.8498746    291.35732466  -544.82143571 ...   316.75835161\n","  -929.86517231 -1134.33086726] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.43812995e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 587: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 587: 0.9993683379392025\n","Performing step 588 of gradient descent.\n","z [  265.57148968   291.21781706  -544.54438517 ...   316.4573604\n","  -929.34429231 -1133.86294915] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.2164616e-237 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 588: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 588: 0.9993683379392025\n","Performing step 589 of gradient descent.\n","z [  265.29692869   291.08030976  -544.26751402 ...   316.16165997\n","  -928.82708175 -1133.39547442] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.24250126e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 589: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 589: 0.9993683379392025\n","Performing step 590 of gradient descent.\n","z [  265.02555105   290.94435271  -543.99127284 ...   315.87046282\n","  -928.31411607 -1132.92948073] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.59231986e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 590: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 590: 0.9993683379392025\n","Performing step 591 of gradient descent.\n","z [  264.75680487   290.80956461  -543.71598674 ...   315.58308721\n","  -927.80575096 -1132.46572405] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.36456728e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 591: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 591: 0.9994472956968022\n","Performing step 592 of gradient descent.\n","z [  264.49023335   290.67563609  -543.44186893 ...   315.29896563\n","  -927.30215506 -1132.00470737] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.68712909e-237 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 592: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 592: 0.9994472956968022\n","Performing step 593 of gradient descent.\n","z [  264.22546638   290.54232281  -543.16904259 ...   315.01763482\n","  -926.80335169 -1131.54672898] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.27257112e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 593: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 593: 0.9995262534544019\n","Performing step 594 of gradient descent.\n","z [  263.96220663   290.40943482  -542.89756336 ...   314.73871891\n","  -926.3092585  -1131.09193254] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.66949065e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 594: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 594: 0.9995262534544019\n","Performing step 595 of gradient descent.\n","z [  263.7002152    290.27682573  -542.62743871 ...   314.46191215\n","  -925.81972064 -1130.64035045] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.18724604e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 595: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 595: 0.9995262534544019\n","Performing step 596 of gradient descent.\n","z [  263.43929905   290.14438323  -542.35864306 ...   314.18696369\n","  -925.33453626 -1130.19193824] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.86176592e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 596: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 596: 0.9995262534544019\n","Performing step 597 of gradient descent.\n","z [  263.17930079   290.01202127  -542.0911292  ...   313.91366546\n","  -924.85347555 -1129.74660047] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.73950314e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 597: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 597: 0.9995262534544019\n","Performing step 598 of gradient descent.\n","z [  262.92009068   289.8796739   -541.82483643 ...   313.64184263\n","  -924.37629421 -1129.30420952] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.88048949e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 598: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 598: 0.9995262534544019\n","Performing step 599 of gradient descent.\n","z [  262.66156046   289.74729052  -541.55969638 ...   313.37134633\n","  -923.90274309 -1128.86461904] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.36227214e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 599: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 599: 0.9995262534544019\n","Performing step 600 of gradient descent.\n","z [  262.40361869   289.61483223  -541.29563702 ...   313.10204821\n","  -923.43257463 -1128.42767331] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.28498575e-236 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 600: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 600: 0.9995262534544019\n","Performing step 601 of gradient descent.\n","z [  262.14618719   289.48226901  -541.03258539 ...   312.83383624\n","  -922.96554732 -1127.9932138 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.07778872e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 601: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 601: 0.9995262534544019\n","Performing step 602 of gradient descent.\n","z [  261.88919827   289.34957756  -540.7704695  ...   312.56661144\n","  -922.50142854 -1127.56108354] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.40077737e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 602: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 602: 0.9995262534544019\n","Performing step 603 of gradient descent.\n","z [  261.63259261   289.2167397   -540.50921953 ...   312.30028545\n","  -922.03999629 -1127.13113011] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.81898258e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 603: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 603: 0.9995262534544019\n","Performing step 604 of gradient descent.\n","z [  261.37631767   289.08374106  -540.24876861 ...   312.0347785\n","  -921.58104031 -1126.70320761] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.36015725e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 604: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 604: 0.9995262534544019\n","Performing step 605 of gradient descent.\n","z [  261.12032629   288.95057009  -539.98905336 ...   311.77001786\n","  -921.12436252 -1126.27717787] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.06008745e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 605: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 605: 0.9995262534544019\n","Performing step 606 of gradient descent.\n","z [  260.86457566   288.81721728  -539.73001418 ...   311.50593644\n","  -920.66977723 -1125.85291127] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.9649081e-235 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 606: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 606: 0.9995262534544019\n","Performing step 607 of gradient descent.\n","z [  260.60902646   288.68367452  -539.4715954  ...   311.24247175\n","  -920.21711106 -1125.43028717] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.13408403e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 607: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 607: 0.9995262534544019\n","Performing step 608 of gradient descent.\n","z [  260.35364211   288.54993463  -539.21374541 ...   310.97956487\n","  -919.76620278 -1125.00919413] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.64424748e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 608: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 608: 0.9995262534544019\n","Performing step 609 of gradient descent.\n","z [  260.09838812   288.41599088  -538.95641664 ...   310.71715963\n","  -919.31690299 -1124.5895301 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.59413686e-235 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 609: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 609: 0.9995262534544019\n","Performing step 610 of gradient descent.\n","z [  259.84323156   288.2818367   -538.6995656  ...   310.45520181\n","  -918.86907382 -1124.17120238] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.11109517e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 610: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 610: 0.9995262534544019\n","Performing step 611 of gradient descent.\n","z [  259.58814059   288.14746533  -538.44315282 ...   310.19363844\n","  -918.42258858 -1123.75412764] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.43585277e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 611: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 611: 0.9995262534544019\n","Performing step 612 of gradient descent.\n","z [  259.33308396   288.0128696   -538.18714289 ...   309.93241713\n","  -917.97733139 -1123.33823193] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.85478516e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 612: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 612: 0.9995262534544019\n","Performing step 613 of gradient descent.\n","z [  259.07803067   287.87804165  -537.93150435 ...   309.67148545\n","  -917.53319688 -1122.92345061] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.39505792e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 613: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 613: 0.9995262534544019\n","Performing step 614 of gradient descent.\n","z [  258.82294957   287.74297274  -537.67620969 ...   309.41079034\n","  -917.09008984 -1122.50972831] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.09164117e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 614: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 614: 0.9995262534544019\n","Performing step 615 of gradient descent.\n","z [  258.56780901   287.6076531   -537.4212353  ...   309.15027758\n","  -916.64792488 -1122.09701889] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.98954207e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 615: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 615: 0.9995262534544019\n","Performing step 616 of gradient descent.\n","z [  258.31257658   287.47207169  -537.16656142 ...   308.88989126\n","  -916.20662612 -1121.68528538] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.14667226e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 616: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 616: 0.9995262534544019\n","Performing step 617 of gradient descent.\n","z [  258.05721885   287.33621614  -536.912172   ...   308.62957334\n","  -915.76612683 -1121.27449988] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.63752904e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 617: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 617: 0.9995262534544019\n","Performing step 618 of gradient descent.\n","z [  257.80170114   287.20007259  -536.65805466 ...   308.36926328\n","  -915.32636908 -1120.8646434 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.55791939e-234 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 618: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 618: 0.9995262534544019\n","Performing step 619 of gradient descent.\n","z [  257.54598738   287.06362567  -536.40420049 ...   308.10889771\n","  -914.88730332 -1120.45570559] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.10310196e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 619: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 619: 0.9995262534544019\n","Performing step 620 of gradient descent.\n","z [  257.29004011   286.92685843  -536.15060384 ...   307.84841032\n","  -914.44888784 -1120.0476844 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.42151447e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 620: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 620: 0.9995262534544019\n","Performing step 621 of gradient descent.\n","z [  257.03382048   286.78975246  -535.89726199 ...   307.58773178\n","  -914.01108828 -1119.6405855 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.83137065e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 621: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 621: 0.9995262534544019\n","Performing step 622 of gradient descent.\n","z [  256.77728849   286.65228796  -535.6441748  ...   307.32678998\n","  -913.57387678 -1119.23442152] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.3587973e-233 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 622: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 622: 0.9995262534544019\n","Performing step 623 of gradient descent.\n","z [  256.52040339   286.51444409  -535.39134408 ...   307.06551046\n","  -913.13723117 -1118.82921093] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.03734139e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 623: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 623: 0.9995262534544019\n","Performing step 624 of gradient descent.\n","z [  256.26312425   286.37619928  -535.13877294 ...   306.8038172\n","  -912.70113384 -1118.42497663] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.91006393e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 624: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 624: 0.9995262534544019\n","Performing step 625 of gradient descent.\n","z [  256.00541088   286.23753183  -534.88646487 ...   306.54163376\n","  -912.26557035 -1118.02174398] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.03222283e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 625: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 625: 0.9995262534544019\n","Performing step 626 of gradient descent.\n","z [  255.74722506   286.09842069  -534.63442254 ...   306.27888495\n","  -911.83052782 -1117.61953842] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.47471197e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 626: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 626: 0.9995262534544019\n","Performing step 627 of gradient descent.\n","z [  255.4885321    285.95884645  -534.3826465  ...   306.01549898\n","  -911.39599294 -1117.21838238] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.32847336e-233 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 627: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 627: 0.9995262534544019\n","Performing step 628 of gradient descent.\n","z [  255.22930289   285.81879259  -534.1311334  ...   305.7514103\n","  -910.96194971 -1116.81829165] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.07101647e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 628: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 628: 0.9995262534544019\n","Performing step 629 of gradient descent.\n","z [  254.96951637   285.67824701  -533.87987412 ...   305.48656295\n","  -910.52837686 -1116.41927096] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.37694524e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 629: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 629: 0.9995262534544019\n","Performing step 630 of gradient descent.\n","z [  254.7091625    285.53720391  -533.6288515  ...   305.22091476\n","  -910.09524493 -1116.02130902] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.76984165e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 630: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 630: 0.9995262534544019\n","Performing step 631 of gradient descent.\n","z [  254.44824571   285.39566582  -533.37803787 ...   304.95444203\n","  -909.66251331 -1115.62437293] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.27437139e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 631: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 631: 0.9995262534544019\n","Performing step 632 of gradient descent.\n","z [  254.18678872   285.25364595  -533.12739252 ...   304.68714491\n","  -909.23012707 -1115.22840223] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.92223594e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 632: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 632: 0.9995262534544019\n","Performing step 633 of gradient descent.\n","z [  253.92483676   285.11117071  -532.87685898 ...   304.4190531\n","  -908.79801412 -1114.83330279] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.75422773e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 633: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 633: 0.9995262534544019\n","Performing step 634 of gradient descent.\n","z [  253.66246183   284.96828219  -532.62636259 ...   304.15023169\n","  -908.36608262 -1114.438941  ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.82291726e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 634: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 634: 0.9995262534544019\n","Performing step 635 of gradient descent.\n","z [  253.39976694   284.82504062  -532.37580839 ...   303.88078685\n","  -907.93421934 -1114.04513874] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.19618134e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 635: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 635: 0.9995262534544019\n","Performing step 636 of gradient descent.\n","z [  253.1368898    284.68152637  -532.12507967 ...   303.61087069\n","  -907.50228905 -1113.65166984] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.96185413e-232 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 636: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 636: 0.9995262534544019\n","Performing step 637 of gradient descent.\n","z [  252.87400551   284.53784135  -531.87403769 ...   303.34068468\n","  -907.07013567 -1113.25825888] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.02338811e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 637: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 637: 0.9995262534544019\n","Performing step 638 of gradient descent.\n","z [  252.61132774   284.39410933  -531.62252283 ...   303.07048083\n","  -906.63758564 -1112.86458351] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.31604845e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 638: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 638: 0.9995262534544019\n","Performing step 639 of gradient descent.\n","z [  252.34910742   284.25047472  -531.37035796 ...   302.80055952\n","  -906.20445404 -1112.47028115] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.69350191e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 639: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 639: 0.9995262534544019\n","Performing step 640 of gradient descent.\n","z [  252.08762836   284.10709953  -531.11735419 ...   302.53126312\n","  -905.77055387 -1112.07496144] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.18104102e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 640: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 640: 0.9995262534544019\n","Performing step 641 of gradient descent.\n","z [  251.82719906   283.96415787  -530.86331959 ...   302.2629644\n","  -905.33570862 -1111.67822489] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.81183387e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 641: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 641: 0.9995262534544019\n","Performing step 642 of gradient descent.\n","z [  251.56814054   283.82182827  -530.60807066 ...   301.99604978\n","  -904.89976763 -1111.27968763] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.62946707e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 642: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 642: 0.9995262534544019\n","Performing step 643 of gradient descent.\n","z [  251.31077061   283.68028389  -530.35144579 ...   301.73089797\n","  -904.4626229  -1110.87901066] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.69130451e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 643: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 643: 0.9995262534544019\n","Performing step 644 of gradient descent.\n","z [  251.0553861    283.53968183  -530.09331929 ...   301.4678563\n","  -904.02422494 -1110.47593013] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.07290574e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 644: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 644: 0.9995262534544019\n","Performing step 645 of gradient descent.\n","z [  250.80224572   283.40015312  -529.83361348 ...   301.20721828\n","  -903.58459452 -1110.07028366] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.87381743e-231 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 645: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 645: 0.9995262534544019\n","Performing step 646 of gradient descent.\n","z [  250.55155653   283.26179534  -529.57230644 ...   300.94920664\n","  -903.143827   -1109.66202692] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.02251466e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 646: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 646: 0.9995262534544019\n","Performing step 647 of gradient descent.\n","z [  250.30346707   283.12466958  -529.30943314 ...   300.69396564\n","  -902.70208689 -1109.25123586] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.32994588e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 647: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 647: 0.9995262534544019\n","Performing step 648 of gradient descent.\n","z [  250.05806852   282.9888026   -529.0450792  ...   300.44156478\n","  -902.25959255 -1108.83809289] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.73237307e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 648: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 648: 0.9995262534544019\n","Performing step 649 of gradient descent.\n","z [  249.81540341   282.85419359  -528.77936858 ...   300.19201252\n","  -901.81659347 -1108.42285964] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.25963389e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 649: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 649: 0.9995262534544019\n","Performing step 650 of gradient descent.\n","z [  249.57547926   282.72082376  -528.51244786 ...   299.94527651\n","  -901.37334474 -1108.00584234] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.95093932e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 650: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 650: 0.9995262534544019\n","Performing step 651 of gradient descent.\n","z [  249.33828333   282.58866631  -528.24447058 ...   299.70130496\n","  -900.93008383 -1107.58735758] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.85781441e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 651: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 651: 0.9995262534544019\n","Performing step 652 of gradient descent.\n","z [  249.10379513   282.45769454  -527.97558479 ...   299.46004425\n","  -900.48701385 -1107.16770496] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.04797208e-230 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 652: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 652: 0.9995262534544019\n","Performing step 653 of gradient descent.\n","z [  248.87199456   282.32788693  -527.70592504 ...   299.22145005\n","  -900.04429493 -1106.74715013] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 6.6104144e-230 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 653: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 653: 0.9995262534544019\n","Performing step 654 of gradient descent.\n","z [  248.64286515   282.19922885  -527.43560908 ...   298.98549131\n","  -899.60204319 -1106.3259182 ] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 8.6621444e-230 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 654: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 654: 0.9995262534544019\n","Performing step 655 of gradient descent.\n","z [  248.41639345   282.07171172  -527.16473796 ...   298.75214863\n","  -899.1603352  -1105.90419517] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.13569902e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 655: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 655: 0.9995262534544019\n","Performing step 656 of gradient descent.\n","z [  248.1925661    281.94533064  -526.89339818 ...   298.5214093\n","  -898.71921519 -1105.48213373] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.48971991e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 656: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 656: 0.9995262534544019\n","Performing step 657 of gradient descent.\n","z [  247.9713662    281.82008158  -526.6216643  ...   298.29326137\n","  -898.27870276 -1105.05986049] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.95486669e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 657: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 657: 0.9995262534544019\n","Performing step 658 of gradient descent.\n","z [  247.75276996   281.69595894  -526.34960133 ...   298.06768825\n","  -897.83879971 -1104.63748252] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.56609416e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 658: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 658: 0.9995262534544019\n","Performing step 659 of gradient descent.\n","z [  247.53674456   281.57295367  -526.07726643 ...   297.8446649\n","  -897.39949525 -1104.21509209] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.36935005e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 659: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 659: 0.9995262534544019\n","Performing step 660 of gradient descent.\n","z [  247.32324704   281.45105231  -525.80470983 ...   297.62415576\n","  -896.96076954 -1103.79276963] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.42502737e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 660: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 660: 0.9995262534544019\n","Performing step 661 of gradient descent.\n","z [  247.11222436   281.33023672  -525.5319753  ...   297.40611416\n","  -896.52259605 -1103.37058531] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.81250129e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 661: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 661: 0.9995262534544019\n","Performing step 662 of gradient descent.\n","z [  246.90361419   281.21048428  -525.25910031 ...   297.19048307\n","  -896.08494289 -1102.94859953] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.63609227e-229 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 662: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 662: 0.9995262534544019\n","Performing step 663 of gradient descent.\n","z [  246.69734626   281.09176849  -524.98611607 ...   296.97719636\n","  -895.64777373 -1102.52686317] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.00329054e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 663: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 663: 0.9995262534544019\n","Performing step 664 of gradient descent.\n","z [  246.49334393   280.97405967  -524.71304771 ...   296.76618067\n","  -895.21104844 -1102.10541764] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 1.3183138e-228 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 664: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 664: 0.9995262534544019\n","Performing step 665 of gradient descent.\n","z [  246.29152582   280.85732578  -524.4399145  ...   296.5573572\n","  -894.77472368 -1101.68429509] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.73236358e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 665: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 665: 0.9995262534544019\n","Performing step 666 of gradient descent.\n","z [  246.09180732   280.74153318  -524.16673028 ...   296.3506435\n","  -894.33875355 -1101.26351893] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.27657228e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 666: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 666: 0.9995262534544019\n","Performing step 667 of gradient descent.\n","z [  245.89410203   280.62664721  -523.893504   ...   296.14595506\n","  -893.90309031 -1100.84310449] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.99186578e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 667: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 667: 0.9994472956968022\n","Performing step 668 of gradient descent.\n","z [  245.69832295   280.51263283  -523.62024038 ...   295.94320661\n","  -893.46768511 -1100.42305997] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.93204964e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 668: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 668: 0.9994472956968022\n","Performing step 669 of gradient descent.\n","z [  245.50438345   280.39945504  -523.34694055 ...   295.74231322\n","  -893.0324888  -1100.00338747] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.16787019e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 669: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 669: 0.9994472956968022\n","Performing step 670 of gradient descent.\n","z [  245.31219803   280.28707919  -523.07360288 ...   295.54319111\n","  -892.59745272 -1099.58408411] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.79235914e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 670: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 670: 0.9994472956968022\n","Performing step 671 of gradient descent.\n","z [  245.12168297   280.17547121  -522.80022362 ...   295.34575824\n","  -892.16252956 -1099.16514334] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.92786765e-228 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 671: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 671: 0.9994472956968022\n","Performing step 672 of gradient descent.\n","z [  244.93275664   280.06459779  -522.52679775 ...   295.14993467\n","  -891.72767417 -1098.74655609] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.17353242e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 672: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 672: 0.9994472956968022\n","Performing step 673 of gradient descent.\n","z [  244.74533978   279.95442635  -522.25331968 ...   294.95564267\n","  -891.29284444 -1098.32831224] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.54264184e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 673: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 673: 0.9994472956968022\n","Performing step 674 of gradient descent.\n","z [  244.55935547   279.84492498  -521.97978415 ...   294.76280667\n","  -890.85800231 -1097.91040204] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.02796324e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 674: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 674: 0.9995262534544019\n","Performing step 675 of gradient descent.\n","z [  244.37472892   279.73606227  -521.70618716 ...   294.57135286\n","  -890.42311501 -1097.49281796] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.66613259e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 675: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 675: 0.9995262534544019\n","Performing step 676 of gradient descent.\n","z [  244.19138701   279.62780687  -521.43252714 ...   294.38120846\n","  -889.98815664 -1097.07555695] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.50534507e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 676: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 676: 0.9995262534544019\n","Performing step 677 of gradient descent.\n","z [  244.00925748   279.52012695  -521.15880659 ...   294.19230062\n","  -889.55311042 -1096.65862351] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.60899359e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 677: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 677: 0.9995262534544019\n","Performing step 678 of gradient descent.\n","z [  243.82826757   279.41298925  -520.88503417 ...   294.00455452\n","  -889.11797194 -1096.24203402] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.06043717e-227 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 678: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 678: 0.9995262534544019\n","Performing step 679 of gradient descent.\n","z [  243.64834204   279.30635764  -520.61122793 ...   293.81789065\n","  -888.68275402 -1095.82582301] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 7.9692324e-227 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 679: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 679: 0.9995262534544019\n","Performing step 680 of gradient descent.\n","z [  243.46940017   279.20019112  -520.33741983 ...   293.63222071\n","  -888.24749377 -1095.41005226] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.04792413e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 680: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 680: 0.9995262534544019\n","Performing step 681 of gradient descent.\n","z [  243.29135135   279.09444066  -520.06366243 ...   293.44744158\n","  -887.81226317 -1094.99482424] (12665,)\n","y hat: [1.000000e+000 1.000000e+000 1.377911e-226 ... 1.000000e+000 0.000000e+000\n"," 0.000000e+000]\n","Loss at step 681: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 681: 0.9995262534544019\n","Performing step 682 of gradient descent.\n","z [  243.11408874   278.98904488  -519.79003856 ...   293.26342664\n","  -887.37718444 -1094.58030133] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.81156744e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 682: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 682: 0.9995262534544019\n","Performing step 683 of gradient descent.\n","z [  242.9374805    278.88392382  -519.51667493 ...   293.08001374\n","  -886.94245182 -1094.16673314] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.38108456e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 683: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 683: 0.9995262534544019\n","Performing step 684 of gradient descent.\n","z [  242.76135812   278.77897076  -519.24376066 ...   292.89698933\n","  -886.5083613  -1093.75449317] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.12823939e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 684: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 684: 0.9995262534544019\n","Performing step 685 of gradient descent.\n","z [  242.58550241   278.6740423   -518.97157062 ...   292.71406934\n","  -886.07534842 -1093.34412492] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.10686679e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 685: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 685: 0.9995262534544019\n","Performing step 686 of gradient descent.\n","z [  242.40962961   278.5689482   -518.70049128 ...   292.5308804\n","  -885.64403055 -1092.93639203] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.38565929e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 686: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 686: 0.9995262534544019\n","Performing step 687 of gradient descent.\n","z [  242.23338387   278.46344538  -518.4310419  ...   292.34695032\n","  -885.21524238 -1092.53231695] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 7.0511387e-226 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 687: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 687: 0.9996052112120016\n","Performing step 688 of gradient descent.\n","z [  242.05634814   278.35724411  -518.16387588 ...   292.16172443\n","  -884.79004125 -1092.13317704] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.21060134e-226 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 688: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 688: 0.9996052112120016\n","Performing step 689 of gradient descent.\n","z [  241.87808973   278.25003754  -517.89974064 ...   291.9746304\n","  -884.36964752 -1091.7404126 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.19950057e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 689: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 689: 0.9996052112120016\n","Performing step 690 of gradient descent.\n","z [  241.69825054   278.14156184  -517.63937751 ...   291.7852054\n","  -883.95529016 -1091.35541071] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.55623339e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 690: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 690: 0.9996052112120016\n","Performing step 691 of gradient descent.\n","z [  241.51666595   278.03167692  -517.38337146 ...   291.59326242\n","  -883.54797074 -1090.97918891] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.01028089e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 691: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 691: 0.9996052112120016\n","Performing step 692 of gradient descent.\n","z [  241.3334564    277.92043069  -517.13200883 ...   291.39901706\n","  -883.1482348  -1090.61210461] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.58477146e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 692: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 692: 0.9996052112120016\n","Performing step 693 of gradient descent.\n","z [  241.14902394   277.80806099  -516.88522564 ...   291.20307963\n","  -882.75607949 -1090.25376527] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.30825311e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 693: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 693: 0.9996052112120016\n","Performing step 694 of gradient descent.\n","z [  240.96393821   277.69492368  -516.64268227 ...   291.00629206\n","  -882.37105675 -1089.90320993] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.21632398e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 694: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 694: 0.9996052112120016\n","Performing step 695 of gradient descent.\n","z [  240.77877669   277.58138839  -516.40391229 ...   290.80950139\n","  -881.9924956  -1089.55924438] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.35340944e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 695: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 695: 0.9996052112120016\n","Performing step 696 of gradient descent.\n","z [  240.59400491   277.46775873  -516.16845627 ...   290.61339085\n","  -881.61970765 -1089.22073625] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.77466318e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 696: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 696: 0.9996841689696012\n","Performing step 697 of gradient descent.\n","z [  240.40993294   277.35424177  -515.93593208 ...   290.41841941\n","  -881.25209918 -1088.88676629] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.54814114e-225 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 697: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 697: 0.9996841689696012\n","Performing step 698 of gradient descent.\n","z [  240.22672987   277.24095557  -515.70604789 ...   290.2248434\n","  -880.88919717 -1088.55665227] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.07574446e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 698: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 698: 0.9996841689696012\n","Performing step 699 of gradient descent.\n","z [  240.04446206   277.12795254  -515.47858658 ...   290.03277168\n","  -880.53063076 -1088.22990868] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.35049913e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 699: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 699: 0.9996841689696012\n","Performing step 700 of gradient descent.\n","z [  239.86313151   277.01524338  -515.25338356 ...   289.84222074\n","  -880.17610174 -1087.90619184] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.69160414e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 700: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 700: 0.9996841689696012\n","Performing step 701 of gradient descent.\n","z [  239.68270521   276.90281528  -515.03030839 ...   289.65315692\n","  -879.82535951 -1087.58525342] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.11436058e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 701: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 701: 0.9996841689696012\n","Performing step 702 of gradient descent.\n","z [  239.50313462   276.79064402  -514.80925252 ...   289.46552446\n","  -879.47818426 -1087.26690762] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.63743884e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 702: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 702: 0.9996841689696012\n","Performing step 703 of gradient descent.\n","z [  239.32436755   276.67870119  -514.59012201 ...   289.27926266\n","  -879.13437711 -1086.95101021] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.28359489e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 703: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 703: 0.9996841689696012\n","Performing step 704 of gradient descent.\n","z [  239.1463549    276.56695818  -514.37283376 ...   289.09431587\n","  -878.793755   -1086.637446  ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.08053077e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 704: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 704: 0.9996841689696012\n","Performing step 705 of gradient descent.\n","z [  238.96905434   276.45538834  -514.15731398 ...   288.91063881\n","  -878.45614838 -1086.32612168] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.06192503e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 705: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 705: 0.9996841689696012\n","Performing step 706 of gradient descent.\n","z [  238.7924321    276.34396789  -513.94349783 ...   288.7281994\n","  -878.12140061 -1086.01696193] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.26866239e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 706: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 706: 0.9996841689696012\n","Performing step 707 of gradient descent.\n","z [  238.61646384   276.23267631  -513.73133002 ...   288.54697993\n","  -877.78936829 -1085.70990775] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.75029423e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 707: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 707: 0.9996841689696012\n","Performing step 708 of gradient descent.\n","z [  238.44113485   276.12149645  -513.52076601 ...   288.36697742\n","  -877.45992233 -1085.40491618] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.56676217e-224 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 708: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 708: 0.9996841689696012\n","Performing step 709 of gradient descent.\n","z [  238.26644006   276.01041441  -513.31177404 ...   288.18820343\n","  -877.13294981 -1085.1019613 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.17904136e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 709: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 709: 0.9996841689696012\n","Performing step 710 of gradient descent.\n","z [  238.09238391   275.89941937  -513.10433813 ...   288.01068345\n","  -876.80835697 -1084.80103663] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.45083261e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 710: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 710: 0.9996841689696012\n","Performing step 711 of gradient descent.\n","z [  237.91898004   275.78850341  -512.89846267 ...   287.834456\n","  -876.48607365 -1084.50215948] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.78249322e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 711: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 711: 0.9996841689696012\n","Performing step 712 of gradient descent.\n","z [  237.74625113   275.67766113  -512.6941791  ...   287.6595715\n","  -876.16606025 -1084.2053778 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.18648807e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 712: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 712: 0.9996841689696012\n","Performing step 713 of gradient descent.\n","z [  237.57422868   275.56688933  -512.49155579 ...   287.48609089\n","  -875.8483178  -1083.91078074] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.67759753e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 713: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 713: 0.9996841689696012\n","Performing step 714 of gradient descent.\n","z [  237.40295316   275.45618642  -512.29071194 ...   287.31408433\n","  -875.53290244 -1083.61851382] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.27318592e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 714: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 714: 0.9996841689696012\n","Performing step 715 of gradient descent.\n","z [  237.23247466   275.34555202  -512.09183657 ...   287.14363034\n","  -875.21994508 -1083.32879973] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.99338471e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 715: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 715: 0.9996841689696012\n","Performing step 716 of gradient descent.\n","z [  237.06285486   275.23498681  -511.89521262 ...   286.97481639\n","  -874.90967614 -1083.04196427] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.86109206e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 716: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 716: 0.9996841689696012\n","Performing step 717 of gradient descent.\n","z [  236.89417128   275.12449344  -511.70124441 ...   286.80774266\n","  -874.60245309 -1082.75846428] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.90164618e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 717: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 717: 0.9996841689696012\n","Performing step 718 of gradient descent.\n","z [  236.72652581   275.01408007  -511.51048247 ...   286.64253184\n","  -874.29878385 -1082.47890878] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.14200302e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 718: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 718: 0.9996841689696012\n","Performing step 719 of gradient descent.\n","z [  236.56005954   274.9037684   -511.32363355 ...   286.47934884\n","  -873.99933223 -1082.20405644] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.60929288e-223 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 719: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 719: 0.9996841689696012\n","Performing step 720 of gradient descent.\n","z [  236.39497548   274.79360862  -511.14153695 ...   286.31843359\n","  -873.70488466 -1081.93476486] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.03288277e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 720: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 720: 0.9996841689696012\n","Performing step 721 of gradient descent.\n","z [  236.23156688   274.68370124  -510.9650873  ...   286.16014506\n","  -873.41625805 -1081.67186981] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.23220266e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 721: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 721: 0.9996841689696012\n","Performing step 722 of gradient descent.\n","z [  236.07024062   274.57421993  -510.79509997 ...   286.0050023\n","  -873.13414833 -1081.41599894] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.46051728e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 722: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 722: 0.9996841689696012\n","Performing step 723 of gradient descent.\n","z [  235.91151608   274.46542204  -510.63215023 ...   285.85369361\n","  -872.85895976 -1081.16737622] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.71899603e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 723: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 723: 0.9996841689696012\n","Performing step 724 of gradient descent.\n","z [  235.75598082   274.35763114  -510.47645195 ...   285.70702431\n","  -872.59069202 -1080.92571462] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.00860147e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 724: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 724: 0.9996841689696012\n","Performing step 725 of gradient descent.\n","z [  235.60420488   274.25118848  -510.32783652 ...   285.56580107\n","  -872.3289486  -1080.69026966] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.33043309e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 725: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 725: 0.9997631267272009\n","Performing step 726 of gradient descent.\n","z [  235.45664486   274.14639149  -510.18583715 ...   285.43069484\n","  -872.07306077 -1080.46003137] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.68600107e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 726: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 726: 0.9998420844848006\n","Performing step 727 of gradient descent.\n","z [  235.31357892   274.04344675  -510.04982484 ...   285.30214174\n","  -871.82225474 -1080.23395125] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.07734067e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 727: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 727: 0.9998420844848006\n","Performing step 728 of gradient descent.\n","z [  235.17509321   273.94245411  -509.91913143 ...   285.1803143\n","  -871.57578595 -1080.0111058 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.50699384e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 728: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 728: 0.9998420844848006\n","Performing step 729 of gradient descent.\n","z [  235.04111068   273.84341863  -509.79312685 ...   285.06515355\n","  -871.33300928 -1079.79076471] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.9779388e-222 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 729: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 729: 0.9998420844848006\n","Performing step 730 of gradient descent.\n","z [  234.91143882   273.74627646  -509.67125229 ...   284.95643067\n","  -871.0933957  -1079.57238747] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.49352899e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 730: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 730: 0.9998420844848006\n","Performing step 731 of gradient descent.\n","z [  234.78581672   273.6509219   -509.55302555 ...   284.85381035\n","  -870.85652022 -1079.35558672] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.05746358e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 731: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 731: 0.9998420844848006\n","Performing step 732 of gradient descent.\n","z [  234.66395174   273.55722894  -509.43803387 ...   284.75690164\n","  -870.62204065 -1079.14008591] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.67378687e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 732: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 732: 0.9998420844848006\n","Performing step 733 of gradient descent.\n","z [  234.54554408   273.46506594  -509.32592312 ...   284.66529331\n","  -870.38967744 -1078.92568414] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.34690648e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 733: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 733: 0.9999210422424003\n","Performing step 734 of gradient descent.\n","z [  234.43030158   273.37430431  -509.21638763 ...   284.57857604\n","  -870.1591978  -1078.71223076] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.08162225e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 734: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 734: 0.9999210422424003\n","Performing step 735 of gradient descent.\n","z [  234.31794764   273.28482311  -509.10916172 ...   284.49635518\n","  -869.93040419 -1078.49960864] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 7.8831607e-222 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 735: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 735: 0.9999210422424003\n","Performing step 736 of gradient descent.\n","z [  234.20822486   273.19651109  -509.00401292 ...   284.41825748\n","  -869.70312633 -1078.28772344] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.75721315e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 736: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 736: 0.9999210422424003\n","Performing step 737 of gradient descent.\n","z [  234.10089614   273.10926722  -508.90073675 ...   284.34393415\n","  -869.47721543 -1078.07649681] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.70997685e-222 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 737: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 737: 0.9999210422424003\n","Performing step 738 of gradient descent.\n","z [  233.9957445    273.02300045  -508.79915241 ...   284.27306173\n","  -869.25254012 -1077.86586214] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.07481993e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 738: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 738: 0.9999210422424003\n","Performing step 739 of gradient descent.\n","z [  233.89257212   272.93762915  -508.69909944 ...   284.20534185\n","  -869.02898332 -1077.65576167] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.18792266e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 739: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 739: 0.9999210422424003\n","Performing step 740 of gradient descent.\n","z [  233.79119917   272.85308037  -508.60043487 ...   284.14050019\n","  -868.80643991 -1077.44614454] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.31110551e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 740: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 740: 0.9999210422424003\n","Performing step 741 of gradient descent.\n","z [  233.69146238   272.76928904  -508.5030309  ...   284.07828523\n","  -868.58481478 -1077.23696539] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.44523892e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 741: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 741: 0.9999210422424003\n","Performing step 742 of gradient descent.\n","z [  233.59321377   272.68619727  -508.40677291 ...   284.01846676\n","  -868.36402132 -1077.02818334] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.59127033e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 742: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 742: 0.9999210422424003\n","Performing step 743 of gradient descent.\n","z [  233.4963193    272.60375369  -508.31155777 ...   283.96083437\n","  -868.14398015 -1076.81976113] (12665,)\n","y hat: [1.000000e+000 1.000000e+000 1.750231e-221 ... 1.000000e+000 0.000000e+000\n"," 0.000000e+000]\n","Loss at step 743: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 743: 0.9999210422424003\n","Performing step 744 of gradient descent.\n","z [  233.40065766   272.52191278  -508.21729248 ...   283.90519591\n","  -867.92461811 -1076.61166453] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.92324348e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 744: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 744: 0.9999210422424003\n","Performing step 745 of gradient descent.\n","z [  233.30611912   272.4406343   -508.12389287 ...   283.85137605\n","  -867.70586739 -1076.40386184] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.11152974e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 745: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 745: 0.9999210422424003\n","Performing step 746 of gradient descent.\n","z [  233.21260446   272.35988284  -508.03128264 ...   283.79921485\n","  -867.4876648  -1076.19632347] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.31642005e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 746: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 746: 0.9999210422424003\n","Performing step 747 of gradient descent.\n","z [  233.120024     272.27962727  -507.9393924  ...   283.74856646\n","  -867.26995122 -1075.98902171] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.53936271e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 747: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 747: 0.9999210422424003\n","Performing step 748 of gradient descent.\n","z [  233.02829668   272.19984036  -507.84815895 ...   283.69929786\n","  -867.05267109 -1075.7819304 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.78193466e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 748: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 748: 0.9999210422424003\n","Performing step 749 of gradient descent.\n","z [  232.93734927   272.12049835  -507.75752457 ...   283.65128772\n","  -866.83577205 -1075.57502485] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.04585296e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 749: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 749: 0.9999210422424003\n","Performing step 750 of gradient descent.\n","z [  232.84711558   272.04158065  -507.66743651 ...   283.60442535\n","  -866.61920457 -1075.36828162] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.33298746e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 750: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 750: 0.9999210422424003\n","Performing step 751 of gradient descent.\n","z [  232.75753581   271.96306946  -507.57784645 ...   283.55860969\n","  -866.40292175 -1075.1616785 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.64537448e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 751: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 751: 0.9999210422424003\n","Performing step 752 of gradient descent.\n","z [  232.66855587   271.88494946  -507.48871008 ...   283.51374845\n","  -866.18687906 -1074.95519442] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.98523176e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 752: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 752: 0.9999210422424003\n","Performing step 753 of gradient descent.\n","z [  232.58012685   271.80720757  -507.39998675 ...   283.46975725\n","  -865.97103422 -1074.74880943] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 4.3549747e-221 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 753: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 753: 0.9999210422424003\n","Performing step 754 of gradient descent.\n","z [  232.49220445   271.72983265  -507.31163912 ...   283.42655891\n","  -865.75534707 -1074.54250466] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 4.7572341e-221 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 754: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 754: 0.9999210422424003\n","Performing step 755 of gradient descent.\n","z [  232.40474855   271.65281527  -507.22363292 ...   283.38408275\n","  -865.53977944 -1074.33626234] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.19487535e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 755: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 755: 0.9999210422424003\n","Performing step 756 of gradient descent.\n","z [  232.31772276   271.57614747  -507.13593666 ...   283.34226397\n","  -865.32429515 -1074.13006583] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.67101939e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 756: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 756: 0.9999210422424003\n","Performing step 757 of gradient descent.\n","z [  232.23109399   271.49982256  -507.04852146 ...   283.30104309\n","  -865.10885993 -1073.9238996 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.18906542e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 757: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 757: 0.9999210422424003\n","Performing step 758 of gradient descent.\n","z [  232.14483213   271.42383495  -506.96136082 ...   283.26036549\n","  -864.89344138 -1073.71774928] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.75271559e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 758: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 758: 0.9999210422424003\n","Performing step 759 of gradient descent.\n","z [  232.05890972   271.3481799   -506.87443048 ...   283.22018086\n","  -864.67800899 -1073.51160169] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.36600183e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 759: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 759: 0.9999210422424003\n","Performing step 760 of gradient descent.\n","z [  231.97330164   271.27285341  -506.78770829 ...   283.18044288\n","  -864.46253411 -1073.30544483] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.03331497e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 760: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 760: 0.9999210422424003\n","Performing step 761 of gradient descent.\n","z [  231.88798488   271.19785206  -506.701174   ...   283.14110885\n","  -864.24698995 -1073.09926794] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.75943631e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 761: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 761: 0.9999210422424003\n","Performing step 762 of gradient descent.\n","z [  231.80293827   271.12317284  -506.61480922 ...   283.10213928\n","  -864.03135159 -1072.89306146] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.54957197e-221 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 762: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 762: 0.9999210422424003\n","Performing step 763 of gradient descent.\n","z [  231.71814228   271.04881306  -506.52859726 ...   283.06349771\n","  -863.81559597 -1072.68681708] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.04093901e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 763: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 763: 0.9999210422424003\n","Performing step 764 of gradient descent.\n","z [  231.63357885   270.97477021  -506.44252302 ...   283.02515039\n","  -863.59970192 -1072.48052769] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.13450614e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 764: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 764: 0.9999210422424003\n","Performing step 765 of gradient descent.\n","z [  231.54923124   270.90104188  -506.35657291 ...   282.98706606\n","  -863.38365006 -1072.27418739] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.23633029e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 765: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 765: 0.9999210422424003\n","Performing step 766 of gradient descent.\n","z [  231.46508384   270.82762568  -506.27073474 ...   282.94921576\n","  -863.1674229  -1072.06779145] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.34714254e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 766: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 766: 0.9999210422424003\n","Performing step 767 of gradient descent.\n","z [  231.3811221    270.75451914  -506.18499762 ...   282.91157266\n","  -862.9510047  -1071.86133625] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.46773859e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 767: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 767: 0.9999210422424003\n","Performing step 768 of gradient descent.\n","z [  231.2973324    270.68171967  -506.09935185 ...   282.87411189\n","  -862.73438154 -1071.65481926] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.59898429e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 768: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 768: 0.9999210422424003\n","Performing step 769 of gradient descent.\n","z [  231.21370195   270.60922452  -506.01378887 ...   282.83681043\n","  -862.51754117 -1071.44823895] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.74182182e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 769: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 769: 0.9999210422424003\n","Performing step 770 of gradient descent.\n","z [  231.13021875   270.53703073  -505.92830114 ...   282.79964695\n","  -862.30047303 -1071.24159474] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.89727627e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 770: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 770: 0.9999210422424003\n","Performing step 771 of gradient descent.\n","z [  231.04687151   270.46513512  -505.84288206 ...   282.76260174\n","  -862.08316815 -1071.0348869 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.06646289e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 771: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 771: 0.9999210422424003\n","Performing step 772 of gradient descent.\n","z [  230.96364957   270.39353424  -505.75752589 ...   282.72565661\n","  -861.86561911 -1070.8281165 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.25059487e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 772: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 772: 0.9999210422424003\n","Performing step 773 of gradient descent.\n","z [  230.8805429    270.32222441  -505.67222765 ...   282.68879477\n","  -861.64781988 -1070.62128528] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.45099193e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 773: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 773: 0.9999210422424003\n","Performing step 774 of gradient descent.\n","z [  230.79754205   270.25120167  -505.58698304 ...   282.6520008\n","  -861.42976585 -1070.41439562] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.66908953e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 774: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 774: 0.9999210422424003\n","Performing step 775 of gradient descent.\n","z [  230.7146381    270.18046182  -505.50178841 ...   282.61526054\n","  -861.21145363 -1070.20745038] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.90644896e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 775: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 775: 0.9999210422424003\n","Performing step 776 of gradient descent.\n","z [  230.63182265   270.11000044  -505.41664062 ...   282.57856102\n","  -860.99288103 -1070.00045287] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.16476827e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 776: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 776: 0.9999210422424003\n","Performing step 777 of gradient descent.\n","z [  230.54908778   270.03981285  -505.33153701 ...   282.54189045\n","  -860.77404692 -1069.79340674] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.44589426e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 777: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 777: 0.9999210422424003\n","Performing step 778 of gradient descent.\n","z [  230.46642607   269.96989418  -505.24647532 ...   282.50523806\n","  -860.55495115 -1069.58631593] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.75183533e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 778: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 778: 0.9999210422424003\n","Performing step 779 of gradient descent.\n","z [  230.38383052   269.90023939  -505.16145364 ...   282.46859413\n","  -860.33559447 -1069.37918454] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 4.0847757e-220 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 779: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 779: 0.9999210422424003\n","Performing step 780 of gradient descent.\n","z [  230.30129461   269.83084325  -505.07647036 ...   282.43194988\n","  -860.11597841 -1069.17201682] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.44709065e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 780: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 780: 0.9999210422424003\n","Performing step 781 of gradient descent.\n","z [  230.21881219   269.7617004   -504.99152411 ...   282.39529743\n","  -859.89610519 -1068.96481706] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.84136328e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 781: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 781: 0.9999210422424003\n","Performing step 782 of gradient descent.\n","z [  230.13637754   269.69280536  -504.9066137  ...   282.35862974\n","  -859.67597765 -1068.75758957] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.27040264e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 782: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 782: 0.9999210422424003\n","Performing step 783 of gradient descent.\n","z [  230.05398533   269.62415256  -504.8217381  ...   282.32194053\n","  -859.45559917 -1068.55033862] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 5.7372635e-220 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 783: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 783: 0.9999210422424003\n","Performing step 784 of gradient descent.\n","z [  229.97163058   269.55573635  -504.73689643 ...   282.28522427\n","  -859.23497357 -1068.34306838] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.24526785e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 784: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 784: 0.9999210422424003\n","Performing step 785 of gradient descent.\n","z [  229.88930868   269.48755104  -504.65208785 ...   282.2484761\n","  -859.01410505 -1068.13578291] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.79802825e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 785: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 785: 0.9999210422424003\n","Performing step 786 of gradient descent.\n","z [  229.80701534   269.41959088  -504.56731163 ...   282.21169175\n","  -858.79299812 -1067.92848611] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.39947335e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 786: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 786: 0.9999210422424003\n","Performing step 787 of gradient descent.\n","z [  229.72474659   269.35185014  -504.48256705 ...   282.17486756\n","  -858.57165755 -1067.72118171] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.05387555e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 787: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 787: 0.9999210422424003\n","Performing step 788 of gradient descent.\n","z [  229.64249877   269.28432309  -504.39785342 ...   282.13800037\n","  -858.35008829 -1067.51387325] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.76588112e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 788: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 788: 0.9999210422424003\n","Performing step 789 of gradient descent.\n","z [  229.5602685    269.21700398  -504.31317006 ...   282.1010875\n","  -858.12829544 -1067.30656404] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.54054301e-220 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 789: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 789: 0.9999210422424003\n","Performing step 790 of gradient descent.\n","z [  229.47805266   269.14988715  -504.22851627 ...   282.0641267\n","  -857.9062842  -1067.09925722] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.03833565e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 790: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 790: 0.9999210422424003\n","Performing step 791 of gradient descent.\n","z [  229.39584839   269.08296695  -504.14389135 ...   282.02711611\n","  -857.68405983 -1066.89195564] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.13002982e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 791: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 791: 0.9999210422424003\n","Performing step 792 of gradient descent.\n","z [  229.31365305   269.01623781  -504.05929456 ...   281.99005422\n","  -857.46162759 -1066.68466199] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.22978679e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 792: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 792: 0.9999210422424003\n","Performing step 793 of gradient descent.\n","z [  229.23146424   268.94969421  -503.97472513 ...   281.95293984\n","  -857.23899276 -1066.47737868] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 1.3383135e-219 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 793: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 793: 0.9999210422424003\n","Performing step 794 of gradient descent.\n","z [  229.14927974   268.88333071  -503.89018226 ...   281.91577205\n","  -857.01616056 -1066.27010793] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.45637883e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 794: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 794: 0.9999210422424003\n","Performing step 795 of gradient descent.\n","z [  229.06709754   268.81714197  -503.80566513 ...   281.8785502\n","  -856.79313614 -1066.06285174] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.58481906e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 795: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 795: 0.9999210422424003\n","Performing step 796 of gradient descent.\n","z [  228.98491579   268.75112273  -503.72117283 ...   281.84127385\n","  -856.56992459 -1065.85561187] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.72454377e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 796: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 796: 0.9999210422424003\n","Performing step 797 of gradient descent.\n","z [  228.90273281   268.68526783  -503.63670447 ...   281.80394275\n","  -856.34653089 -1065.6483899 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.87654232e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 797: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 797: 0.9999210422424003\n","Performing step 798 of gradient descent.\n","z [  228.82054707   268.6195722   -503.55225909 ...   281.76655683\n","  -856.12295989 -1065.44118719] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.04189087e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 798: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 798: 0.9999210422424003\n","Performing step 799 of gradient descent.\n","z [  228.73835718   268.5540309   -503.46783569 ...   281.72911616\n","  -855.89921635 -1065.23400494] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.22175999e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 799: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 799: 0.9999210422424003\n","Performing step 800 of gradient descent.\n","z [  228.65616188   268.48863906  -503.38343326 ...   281.69162096\n","  -855.67530485 -1065.02684414] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.41742303e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 800: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 800: 0.9999210422424003\n","Performing step 801 of gradient descent.\n","z [  228.57396001   268.42339197  -503.29905073 ...   281.65407154\n","  -855.45122986 -1064.81970562] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.6302651e-219 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 801: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 801: 0.9999210422424003\n","Performing step 802 of gradient descent.\n","z [  228.49175054   268.35828498  -503.21468701 ...   281.61646832\n","  -855.2269957  -1064.61259006] (12665,)\n","y hat: [1.000000e+000 1.000000e+000 2.861793e-219 ... 1.000000e+000 0.000000e+000\n"," 0.000000e+000]\n","Loss at step 802: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 802: 0.9999210422424003\n","Performing step 803 of gradient descent.\n","z [  228.40953254   268.2933136   -503.13034101 ...   281.5788118\n","  -855.0026065  -1064.40549796] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.11364591e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 803: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 803: 0.9999210422424003\n","Performing step 804 of gradient descent.\n","z [  228.32730515   268.22847341  -503.04601157 ...   281.54110255\n","  -854.77806629 -1064.1984297 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.38760708e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 804: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 804: 0.9999210422424003\n","Performing step 805 of gradient descent.\n","z [  228.24506763   268.16376013  -502.96169754 ...   281.50334121\n","  -854.5533789  -1063.99138553] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.68561654e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 805: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 805: 0.9999210422424003\n","Performing step 806 of gradient descent.\n","z [  228.16281928   268.0991696   -502.87739776 ...   281.46552845\n","  -854.32854802 -1063.78436556] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.00978493e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 806: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 806: 0.9999210422424003\n","Performing step 807 of gradient descent.\n","z [  228.0805595    268.03469774  -502.79311103 ...   281.427665\n","  -854.10357719 -1063.57736979] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 4.3624086e-219 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 807: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 807: 0.9999210422424003\n","Performing step 808 of gradient descent.\n","z [  227.99828774   267.97034063  -502.70883616 ...   281.38975161\n","  -853.87846978 -1063.37039812] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.74598599e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 808: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 808: 0.9999210422424003\n","Performing step 809 of gradient descent.\n","z [  227.91600353   267.90609442  -502.62457194 ...   281.35178906\n","  -853.65322902 -1063.16345035] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.16323554e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 809: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 809: 0.9999210422424003\n","Performing step 810 of gradient descent.\n","z [  227.83370642   267.84195538  -502.54031717 ...   281.31377817\n","  -853.42785799 -1062.95652619] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.61711506e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 810: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 810: 0.9999210422424003\n","Performing step 811 of gradient descent.\n","z [  227.75139605   267.7779199   -502.45607063 ...   281.27571976\n","  -853.20235961 -1062.74962525] (12665,)\n","y hat: [1.000000e+000 1.000000e+000 6.110843e-219 ... 1.000000e+000 0.000000e+000\n"," 0.000000e+000]\n","Loss at step 811: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 811: 0.9999210422424003\n","Performing step 812 of gradient descent.\n","z [  227.66907208   267.71398448  -502.37183112 ...   281.23761465\n","  -852.97673667 -1062.54274709] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.64792146e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 812: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 812: 0.9999210422424003\n","Performing step 813 of gradient descent.\n","z [  227.58673425   267.65014571  -502.28759743 ...   281.19946371\n","  -852.75099182 -1062.33589119] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.23216134e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 813: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 813: 0.9999210422424003\n","Performing step 814 of gradient descent.\n","z [  227.50438229   267.58640029  -502.20336837 ...   281.16126777\n","  -852.52512757 -1062.12905697] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.86770965e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 814: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 814: 0.9999210422424003\n","Performing step 815 of gradient descent.\n","z [  227.422016     267.52274503  -502.11914274 ...   281.1230277\n","  -852.29914631 -1061.92224378] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.55907934e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 815: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 815: 0.9999210422424003\n","Performing step 816 of gradient descent.\n","z [  227.33963522   267.45917683  -502.03491935 ...   281.08474434\n","  -852.07305029 -1061.71545094] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.31118171e-219 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 816: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 816: 0.9999210422424003\n","Performing step 817 of gradient descent.\n","z [  227.25723979   267.39569268  -501.95069706 ...   281.04641857\n","  -851.84684164 -1061.50867771] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.01293618e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 817: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 817: 0.9999210422424003\n","Performing step 818 of gradient descent.\n","z [  227.17482962   267.3322897   -501.86647469 ...   281.00805122\n","  -851.62052239 -1061.30192332] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.10194366e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 818: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 818: 0.9999210422424003\n","Performing step 819 of gradient descent.\n","z [  227.09240461   267.26896506  -501.78225111 ...   280.96964315\n","  -851.39409444 -1061.09518697] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.19877376e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 819: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 819: 0.9999210422424003\n","Performing step 820 of gradient descent.\n","z [  227.00996472   267.20571605  -501.6980252  ...   280.9311952\n","  -851.1675596  -1060.88846779] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.30411555e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 820: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 820: 0.9999210422424003\n","Performing step 821 of gradient descent.\n","z [  226.9275099    267.14254005  -501.61379586 ...   280.89270822\n","  -850.94091955 -1060.68176494] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.41871908e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 821: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 821: 0.9999210422424003\n","Performing step 822 of gradient descent.\n","z [  226.84504014   267.0794345   -501.52956201 ...   280.85418304\n","  -850.71417589 -1060.47507751] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.54340076e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 822: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 822: 0.9999210422424003\n","Performing step 823 of gradient descent.\n","z [  226.76255545   267.01639697  -501.44532257 ...   280.81562047\n","  -850.48733014 -1060.2684046 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.67904923e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 823: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 823: 0.9999210422424003\n","Performing step 824 of gradient descent.\n","z [  226.68005586   266.95342507  -501.36107652 ...   280.77702135\n","  -850.26038371 -1060.06174529] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.82663184e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 824: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 824: 0.9999210422424003\n","Performing step 825 of gradient descent.\n","z [  226.5975414    266.89051651  -501.27682284 ...   280.73838648\n","  -850.03333793 -1059.85509863] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.98720163e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 825: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 825: 0.9999210422424003\n","Performing step 826 of gradient descent.\n","z [  226.51501214   266.82766909  -501.19256053 ...   280.69971666\n","  -849.80619405 -1059.64846367] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.16190493e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 826: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 826: 0.9999210422424003\n","Performing step 827 of gradient descent.\n","z [  226.43246814   266.76488066  -501.10828862 ...   280.66101269\n","  -849.57895325 -1059.44183946] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.35198971e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 827: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 827: 0.9999210422424003\n","Performing step 828 of gradient descent.\n","z [  226.34990951   266.70214916  -501.02400616 ...   280.62227535\n","  -849.35161663 -1059.23522504] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.55881462e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 828: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 828: 0.9999210422424003\n","Performing step 829 of gradient descent.\n","z [  226.26733633   266.63947261  -500.93971224 ...   280.58350542\n","  -849.12418524 -1059.02861944] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.78385883e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 829: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 829: 0.9999210422424003\n","Performing step 830 of gradient descent.\n","z [  226.18474872   266.57684908  -500.85540596 ...   280.54470367\n","  -848.89666003 -1058.82202171] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.0287328e-218 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 830: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 830: 0.9999210422424003\n","Performing step 831 of gradient descent.\n","z [  226.1021468    266.51427672  -500.77108645 ...   280.50587085\n","  -848.66904192 -1058.61543089] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.29518999e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 831: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 831: 0.9999210422424003\n","Performing step 832 of gradient descent.\n","z [  226.0195307    266.45175376  -500.68675286 ...   280.46700772\n","  -848.44133177 -1058.408846  ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.58513959e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 832: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 832: 0.9999210422424003\n","Performing step 833 of gradient descent.\n","z [  225.93690057   266.38927846  -500.60240438 ...   280.42811501\n","  -848.21353038 -1058.20226612] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.90066046e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 833: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 833: 0.9999210422424003\n","Performing step 834 of gradient descent.\n","z [  225.85425655   266.32684917  -500.51804022 ...   280.38919346\n","  -847.98563848 -1057.99569028] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.24401625e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 834: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 834: 0.9999210422424003\n","Performing step 835 of gradient descent.\n","z [  225.77159881   266.26446429  -500.43365961 ...   280.35024379\n","  -847.7576568  -1057.78911756] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.61767189e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 835: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 835: 0.9999210422424003\n","Performing step 836 of gradient descent.\n","z [  225.6889275    266.20212229  -500.34926181 ...   280.31126671\n","  -847.52958598 -1057.58254703] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.02431164e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 836: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 836: 0.9999210422424003\n","Performing step 837 of gradient descent.\n","z [  225.60624281   266.13982168  -500.26484611 ...   280.27226292\n","  -847.30142664 -1057.37597777] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.46685863e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 837: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 837: 0.9999210422424003\n","Performing step 838 of gradient descent.\n","z [  225.5235449    266.07756103  -500.18041182 ...   280.23323314\n","  -847.07317936 -1057.16940888] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.94849624e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 838: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 838: 0.9999210422424003\n","Performing step 839 of gradient descent.\n","z [  225.44083397   266.01533898  -500.09595827 ...   280.19417803\n","  -846.84484468 -1056.96283946] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.47269141e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 839: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 839: 0.9999210422424003\n","Performing step 840 of gradient descent.\n","z [  225.35811019   265.95315419  -500.01148483 ...   280.15509828\n","  -846.61642311 -1056.75626865] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.04321996e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 840: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 840: 0.9999210422424003\n","Performing step 841 of gradient descent.\n","z [  225.27537378   265.89100539  -499.92699088 ...   280.11599456\n","  -846.38791514 -1056.54969556] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.66419431e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 841: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 841: 0.9999210422424003\n","Performing step 842 of gradient descent.\n","z [  225.19262492   265.82889137  -499.84247584 ...   280.07686754\n","  -846.1593212  -1056.34311937] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.34009356e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 842: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 842: 0.9999210422424003\n","Performing step 843 of gradient descent.\n","z [  225.10986381   265.76681093  -499.75793914 ...   280.03771786\n","  -845.93064172 -1056.13653923] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.07579637e-218 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 843: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 843: 0.9999210422424003\n","Performing step 844 of gradient descent.\n","z [  225.02709066   265.70476296  -499.67338025 ...   279.99854617\n","  -845.70187711 -1055.92995432] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 9.8766168e-218 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 844: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 844: 0.9999210422424003\n","Performing step 845 of gradient descent.\n","z [  224.94430569   265.64274635  -499.58879864 ...   279.9593531\n","  -845.47302773 -1055.72336385] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.07483433e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 845: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 845: 0.9999210422424003\n","Performing step 846 of gradient descent.\n","z [  224.8615091    265.58076007  -499.50419383 ...   279.9201393\n","  -845.24409394 -1055.51676703] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.16972812e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 846: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 846: 0.9999210422424003\n","Performing step 847 of gradient descent.\n","z [  224.77870111   265.5188031   -499.41956535 ...   279.88090536\n","  -845.01507609 -1055.3101631 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.27302993e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 847: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 847: 0.9999210422424003\n","Performing step 848 of gradient descent.\n","z [  224.69588193   265.45687449  -499.33491274 ...   279.84165192\n","  -844.78597448 -1055.10355131] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.38548802e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 848: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 848: 0.9999210422424003\n","Performing step 849 of gradient descent.\n","z [  224.61305179   265.39497329  -499.25023559 ...   279.80237957\n","  -844.55678943 -1054.89693093] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.50791755e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 849: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 849: 0.9999210422424003\n","Performing step 850 of gradient descent.\n","z [  224.53021091   265.33309862  -499.16553348 ...   279.76308891\n","  -844.32752123 -1054.69030123] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.64120659e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 850: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 850: 0.9999210422424003\n","Performing step 851 of gradient descent.\n","z [  224.4473595    265.27124961  -499.08080604 ...   279.72378053\n","  -844.09817015 -1054.48366154] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.78632268e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 851: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 851: 0.9999210422424003\n","Performing step 852 of gradient descent.\n","z [  224.36449779   265.20942545  -498.9960529  ...   279.68445501\n","  -843.86873647 -1054.27701116] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.94431995e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 852: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 852: 0.9999210422424003\n","Performing step 853 of gradient descent.\n","z [  224.28162602   265.14762535  -498.91127373 ...   279.64511293\n","  -843.63922043 -1054.07034945] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.11634692e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 853: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 853: 0.9999210422424003\n","Performing step 854 of gradient descent.\n","z [  224.19874439   265.08584855  -498.82646819 ...   279.60575484\n","  -843.4096223  -1053.86367576] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.30365499e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 854: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 854: 0.9999210422424003\n","Performing step 855 of gradient descent.\n","z [  224.11585315   265.02409431  -498.74163599 ...   279.56638131\n","  -843.17994232 -1053.65698947] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.5076077e-217 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 855: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 855: 0.9999210422424003\n","Performing step 856 of gradient descent.\n","z [  224.03295253   264.96236194  -498.65677684 ...   279.52699289\n","  -842.95018071 -1053.45028997] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.7296908e-217 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 856: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 856: 0.9999210422424003\n","Performing step 857 of gradient descent.\n","z [  223.95004273   264.90065077  -498.57189047 ...   279.48759013\n","  -842.72033772 -1053.24357668] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.97152328e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 857: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 857: 0.9999210422424003\n","Performing step 858 of gradient descent.\n","z [  223.86712401   264.83896015  -498.48697664 ...   279.44817356\n","  -842.49041357 -1053.03684903] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.23486936e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 858: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 858: 0.9999210422424003\n","Performing step 859 of gradient descent.\n","z [  223.78419659   264.77728947  -498.40203511 ...   279.4087437\n","  -842.26040849 -1052.83010647] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.52165155e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 859: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 859: 0.9999210422424003\n","Performing step 860 of gradient descent.\n","z [  223.70126069   264.71563814  -498.31706567 ...   279.36930108\n","  -842.03032269 -1052.62334847] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.83396498e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 860: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 860: 0.9999210422424003\n","Performing step 861 of gradient descent.\n","z [  223.61831654   264.65400559  -498.23206812 ...   279.32984622\n","  -841.8001564  -1052.41657451] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.17409287e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 861: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 861: 0.9999210422424003\n","Performing step 862 of gradient descent.\n","z [  223.53536439   264.59239127  -498.14704228 ...   279.29037963\n","  -841.56990984 -1052.2097841 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.54452358e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 862: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 862: 0.9999210422424003\n","Performing step 863 of gradient descent.\n","z [  223.45240444   264.53079466  -498.06198798 ...   279.25090179\n","  -841.33958323 -1052.00297676] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.94796904e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 863: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 863: 0.9999210422424003\n","Performing step 864 of gradient descent.\n","z [  223.36943694   264.46921527  -497.97690507 ...   279.21141322\n","  -841.10917679 -1051.79615201] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.38738498e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 864: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 864: 0.9999210422424003\n","Performing step 865 of gradient descent.\n","z [  223.28646211   264.40765262  -497.89179342 ...   279.17191438\n","  -840.87869074 -1051.58930943] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 5.8659929e-217 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 865: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 865: 0.9999210422424003\n","Performing step 866 of gradient descent.\n","z [  223.20348018   264.34610624  -497.80665289 ...   279.13240577\n","  -840.6481253  -1051.38244858] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.38730411e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 866: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 866: 0.9999210422424003\n","Performing step 867 of gradient descent.\n","z [  223.12049137   264.28457569  -497.72148338 ...   279.09288785\n","  -840.41748071 -1051.17556904] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.95514585e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 867: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 867: 0.9999210422424003\n","Performing step 868 of gradient descent.\n","z [  223.03749591   264.22306056  -497.6362848  ...   279.0533611\n","  -840.18675719 -1050.96867043] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.57368985e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 868: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 868: 0.9999210422424003\n","Performing step 869 of gradient descent.\n","z [  222.95449403   264.16156045  -497.55105706 ...   279.01382596\n","  -839.95595498 -1050.76175236] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.24748351e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 869: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 869: 0.9999210422424003\n","Performing step 870 of gradient descent.\n","z [  222.87148594   264.10007496  -497.46580008 ...   278.97428289\n","  -839.7250743  -1050.55481447] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.98148378e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 870: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 870: 0.9999210422424003\n","Performing step 871 of gradient descent.\n","z [  222.78847188   264.03860372  -497.38051381 ...   278.93473234\n","  -839.49411541 -1050.34785642] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.78109427e-217 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 871: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 871: 0.9999210422424003\n","Performing step 872 of gradient descent.\n","z [  222.70545206   263.97714639  -497.29519821 ...   278.89517474\n","  -839.26307855 -1050.14087786] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.06522056e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 872: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 872: 0.9999210422424003\n","Performing step 873 of gradient descent.\n","z [  222.62242671   263.91570262  -497.20985324 ...   278.85561053\n","  -839.03196397 -1049.9338785 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.16012395e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 873: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 873: 0.9999210422424003\n","Performing step 874 of gradient descent.\n","z [  222.53939604   263.85427209  -497.12447886 ...   278.81604013\n","  -838.80077192 -1049.72685801] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.26351968e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 874: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 874: 0.9999210422424003\n","Performing step 875 of gradient descent.\n","z [  222.45636028   263.79285448  -497.03907508 ...   278.77646395\n","  -838.56950267 -1049.51981613] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.37617101e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 875: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 875: 0.9999210422424003\n","Performing step 876 of gradient descent.\n","z [  222.37331963   263.7314495   -496.95364187 ...   278.73688242\n","  -838.33815649 -1049.31275257] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.49891005e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 876: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 876: 0.9999210422424003\n","Performing step 877 of gradient descent.\n","z [  222.29027431   263.67005687  -496.86817926 ...   278.69729592\n","  -838.10673364 -1049.10566709] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.63264406e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 877: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 877: 0.9999210422424003\n","Performing step 878 of gradient descent.\n","z [  222.20722454   263.60867631  -496.78268726 ...   278.65770487\n","  -837.87523442 -1048.89855943] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 1.7783622e-216 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 878: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 878: 0.9999210422424003\n","Performing step 879 of gradient descent.\n","z [  222.12417052   263.54730756  -496.69716588 ...   278.61810966\n","  -837.6436591  -1048.69142937] (12665,)\n","y hat: [1.000000e+000 1.000000e+000 1.937143e-216 ... 1.000000e+000 0.000000e+000\n"," 0.000000e+000]\n","Loss at step 879: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 879: 0.9999210422424003\n","Performing step 880 of gradient descent.\n","z [  222.04111248   263.48595037  -496.61161517 ...   278.57851066\n","  -837.41200798 -1048.48427669] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.11016241e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 880: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 880: 0.9999210422424003\n","Performing step 881 of gradient descent.\n","z [  221.9580506    263.4246045   -496.52603517 ...   278.53890825\n","  -837.18028136 -1048.27710119] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.2987027e-216 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 881: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 881: 0.9999210422424003\n","Performing step 882 of gradient descent.\n","z [  221.87498511   263.36326972  -496.44042592 ...   278.49930281\n","  -836.94847955 -1048.06990269] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.50416205e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 882: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 882: 0.9999210422424003\n","Performing step 883 of gradient descent.\n","z [  221.7919162    263.30194582  -496.35478749 ...   278.45969471\n","  -836.71660285 -1047.86268101] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.72806509e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 883: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 883: 0.9999210422424003\n","Performing step 884 of gradient descent.\n","z [  221.70884408   263.24063258  -496.26911994 ...   278.42008429\n","  -836.48465159 -1047.65543599] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.97207436e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 884: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 884: 0.9999210422424003\n","Performing step 885 of gradient descent.\n","z [  221.62576895   263.1793298   -496.18342335 ...   278.38047192\n","  -836.25262611 -1047.44816747] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.23800285e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 885: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 885: 0.9999210422424003\n","Performing step 886 of gradient descent.\n","z [  221.542691     263.11803729  -496.0976978  ...   278.34085793\n","  -836.02052672 -1047.24087533] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.52782766e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 886: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 886: 0.9999210422424003\n","Performing step 887 of gradient descent.\n","z [  221.45961043   263.05675486  -496.01194337 ...   278.30124267\n","  -835.78835378 -1047.03355942] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.84370488e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 887: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 887: 0.9999210422424003\n","Performing step 888 of gradient descent.\n","z [  221.37652744   262.99548234  -495.92616017 ...   278.26162647\n","  -835.55610764 -1046.82621965] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.18798585e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 888: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 888: 0.9999210422424003\n","Performing step 889 of gradient descent.\n","z [  221.29344221   262.93421956  -495.84034829 ...   278.22200966\n","  -835.32378864 -1046.61885591] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.56323496e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 889: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 889: 0.9999210422424003\n","Performing step 890 of gradient descent.\n","z [  221.21035493   262.87296635  -495.75450785 ...   278.18239254\n","  -835.09139716 -1046.41146811] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.97224894e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 890: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 890: 0.9999210422424003\n","Performing step 891 of gradient descent.\n","z [  221.1272658    262.81172257  -495.66863894 ...   278.14277545\n","  -834.85893356 -1046.20405616] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.41807801e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 891: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 891: 0.9999210422424003\n","Performing step 892 of gradient descent.\n","z [  221.04417499   262.75048806  -495.58274171 ...   278.10315868\n","  -834.62639823 -1045.99662001] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.90404895e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 892: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 892: 0.9999210422424003\n","Performing step 893 of gradient descent.\n","z [  220.96108268   262.68926267  -495.49681626 ...   278.06354253\n","  -834.39379154 -1045.78915959] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.43379022e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 893: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 893: 0.9999210422424003\n","Performing step 894 of gradient descent.\n","z [  220.87798906   262.62804628  -495.41086274 ...   278.02392731\n","  -834.1611139  -1045.58167486] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.01125944e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 894: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 894: 0.9999210422424003\n","Performing step 895 of gradient descent.\n","z [  220.79489431   262.56683874  -495.32488127 ...   277.98431329\n","  -833.92836568 -1045.37416578] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.64077331e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 895: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 895: 0.9999210422424003\n","Performing step 896 of gradient descent.\n","z [  220.71179859   262.50563994  -495.238872   ...   277.94470076\n","  -833.69554731 -1045.16663232] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 8.3270403e-216 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 896: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 896: 0.9999210422424003\n","Performing step 897 of gradient descent.\n","z [  220.62870207   262.44444975  -495.15283507 ...   277.90508999\n","  -833.4626592  -1044.95907447] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.07519634e-216 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 897: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 897: 0.9999210422424003\n","Performing step 898 of gradient descent.\n","z [  220.54560494   262.38326806  -495.06677064 ...   277.86548125\n","  -833.22970175 -1044.75149222] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 9.8908437e-216 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 898: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 898: 0.9999210422424003\n","Performing step 899 of gradient descent.\n","z [  220.46250735   262.32209475  -494.98067885 ...   277.82587481\n","  -832.9966754  -1044.54388557] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.07800935e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 899: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 899: 0.9999210422424003\n","Performing step 900 of gradient descent.\n","z [  220.37940946   262.26092971  -494.89455987 ...   277.78627093\n","  -832.76358058 -1044.33625452] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 1.1749612e-215 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 900: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 900: 0.9999210422424003\n","Performing step 901 of gradient descent.\n","z [  220.29631144   262.19977284  -494.80841385 ...   277.74666984\n","  -832.53041772 -1044.12859911] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.28066714e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 901: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 901: 0.9999210422424003\n","Performing step 902 of gradient descent.\n","z [  220.21321345   262.13862405  -494.72224097 ...   277.70707181\n","  -832.29718727 -1043.92091936] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.39592046e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 902: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 902: 0.9999210422424003\n","Performing step 903 of gradient descent.\n","z [  220.13011563   262.07748323  -494.63604139 ...   277.66747706\n","  -832.06388967 -1043.71321531] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.52158659e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 903: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 903: 0.9999210422424003\n","Performing step 904 of gradient descent.\n","z [  220.04701815   262.0163503   -494.54981529 ...   277.62788584\n","  -831.83052538 -1043.50548699] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.65860967e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 904: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 904: 0.9999210422424003\n","Performing step 905 of gradient descent.\n","z [  219.96392116   261.95522515  -494.46356284 ...   277.58829836\n","  -831.59709485 -1043.29773447] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.80801969e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 905: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 905: 0.9999210422424003\n","Performing step 906 of gradient descent.\n","z [  219.88082479   261.89410771  -494.37728423 ...   277.54871484\n","  -831.36359856 -1043.08995781] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.97094035e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 906: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 906: 0.9999210422424003\n","Performing step 907 of gradient descent.\n","z [  219.7977292    261.8329979   -494.29097963 ...   277.50913551\n","  -831.13003697 -1042.88215706] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.14859762e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 907: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 907: 0.9999210422424003\n","Performing step 908 of gradient descent.\n","z [  219.71463452   261.77189563  -494.20464924 ...   277.46956057\n","  -830.89641056 -1042.67433232] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.34232906e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 908: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 908: 0.9999210422424003\n","Performing step 909 of gradient descent.\n","z [  219.6315409    261.71080082  -494.11829323 ...   277.42999023\n","  -830.6627198  -1042.46648366] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.55359396e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 909: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 909: 0.9999210422424003\n","Performing step 910 of gradient descent.\n","z [  219.54844846   261.64971341  -494.03191181 ...   277.39042468\n","  -830.42896518 -1042.25861117] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.78398453e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 910: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 910: 0.9999210422424003\n","Performing step 911 of gradient descent.\n","z [  219.46535735   261.58863331  -493.94550516 ...   277.35086412\n","  -830.1951472  -1042.05071494] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.03523799e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 911: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 911: 0.9999210422424003\n","Performing step 912 of gradient descent.\n","z [  219.38226769   261.52756045  -493.85907348 ...   277.31130873\n","  -829.96126633 -1041.84279509] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.3092498e-215 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 912: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 912: 0.9999210422424003\n","Performing step 913 of gradient descent.\n","z [  219.29917961   261.46649477  -493.77261696 ...   277.27175869\n","  -829.72732308 -1041.63485172] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.60808814e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 913: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 913: 0.9999210422424003\n","Performing step 914 of gradient descent.\n","z [  219.21609323   261.4054362   -493.68613582 ...   277.23221418\n","  -829.49331795 -1041.42688494] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.93400966e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 914: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 914: 0.9999210422424003\n","Performing step 915 of gradient descent.\n","z [  219.13300868   261.34438468  -493.59963024 ...   277.19267537\n","  -829.25925144 -1041.21889488] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.28947674e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 915: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 915: 0.9999210422424003\n","Performing step 916 of gradient descent.\n","z [  219.04992607   261.28334014  -493.51310044 ...   277.15314242\n","  -829.02512406 -1041.01088166] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.67717623e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 916: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 916: 0.9999210422424003\n","Performing step 917 of gradient descent.\n","z [  218.96684552   261.22230252  -493.42654661 ...   277.1136155\n","  -828.79093632 -1040.80284541] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.10004002e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 917: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 917: 0.9999210422424003\n","Performing step 918 of gradient descent.\n","z [  218.88376714   261.16127176  -493.33996896 ...   277.07409475\n","  -828.55668873 -1040.59478628] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.56126739e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 918: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 918: 0.9999210422424003\n","Performing step 919 of gradient descent.\n","z [  218.80069104   261.1002478   -493.25336771 ...   277.03458034\n","  -828.32238181 -1040.3867044 ] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 6.0643495e-215 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 919: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 919: 0.9999210422424003\n","Performing step 920 of gradient descent.\n","z [  218.71761733   261.03923059  -493.16674306 ...   276.99507239\n","  -828.08801608 -1040.17859992] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.61309605e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 920: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 920: 0.9999210422424003\n","Performing step 921 of gradient descent.\n","z [  218.63454611   260.97822007  -493.08009522 ...   276.95557105\n","  -827.85359205 -1039.97047299] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.21166442e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 921: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 921: 0.9999210422424003\n","Performing step 922 of gradient descent.\n","z [  218.55147748   260.91721618  -492.9934244  ...   276.91607645\n","  -827.61911026 -1039.76232378] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.86459146e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 922: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 922: 0.9999210422424003\n","Performing step 923 of gradient descent.\n","z [  218.46841155   260.85621888  -492.90673082 ...   276.87658872\n","  -827.38457123 -1039.55415244] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.57682821e-215 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 923: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 923: 0.9999210422424003\n","Performing step 924 of gradient descent.\n","z [  218.3853484    260.7952281   -492.82001469 ...   276.83710799\n","  -827.14997548 -1039.34595913] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 9.3537778e-215 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 924: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 924: 0.9999210422424003\n","Performing step 925 of gradient descent.\n","z [  218.30228814   260.73424381  -492.73327622 ...   276.79763438\n","  -826.91532354 -1039.13774403] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.02013368e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 925: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 925: 0.9999210422424003\n","Performing step 926 of gradient descent.\n","z [  218.21923085   260.67326594  -492.64651563 ...   276.75816799\n","  -826.68061595 -1038.9295073 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.11259405e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 926: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 926: 0.9999210422424003\n","Performing step 927 of gradient descent.\n","z [  218.13617662   260.61229446  -492.55973313 ...   276.71870895\n","  -826.44585323 -1038.72124913] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 1.2134612e-214 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 927: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 927: 0.9999210422424003\n","Performing step 928 of gradient descent.\n","z [  218.05312553   260.5513293   -492.47292894 ...   276.67925734\n","  -826.21103592 -1038.5129697 ] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.32350161e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 928: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 928: 0.9999210422424003\n","Performing step 929 of gradient descent.\n","z [  217.97007767   260.49037043  -492.38610328 ...   276.63981329\n","  -825.97616454 -1038.30466918] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.44355183e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 929: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 929: 0.9999210422424003\n","Performing step 930 of gradient descent.\n","z [  217.88703311   260.42941779  -492.29925636 ...   276.60037687\n","  -825.74123964 -1038.09634776] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.57452485e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 930: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 930: 0.9999210422424003\n","Performing step 931 of gradient descent.\n","z [  217.80399194   260.36847134  -492.2123884  ...   276.56094819\n","  -825.50626175 -1037.88800563] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.71741714e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 931: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 931: 0.9999210422424003\n","Performing step 932 of gradient descent.\n","z [  217.72095423   260.30753104  -492.12549962 ...   276.52152733\n","  -825.2712314  -1037.67964299] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 1.8733163e-214 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 932: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 932: 0.9999210422424003\n","Performing step 933 of gradient descent.\n","z [  217.63792004   260.24659683  -492.03859023 ...   276.48211437\n","  -825.03614913 -1037.47126002] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.04340937e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 933: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 933: 0.9999210422424003\n","Performing step 934 of gradient descent.\n","z [  217.55488945   260.18566868  -491.95166045 ...   276.44270939\n","  -824.80101547 -1037.26285693] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.22899196e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 934: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 934: 0.9999210422424003\n","Performing step 935 of gradient descent.\n","z [  217.47186253   260.12474654  -491.86471051 ...   276.40331248\n","  -824.56583096 -1037.05443392] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.43147822e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 935: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 935: 0.9999210422424003\n","Performing step 936 of gradient descent.\n","z [  217.38883934   260.06383037  -491.77774061 ...   276.36392369\n","  -824.33059613 -1036.84599117] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.65241167e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 936: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 936: 0.9999210422424003\n","Performing step 937 of gradient descent.\n","z [  217.30581994   260.00292012  -491.69075097 ...   276.32454309\n","  -824.09531151 -1036.63752891] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.8934771e-214 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 937: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 937: 0.9999210422424003\n","Performing step 938 of gradient descent.\n","z [  217.22280439   259.94201576  -491.60374182 ...   276.28517076\n","  -823.85997764 -1036.42904732] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.15651345e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 938: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 938: 0.9999210422424003\n","Performing step 939 of gradient descent.\n","z [  217.13979274   259.88111723  -491.51671336 ...   276.24580675\n","  -823.62459505 -1036.22054663] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.44352803e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 939: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 939: 0.9999210422424003\n","Performing step 940 of gradient descent.\n","z [  217.05678506   259.8202245   -491.42966582 ...   276.2064511\n","  -823.38916427 -1036.01202704] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.7567119e-214 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 940: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 940: 0.9999210422424003\n","Performing step 941 of gradient descent.\n","z [  216.97378139   259.75933752  -491.34259941 ...   276.16710389\n","  -823.15368583 -1035.80348875] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.09845671e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 941: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 941: 0.9999210422424003\n","Performing step 942 of gradient descent.\n","z [  216.89078179   259.69845626  -491.25551434 ...   276.12776515\n","  -822.91816026 -1035.59493198] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.47137317e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 942: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 942: 0.9999210422424003\n","Performing step 943 of gradient descent.\n","z [  216.8077863    259.63758067  -491.16841084 ...   276.08843493\n","  -822.68258809 -1035.38635695] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.87831105e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 943: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 943: 0.9999210422424003\n","Performing step 944 of gradient descent.\n","z [  216.72479497   259.57671072  -491.08128911 ...   276.04911327\n","  -822.44696984 -1035.17776386] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.32238118e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 944: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 944: 0.9999210422424003\n","Performing step 945 of gradient descent.\n","z [  216.64180783   259.51584636  -490.99414937 ...   276.0098002\n","  -822.21130603 -1034.96915293] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.80697938e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 945: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 945: 0.9999210422424003\n","Performing step 946 of gradient descent.\n","z [  216.55882494   259.45498755  -490.90699183 ...   275.97049577\n","  -821.9755972  -1034.76052437] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.33581258e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 946: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 946: 0.9999210422424003\n","Performing step 947 of gradient descent.\n","z [  216.47584633   259.39413425  -490.8198167  ...   275.93120001\n","  -821.73984385 -1034.55187841] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.91292743e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 947: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 947: 0.9999210422424003\n","Performing step 948 of gradient descent.\n","z [  216.39287203   259.33328643  -490.7326242  ...   275.89191294\n","  -821.50404652 -1034.34321525] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.54274139e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 948: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 948: 0.9999210422424003\n","Performing step 949 of gradient descent.\n","z [  216.30990209   259.27244404  -490.64541455 ...   275.85263459\n","  -821.26820571 -1034.13453512] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.23007687e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 949: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 949: 0.9999210422424003\n","Performing step 950 of gradient descent.\n","z [  216.22693654   259.21160704  -490.55818794 ...   275.81336497\n","  -821.03232193 -1033.92583823] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.98019831e-214 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 950: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 950: 0.9999210422424003\n","Performing step 951 of gradient descent.\n","z [  216.14397539   259.1507754   -490.47094458 ...   275.77410412\n","  -820.79639571 -1033.71712481] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 9.7988528e-214 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 951: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 951: 0.9999210422424003\n","Performing step 952 of gradient descent.\n","z [  216.0610187    259.08994908  -490.3836847  ...   275.73485205\n","  -820.56042755 -1033.50839506] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.06923144e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 952: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 952: 0.9999210422424003\n","Performing step 953 of gradient descent.\n","z [  215.97806647   259.02912803  -490.29640849 ...   275.69560876\n","  -820.32441795 -1033.29964921] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.16674325e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 953: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 953: 0.9999210422424003\n","Performing step 954 of gradient descent.\n","z [  215.89511874   258.96831222  -490.20911616 ...   275.65637428\n","  -820.08836743 -1033.09088748] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.27316846e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 954: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 954: 0.9999210422424003\n","Performing step 955 of gradient descent.\n","z [  215.81217553   258.90750161  -490.12180792 ...   275.6171486\n","  -819.85227648 -1032.88211008] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.38932343e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 955: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 955: 0.9999210422424003\n","Performing step 956 of gradient descent.\n","z [  215.72923686   258.84669616  -490.03448398 ...   275.57793174\n","  -819.6161456  -1032.67331724] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.51609937e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 956: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 956: 0.9999210422424003\n","Performing step 957 of gradient descent.\n","z [  215.64630274   258.78589584  -489.94714453 ...   275.53872369\n","  -819.37997529 -1032.46450917] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.65446928e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 957: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 957: 0.9999210422424003\n","Performing step 958 of gradient descent.\n","z [  215.56337321   258.7251006   -489.85978978 ...   275.49952447\n","  -819.14376603 -1032.25568609] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.80549542e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 958: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 958: 0.9999210422424003\n","Performing step 959 of gradient descent.\n","z [  215.48044827   258.6643104   -489.77241994 ...   275.46033405\n","  -818.90751833 -1032.04684822] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.97033753e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 959: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 959: 0.9999210422424003\n","Performing step 960 of gradient descent.\n","z [  215.39752794   258.60352522  -489.68503521 ...   275.42115246\n","  -818.67123266 -1031.83799577] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 2.1502618e-213 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 960: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 960: 0.9999210422424003\n","Performing step 961 of gradient descent.\n","z [  215.31461222   258.542745    -489.59763578 ...   275.38197966\n","  -818.43490952 -1031.62912897] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.34665058e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 961: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 961: 0.9999210422424003\n","Performing step 962 of gradient descent.\n","z [  215.23170114   258.48196972  -489.51022186 ...   275.34281566\n","  -818.19854937 -1031.42024803] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.56101317e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 962: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 962: 0.9999210422424003\n","Performing step 963 of gradient descent.\n","z [  215.1487947    258.42119933  -489.42279364 ...   275.30366045\n","  -817.96215271 -1031.21135316] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.79499737e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 963: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 963: 0.9999210422424003\n","Performing step 964 of gradient descent.\n","z [  215.06589291   258.36043379  -489.33535133 ...   275.26451401\n","  -817.72571999 -1031.00244458] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.05040229e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 964: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 964: 0.9999210422424003\n","Performing step 965 of gradient descent.\n","z [  214.98299578   258.29967308  -489.24789512 ...   275.22537632\n","  -817.4892517  -1030.79352251] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.3291922e-213 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 965: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 965: 0.9999210422424003\n","Performing step 966 of gradient descent.\n","z [  214.9001033    258.23891715  -489.1604252  ...   275.18624738\n","  -817.2527483  -1030.58458716] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.63351176e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 966: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 966: 0.9999210422424003\n","Performing step 967 of gradient descent.\n","z [  214.81721549   258.17816596  -489.07294177 ...   275.14712715\n","  -817.01621026 -1030.37563875] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.96570258e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 967: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 967: 0.9999210422424003\n","Performing step 968 of gradient descent.\n","z [  214.73433234   258.11741948  -488.98544503 ...   275.10801563\n","  -816.77963803 -1030.16667748] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.32832129e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 968: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 968: 0.9999210422424003\n","Performing step 969 of gradient descent.\n","z [  214.65145386   258.05667767  -488.89793517 ...   275.06891278\n","  -816.54303207 -1029.95770357] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.72415937e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 969: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 969: 0.9999210422424003\n","Performing step 970 of gradient descent.\n","z [  214.56858004   257.99594048  -488.81041239 ...   275.02981858\n","  -816.30639284 -1029.74871722] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 5.1562647e-213 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 970: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 970: 0.9999210422424003\n","Performing step 971 of gradient descent.\n","z [  214.48571089   257.93520789  -488.72287686 ...   274.99073301\n","  -816.06972079 -1029.53971866] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.62796515e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 971: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 971: 0.9999210422424003\n","Performing step 972 of gradient descent.\n","z [  214.4028464    257.87447986  -488.63532879 ...   274.95165603\n","  -815.83301635 -1029.33070809] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.14289431e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 972: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 972: 0.9999210422424003\n","Performing step 973 of gradient descent.\n","z [  214.31998656   257.81375635  -488.54776837 ...   274.91258762\n","  -815.59627999 -1029.12168571] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.70501962e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 973: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 973: 0.9999210422424003\n","Performing step 974 of gradient descent.\n","z [  214.23713138   257.75303731  -488.46019577 ...   274.87352774\n","  -815.35951212 -1028.91265174] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.31867305e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 974: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 974: 0.9999210422424003\n","Performing step 975 of gradient descent.\n","z [  214.15428083   257.69232272  -488.37261121 ...   274.83447637\n","  -815.1227132  -1028.70360638] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 7.98858463e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 975: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 975: 0.9999210422424003\n","Performing step 976 of gradient descent.\n","z [  214.07143492   257.63161254  -488.28501485 ...   274.79543346\n","  -814.88588365 -1028.49454984] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 8.71991908e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 976: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 976: 0.9999210422424003\n","Performing step 977 of gradient descent.\n","z [  213.98859363   257.57090673  -488.19740689 ...   274.75639899\n","  -814.64902391 -1028.28548233] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 9.51831574e-213 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 977: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 977: 0.9999210422424003\n","Performing step 978 of gradient descent.\n","z [  213.90575696   257.51020525  -488.10978752 ...   274.7173729\n","  -814.41213439 -1028.07640403] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.03899323e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 978: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 978: 0.9999210422424003\n","Performing step 979 of gradient descent.\n","z [  213.82292488   257.44950806  -488.02215692 ...   274.67835518\n","  -814.17521552 -1027.86731517] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.13414923e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 979: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 979: 0.9999210422424003\n","Performing step 980 of gradient descent.\n","z [  213.7400974    257.38881513  -487.93451529 ...   274.63934577\n","  -813.93826772 -1027.65821594] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.23803374e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 980: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 980: 0.9999210422424003\n","Performing step 981 of gradient descent.\n","z [  213.65727449   257.32812642  -487.84686279 ...   274.60034463\n","  -813.7012914  -1027.44910655] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.35144842e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 981: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 981: 0.9999210422424003\n","Performing step 982 of gradient descent.\n","z [  213.57445614   257.26744189  -487.75919963 ...   274.56135172\n","  -813.46428698 -1027.23998719] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.47526861e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 982: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 982: 0.9999210422424003\n","Performing step 983 of gradient descent.\n","z [  213.49164233   257.2067615   -487.67152599 ...   274.522367\n","  -813.22725486 -1027.03085806] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.61045013e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 983: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 983: 0.9999210422424003\n","Performing step 984 of gradient descent.\n","z [  213.40883305   257.14608522  -487.58384204 ...   274.48339041\n","  -812.99019544 -1026.82171936] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.75803669e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 984: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 984: 0.9999210422424003\n","Performing step 985 of gradient descent.\n","z [  213.32602827   257.08541301  -487.49614799 ...   274.44442193\n","  -812.75310913 -1026.61257129] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 1.91916794e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 985: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 985: 0.9999210422424003\n","Performing step 986 of gradient descent.\n","z [  213.24322799   257.02474484  -487.40844401 ...   274.40546148\n","  -812.51599633 -1026.40341405] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.09508832e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 986: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 986: 0.9999210422424003\n","Performing step 987 of gradient descent.\n","z [  213.16043217   256.96408065  -487.32073028 ...   274.36650904\n","  -812.27885743 -1026.19424783] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.28715671e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 987: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 987: 0.9999210422424003\n","Performing step 988 of gradient descent.\n","z [  213.0776408    256.90342043  -487.233007   ...   274.32756454\n","  -812.04169282 -1025.98507283] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.49685693e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 988: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 988: 0.9999210422424003\n","Performing step 989 of gradient descent.\n","z [  212.99485385   256.84276412  -487.14527435 ...   274.28862794\n","  -811.80450289 -1025.77588924] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 2.72580927e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 989: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 989: 0.9999210422424003\n","Performing step 990 of gradient descent.\n","z [  212.91207131   256.78211169  -487.05753252 ...   274.24969918\n","  -811.56728802 -1025.56669726] (12665,)\n","y hat: [1.000000e+000 1.000000e+000 2.975783e-212 ... 1.000000e+000 0.000000e+000\n"," 0.000000e+000]\n","Loss at step 990: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 990: 0.9999210422424003\n","Performing step 991 of gradient descent.\n","z [  212.82929314   256.7214631   -486.96978169 ...   274.21077822\n","  -811.3300486  -1025.35749707] (12665,)\n","y hat: [1.0000000e+000 1.0000000e+000 3.2487101e-212 ... 1.0000000e+000\n"," 0.0000000e+000 0.0000000e+000]\n","Loss at step 991: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 991: 0.9999210422424003\n","Performing step 992 of gradient descent.\n","z [  212.74651933   256.66081832  -486.88202205 ...   274.17186499\n","  -811.09278501 -1025.14828889] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.54670025e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 992: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 992: 0.9999210422424003\n","Performing step 993 of gradient descent.\n","z [  212.66374984   256.6001773   -486.7942538  ...   274.13295944\n","  -810.85549761 -1024.93907289] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 3.87205711e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 993: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 993: 0.9999210422424003\n","Performing step 994 of gradient descent.\n","z [  212.58098466   256.53954002  -486.70647711 ...   274.09406152\n","  -810.6181868  -1024.72984926] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.22729625e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 994: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 994: 0.9999210422424003\n","Performing step 995 of gradient descent.\n","z [  212.49822374   256.47890642  -486.61869219 ...   274.05517116\n","  -810.38085292 -1024.52061821] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 4.61516458e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 995: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 995: 0.9999210422424003\n","Performing step 996 of gradient descent.\n","z [  212.41546707   256.41827647  -486.53089921 ...   274.01628831\n","  -810.14349637 -1024.31137992] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.03866165e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 996: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 996: 0.9999210422424003\n","Performing step 997 of gradient descent.\n","z [  212.33271462   256.35765013  -486.44309839 ...   273.97741292\n","  -809.90611748 -1024.10213458] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 5.50106288e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 997: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 997: 0.9999210422424003\n","Performing step 998 of gradient descent.\n","z [  212.24996635   256.29702736  -486.35528991 ...   273.93854491\n","  -809.66871664 -1023.89288239] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.00594496e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 998: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 998: 0.9999210422424003\n","Performing step 999 of gradient descent.\n","z [  212.16722223   256.23640812  -486.26747397 ...   273.89968424\n","  -809.4312942  -1023.68362353] (12665,)\n","y hat: [1.00000000e+000 1.00000000e+000 6.55721354e-212 ... 1.00000000e+000\n"," 0.00000000e+000 0.00000000e+000]\n","Loss at step 999: [nan nan  0. ... nan nan nan]\n","(12665, 785)\n","(12665,)\n","(785,)\n","[1 1 0 ... 1 0 0]\n","(12665,)\n","Accuracy at step 999: 0.9999210422424003\n"]}]},{"cell_type":"markdown","metadata":{"id":"Dt_XyizgcD7H"},"source":["Evaluate model on test set"]},{"cell_type":"code","metadata":{"id":"RIysfpyCcIN2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c42a9e10-3f84-4a74-9ae9-2c9b9ad243b7","executionInfo":{"status":"ok","timestamp":1682323987034,"user_tz":-120,"elapsed":307,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(\"_______________________________\")\n","print(\"Starting evaluation of test set\")\n","\n","X,y = data_preprocess(x_test, y_test)\n","z = np.dot(X,w) \n","y_hat = sigmoid(z)\n","predictions = (y_hat>0.5).astype(np.int32)\n","accuracy = np.mean(predictions==y)\n","print(\"Accuracy of test set: \" + str(accuracy))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["_______________________________\n","Starting evaluation of test set\n","10000\n","length  2115\n","(2115, 785)\n","(2115, 785)\n","Accuracy of test set: 1.0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-dc64dee5606b>:2: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-z))\n"]}]}]}