{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmURzPPRtODn"
      },
      "source": [
        "# Logistic Regression Assignment (due 26 November)\n",
        "\n",
        "In this practical you will learn how to apply logistic regression to the task of predicting two digits from the MNIST database: http://yann.lecun.com/exdb/mnist/. The database contains 60000 train images containing digits and 10000 test images. The images are of size 28 Ã— 28. We will use the images in a vectorized form: a vector of size of 784. The code extracting the digits 0 and 1 is provided in the stubs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQkzOf0dFpxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00b33b55-c6b3-477f-e9a8-90584d610546"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI0dT0-WEYuh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqiqzu3dFXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c85661-59c2-49b1-b7ae-452f0c1d7495"
      },
      "source": [
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSjE8NINEvVO"
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybQxEwHnGsgY"
      },
      "source": [
        "def data_preprocess(images, labels):\n",
        "\n",
        "    # number of examples m  \n",
        "    m = images.shape[0]\n",
        "    \n",
        "    print(m)\n",
        "    # create vector of ones to concatenate to our data matrix (for intercept terms)\n",
        "    ones = np.ones(shape=[m, 1])\n",
        "    images = np.concatenate((ones, images), axis=1)\n",
        "    \n",
        "    # to retrieve the images and corresponding labels where the label is either 0 or 1, \n",
        "    # we define two logical vectors that can be used to subset our data_matrices\n",
        "    logical_mask_0 = labels == 0\n",
        "    logical_mask_1 = labels == 1\n",
        "    \n",
        "    images_zeros = images[logical_mask_0]\n",
        "    labels_zeros = labels[logical_mask_0]\n",
        "    images_ones = images[logical_mask_1]\n",
        "    labels_ones = labels[logical_mask_1]\n",
        "    \n",
        "    X = np.concatenate((images_zeros, images_ones), axis=0)\n",
        "    y = np.concatenate((labels_zeros, labels_ones), axis=0)\n",
        "    \n",
        "    # shuffle the data and corresponding labels in unison\n",
        "    def _shuffle_in_unison(a, b):\n",
        "        assert len(a) == len(b)\n",
        "        p = np.random.permutation(len(a))\n",
        "        print('length ', len(a))\n",
        "        print(a.shape)\n",
        "        print(a[p].shape)\n",
        "        return a[p], b[p]\n",
        "\n",
        "    return _shuffle_in_unison(X,y)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrkpG7BYI8MT"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHEiLlEnNhIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51db60c2-0e6f-49b5-e079-1909d9e66af7"
      },
      "source": [
        "print (x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5Hr9aENMTy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "686c121e-89d7-4858-ebf5-59cc1cd25256"
      },
      "source": [
        "x_train = x_train.reshape([60000,784])\n",
        "x_test = x_test.reshape([10000,784])\n",
        "print(x_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMi47AaKcHzQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "acc88d49-4860-4a6e-a3c3-1676486cbf7f"
      },
      "source": [
        "X,y = data_preprocess(x_train, y_train)\n",
        "print('shape: ', X.shape)\n",
        "print('shape: ', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "length  12665\n",
            "(12665, 785)\n",
            "(12665, 785)\n",
            "shape:  (12665, 785)\n",
            "shape:  (12665,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTx9l6kePGbf"
      },
      "source": [
        "Define hyperparams: learning rate and gradient descent steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzE8JqGXdBy2"
      },
      "source": [
        "learning_rate = \n",
        "gdc_steps = \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5ujHEHrQDy_"
      },
      "source": [
        "Initialize your parameters W\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnXlpZqMR4jP"
      },
      "source": [
        "# number of features n\n",
        "n = X.shape[1]\n",
        "# we need to define our model parameters to be learned. we use W (weights) instead of theta this time.\n",
        "mu, sigma = 0, 0.01 # mean and standard deviation\n",
        "w = np.random.normal(mu, sigma, n)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b-fVYzzR6_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c26f528-7e68-44c3-8f81-fa37449bfc13"
      },
      "source": [
        "print(X.shape, w.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12665, 785) (785,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp3Uc6qUSIi0"
      },
      "source": [
        "Define the sigmoid function, your code here:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_gdiFglSMYN"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m14UBJJ6SRGS"
      },
      "source": [
        "Define the loss function as provided in equation 12 (Logistic regression slides)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyCliB0USThU"
      },
      "source": [
        "def compute_cross_entropy_loss(y, y_hat):\n",
        "    \n",
        "    return 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vusNro8SX5-"
      },
      "source": [
        "Start optimization. During training you minimize the loss function. In every iteration your loss should decrease. You also want to look how many correct predictions you have at every iteration. Reminder: the belonging to class digit 1 is when your prediction, $\\hat y$ is greater or equal to 0.5. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKXM0YdDcajm"
      },
      "source": [
        "When you test your prediction vector (containing zero and ones) with the labels (also zero and ones) you can use the equal function. \n",
        "\n",
        "Example:\n",
        "prediction = (1, 0, 1, 1) and the true labels are y = (0, 0, 1, 0).\n",
        "\n",
        "When you test on equality you get following result: correct = (0, 1, 1, 0). Your accuracy is: 0+1+1+0\n",
        "4 = 0.5.\n",
        "You compute the accuracy for the training and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ToLLOp2Scv0"
      },
      "source": [
        "for step in range(0, gdc_steps):\n",
        "    print(\"Performing step \" + str(step) + \" of gradient descent.\")\n",
        "    # perform the dot product between the weights and the examples\n",
        "    z =    \n",
        "    print('z', z, z.shape)\n",
        "    # apply the nonlinearity\n",
        "    y_hat = sigmoid(z)\n",
        "    print(\"y hat: \" + str(y_hat))\n",
        "    # normally normalized with -1/m \n",
        "    loss = compute_cross_entropy_loss(y, y_hat)\n",
        "    print(\"Loss at step \" + str(step) + \": \" + str(loss))\n",
        "    \n",
        "    # compute the error term, i.e. the difference between labels and estimated labels y_hat, see equation 24 in the slides\n",
        "    error_term = \n",
        "    \n",
        "    # compute the gradient. as our data matrix X is currently layed out as X_j_i, we got to transpose it\n",
        "    # see derived formula of the gradient calculation\n",
        "    gradients = \n",
        "    print(X.shape)\n",
        "    print(error_term.shape)\n",
        "    print(gradients.shape)\n",
        "    \n",
        "    # update w using the gdc update rule\n",
        "    w = w-learning_rate*gradients\n",
        "    \n",
        "    # compute the predictions and cast them to int values\n",
        "    predictions = \n",
        "    print(predictions.shape)\n",
        "    # compute mean accuracy\n",
        "    accuracy = \n",
        "    print(\"Accuracy at step \" + str(step) + \": \" + str(accuracy))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt_XyizgcD7H"
      },
      "source": [
        "Evaluate model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIysfpyCcIN2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "18ba69d1-2160-49fb-e640-09f7ac0a3b74"
      },
      "source": [
        "print(\"_______________________________\")\n",
        "print(\"Starting evaluation of test set\")\n",
        "\n",
        "X,y = data_preprocess(x_test, y_test)\n",
        "z = np.dot(X,w) \n",
        "y_hat = sigmoid(z)\n",
        "predictions = (y_hat>0.5).astype(np.int32)\n",
        "accuracy = np.mean(predictions==y)\n",
        "print(\"Accuracy of test set: \" + str(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_______________________________\n",
            "Starting evaluation of test set\n",
            "10000\n",
            "length  2115\n",
            "(2115, 785)\n",
            "(2115, 785)\n",
            "Accuracy of test set: 0.9990543735224586\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}