{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7lL8UDAvPQaB"},"source":["# In this notebook you will implement the **Logistic Regression** by means of **Softmax**."]},{"cell_type":"code","metadata":{"id":"xI0dT0-WEYuh","executionInfo":{"status":"ok","timestamp":1682323509280,"user_tz":-120,"elapsed":6991,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqiqzu3dFXKj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b91e5622-04a6-469e-e055-e02331dfc8c8","executionInfo":{"status":"ok","timestamp":1682323513123,"user_tz":-120,"elapsed":407,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print(tf.__version__)\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"ZSjE8NINEvVO","executionInfo":{"status":"ok","timestamp":1682323514856,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["import numpy as np "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrkpG7BYI8MT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"377f3ef7-30e9-4e7f-e8dd-2c3d1366bfe7","executionInfo":{"status":"ok","timestamp":1682323520670,"user_tz":-120,"elapsed":3542,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"CHEiLlEnNhIm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"56d7fe8e-202d-4e00-ebe0-1c06ba5e5dd9","executionInfo":{"status":"ok","timestamp":1682323529542,"user_tz":-120,"elapsed":587,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["print (x_train.shape, x_train.dtype)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28) float64\n"]}]},{"cell_type":"code","metadata":{"id":"Cb5Hr9aENMTy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e493b447-011e-4be1-deb8-0d488f48ead9","executionInfo":{"status":"ok","timestamp":1682323540780,"user_tz":-120,"elapsed":542,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["x_train = x_train.reshape([60000,784])\n","x_test = x_test.reshape([10000,784])\n","print(x_train.shape)\n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n"]}]},{"cell_type":"code","metadata":{"id":"VchHR4WRwYLT","executionInfo":{"status":"ok","timestamp":1682323543709,"user_tz":-120,"elapsed":5,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["logical_mask_0 = y_train == 0\n","logical_mask_1 = y_train == 1"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fcSdmzcxUKv","executionInfo":{"status":"ok","timestamp":1682323546324,"user_tz":-120,"elapsed":1082,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["zeros = tf.boolean_mask(x_train, logical_mask_0)\n","labels_0 = tf.boolean_mask(y_train, logical_mask_0)\n","ones = tf.boolean_mask(x_train, logical_mask_1)\n","labels_1 = tf.boolean_mask(y_train, logical_mask_1)\n","all0_1 = tf.cast(tf.concat([zeros, ones], 0), tf.float32)\n","all0_1_labels = tf.cast(tf.concat([labels_0, labels_1], 0), tf.int32)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzz7vocjXDRW","executionInfo":{"status":"ok","timestamp":1682323548194,"user_tz":-120,"elapsed":7,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["dataset = tf.data.Dataset.from_tensor_slices((all0_1,all0_1_labels))\n","# Shuffle, repeat, and batch the examples.\n","dataset = dataset.shuffle(12000).repeat(10).batch(10)\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pd6gwY7xydcm"},"source":["Define the network model using tf.keras.models.Sequential\n","\n","Think how many output neurons you need (see also lecture slides)"]},{"cell_type":"code","metadata":{"id":"2FxhTxo3yhEH","executionInfo":{"status":"ok","timestamp":1682323550396,"user_tz":-120,"elapsed":366,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba594ce8-28c9-4454-996d-a44a5208a2c7"},"source":["\n","model = tf.keras.models.Sequential(\n","    [\n","        tf.keras.layers.Input(shape=(784,)),\n","        tf.keras.layers.Dense(392,activation=tf.nn.softmax),\n","        tf.keras.layers.Dense(2,activation=tf.nn.softmax),\n","    ]\n",")\n","\n","\n","model.summary();"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 392)               307720    \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 786       \n","                                                                 \n","=================================================================\n","Total params: 308,506\n","Trainable params: 308,506\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"cpr8yZVO6xsB"},"source":["Define your optimizer, loss, metrics. Your loss should match the softmax activation in the output. Remember the true values are not one-hot encoded but sparse. \n","\n","See model.compile"]},{"cell_type":"code","metadata":{"id":"CVJHVDzc62BE","executionInfo":{"status":"ok","timestamp":1682323553624,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}}},"source":["\n","model.compile(optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=\"Accuracy\")\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W--SO4QKzpxJ"},"source":["Fit your model"]},{"cell_type":"code","metadata":{"id":"1EM8qCqizwEe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682323596330,"user_tz":-120,"elapsed":40585,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"2070298b-04e8-47e1-8a7a-8e6dabbf82db"},"source":["model.fit(dataset)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["12665/12665 [==============================] - 40s 3ms/step - loss: 0.3920 - Accuracy: 0.7836\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fc6a5e2bf40>"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Lwy44sErwQPl"},"source":["Apply your model to the test data.\n","\n","First extract the zeros and ones out of the whole test dataset"]},{"cell_type":"code","metadata":{"id":"-prp7AicwTWK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682323605279,"user_tz":-120,"elapsed":3490,"user":{"displayName":"Nick Häcker","userId":"10634649505651262181"}},"outputId":"3b3fe74a-1dbf-41f1-841e-9257b0bb0999"},"source":["logical_mask_0 = y_test == 0\n","logical_mask_1 = y_test == 1\n","zeros = tf.boolean_mask(x_test, logical_mask_0)\n","labels_0 = tf.boolean_mask(y_test, logical_mask_0)\n","ones = tf.boolean_mask(x_test, logical_mask_1)\n","labels_1 = tf.boolean_mask(y_test, logical_mask_1)\n","all0_1 = tf.cast(tf.concat([zeros, ones], 0), tf.float32)\n","all0_1_labels = tf.cast(tf.concat([labels_0, labels_1], 0), tf.int32)\n","\n","dataset_test = tf.data.Dataset.from_tensor_slices((all0_1,all0_1_labels))\n","# Shuffle, repeat, and batch the examples.\n","dataset_test = dataset_test.batch(1)\n","print(dataset_test)\n","\n","#evaluate, print the accuracy\n","#your code here\n","loss, accuracy = model.evaluate(dataset_test)\n","print(loss)\n","print(accuracy)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<_BatchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n","2115/2115 [==============================] - 3s 1ms/step - loss: 0.0444 - Accuracy: 0.9986\n","0.04438941925764084\n","0.99858158826828\n"]}]}]}